{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsBERT on Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrRtlUpXmOcC",
        "colab_type": "text"
      },
      "source": [
        "# NewsBERT on Colab\n",
        "\n",
        "This notebook uses awesome [streamlit tutorial](https://youtu.be/x0NdZkaciws) to run NewsBERT on Colab.\n",
        "\n",
        "For running this code you will need to setup your [ngrok](https://ngrok.com/) account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puIeoSMDmNzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install streamlit pyngrok\n",
        "!pip install git+https://github.com/lambdaofgod/pytorch_hackathon/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdjyd9NdZ2v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "5f9cb2df-6262-4bf8-cd20-e6978b0c4248"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 24kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision==0.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0+cu101) (7.0.0)\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.5.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.5.1\n",
            "    Uninstalling torch-1.5.1:\n",
            "      Successfully uninstalled torch-1.5.1\n",
            "Successfully installed torch-1.5.1+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqz1wl14aIYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7725c2f2-5702-4fba-deff-20da75774b39"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95E2S3CMQNgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96b61431-490f-4c94-a893-5a32e65f664b"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import os\n",
        "from operator import itemgetter\n",
        "\n",
        "import torch\n",
        "from pytorch_hackathon import rss_feeds, zero_shot_learning, haystack_search\n",
        "import seaborn as sns\n",
        "\n",
        "st.title('Zero-shot RSS feed article classifier')\n",
        "\n",
        "cm = sns.light_palette(\"green\", as_cmap=True)\n",
        "topic_strings = list(pd.read_table('https://raw.githubusercontent.com/lambdaofgod/pytorch_hackathon/master/data/topics.txt', header=None).iloc[:,0].values)\n",
        "rss_feed_urls = list(pd.read_table('https://raw.githubusercontent.com/lambdaofgod/pytorch_hackathon/master/data/feeds.txt', header=None).iloc[:,0].values)\n",
        "rss_feed_urls = rss_feeds.rss_feed_urls.copy()\n",
        "\n",
        "\n",
        "model_device = st.selectbox(\"Model device\", [\"cpu\", \"cuda\"], index=int(torch.cuda.is_available()))\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_feed_df():\n",
        "    with st.spinner('Retrieving articles from feeds...'):\n",
        "        return rss_feeds.get_feed_df(rss_feed_urls)\n",
        "\n",
        "\n",
        "feed_df = get_feed_df()\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def setup_searcher(feed_df, use_gpu, model_name=\"deepset/sentence_bert\"):\n",
        "    with st.spinner('No precomputed topics found, running zero-shot learning...'):\n",
        "        searcher = haystack_search.Searcher(model_name, 'text', use_gpu=use_gpu)\n",
        "        searcher.add_texts(feed_df)\n",
        "    return searcher \n",
        "\n",
        "\n",
        "# we need to copy feed_df so that streamlit doesn't recompute embeddings when feed_df changes \n",
        "searcher = setup_searcher(feed_df, use_gpu=model_device == 'cuda') \n",
        "\n",
        "\n",
        "@st.cache\n",
        "def get_retrieved_df(topic_strings):\n",
        "    results = [\n",
        "        result \n",
        "        for topic in topic_strings\n",
        "        for result in searcher.retriever.retrieve(\n",
        "            \"text is about {}\".format(topic)\n",
        "        )\n",
        "    ]\n",
        "    return searcher.get_topic_score_df(\n",
        "        results,\n",
        "        topic_strings\n",
        "    ).drop_duplicates(subset='title')\n",
        "    \n",
        "\n",
        "selected_df = get_retrieved_df(topic_strings).reset_index(drop=True)\n",
        "selected_df['text'] = selected_df['text'].apply(lambda s: s[:1000])\n",
        "topics = st.multiselect('Choose topics', topic_strings, default=[topic_strings[0]])\n",
        "sort_by = st.selectbox(\"Sort by\", topics)\n",
        "display_df = selected_df[selected_df[topics].min(axis=1) > 0.5].sort_values(sort_by, ascending=False)\n",
        "\n",
        "st.markdown('## Articles on {}'.format(', '.join(topics)))\n",
        "\n",
        "st.table(display_df[display_df[topics].min(axis=1) > 0.5].style.background_gradient(cmap=cm))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1laW7BxnDY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Checking if GPU is available\n",
        "\n",
        "Running this on GPU will be much faster (embeddings for articles will be calculated much faster)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Fve21IZIYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fecedee1-db81-40b7-98bb-e9cb013c1d8b"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isXI-T7zTPYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6yG4G9OmtGH",
        "colab_type": "text"
      },
      "source": [
        "# ngrok token\n",
        "\n",
        "You will need to paste your ngrok authtoken. You can find it [here](https://dashboard.ngrok.com/auth/your-authtoken) provided you have an ngrok account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGUrrsG1T4Oj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "890f8111-b24a-41ba-801e-84125db3c6f8"
      },
      "source": [
        "#@title ngrok token\n",
        "\n",
        "token = '' #@param {type:\"string\"}\n",
        "!ngrok authtoken $token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJKlMN25UEpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!streamlit run app.py&>log&"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKeo80BSU5l6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "outputId": "8e929c89-7ec6-4d29-ca14-4c12809f09a5"
      },
      "source": [
        "!cat log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  You can now view your Streamlit app in your browser.\n",
            "\n",
            "  Network URL: http://172.28.0.2:8503\n",
            "  External URL: http://34.87.86.86:8503\n",
            "\n",
            "2020-08-16 18:02:32.817699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "\r  0%|          | 0/16 [00:00<?, ?it/s]\r  6%|▋         | 1/16 [00:03<00:55,  3.70s/it]\r 12%|█▎        | 2/16 [00:04<00:39,  2.81s/it]\r 19%|█▉        | 3/16 [00:04<00:27,  2.13s/it]\r 25%|██▌       | 4/16 [00:05<00:21,  1.80s/it]\r 31%|███▏      | 5/16 [00:06<00:15,  1.37s/it]\r 38%|███▊      | 6/16 [00:06<00:10,  1.05s/it]\r 44%|████▍     | 7/16 [00:07<00:07,  1.20it/s]\r 50%|█████     | 8/16 [00:07<00:05,  1.40it/s]\r 56%|█████▋    | 9/16 [00:08<00:05,  1.32it/s]\r 62%|██████▎   | 10/16 [00:09<00:04,  1.28it/s]\r 69%|██████▉   | 11/16 [00:10<00:04,  1.18it/s]\r 75%|███████▌  | 12/16 [00:10<00:03,  1.24it/s]\r 81%|████████▏ | 13/16 [00:11<00:02,  1.17it/s]\r 88%|████████▊ | 14/16 [00:12<00:01,  1.27it/s]\r 94%|█████████▍| 15/16 [00:13<00:00,  1.35it/s]\r100%|██████████| 16/16 [00:13<00:00,  1.43it/s]\r100%|██████████| 16/16 [00:13<00:00,  1.17it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_hackathon/rss_feeds.py:63: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 63 of the file /usr/local/lib/python3.6/dist-packages/pytorch_hackathon/rss_feeds.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  feed_df['text'] = feed_df['summary'].apply(lambda s: bs4.BeautifulSoup(s).text)\n",
            "2020-08-16 18:02:48.872 Init retriever using embeddings of model deepset/sentence_bert\n",
            "2020-08-16 18:02:48.873 device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
            "2020-08-16 18:02:48.873 Could not find `deepset/sentence_bert` locally. Try to download from model hub ...\n",
            "2020-08-16 18:02:52.374 Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "2020-08-16 18:03:03.029 device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
            "\rInferencing Samples:   0%|          | 0/74 [00:00<?, ? Batches/s]\rInferencing Samples:   1%|▏         | 1/74 [00:00<00:14,  5.12 Batches/s]\rInferencing Samples:   3%|▎         | 2/74 [00:00<00:13,  5.51 Batches/s]\rInferencing Samples:   4%|▍         | 3/74 [00:00<00:12,  5.82 Batches/s]\rInferencing Samples:   5%|▌         | 4/74 [00:00<00:11,  6.04 Batches/s]\rInferencing Samples:   7%|▋         | 5/74 [00:00<00:11,  6.25 Batches/s]\rInferencing Samples:   8%|▊         | 6/74 [00:00<00:10,  6.39 Batches/s]\rInferencing Samples:   9%|▉         | 7/74 [00:01<00:10,  6.32 Batches/s]\rInferencing Samples:  11%|█         | 8/74 [00:01<00:10,  6.31 Batches/s]\rInferencing Samples:  12%|█▏        | 9/74 [00:01<00:10,  6.42 Batches/s]\rInferencing Samples:  14%|█▎        | 10/74 [00:01<00:09,  6.48 Batches/s]\rInferencing Samples:  15%|█▍        | 11/74 [00:01<00:09,  6.56 Batches/s]\rInferencing Samples:  16%|█▌        | 12/74 [00:01<00:09,  6.62 Batches/s]\rInferencing Samples:  18%|█▊        | 13/74 [00:02<00:09,  6.59 Batches/s]\rInferencing Samples:  19%|█▉        | 14/74 [00:02<00:09,  6.54 Batches/s]\rInferencing Samples:  20%|██        | 15/74 [00:02<00:09,  6.53 Batches/s]\rInferencing Samples:  22%|██▏       | 16/74 [00:02<00:08,  6.51 Batches/s]\rInferencing Samples:  23%|██▎       | 17/74 [00:02<00:08,  6.57 Batches/s]\rInferencing Samples:  24%|██▍       | 18/74 [00:02<00:11,  5.03 Batches/s]\rInferencing Samples:  26%|██▌       | 19/74 [00:03<00:10,  5.41 Batches/s]\rInferencing Samples:  27%|██▋       | 20/74 [00:03<00:09,  5.75 Batches/s]\rInferencing Samples:  28%|██▊       | 21/74 [00:03<00:08,  5.99 Batches/s]\rInferencing Samples:  30%|██▉       | 22/74 [00:03<00:08,  6.18 Batches/s]\rInferencing Samples:  31%|███       | 23/74 [00:03<00:08,  5.92 Batches/s]\rInferencing Samples:  32%|███▏      | 24/74 [00:03<00:08,  6.05 Batches/s]\rInferencing Samples:  34%|███▍      | 25/74 [00:04<00:07,  6.20 Batches/s]\rInferencing Samples:  35%|███▌      | 26/74 [00:04<00:07,  6.35 Batches/s]\rInferencing Samples:  36%|███▋      | 27/74 [00:04<00:07,  6.42 Batches/s]\rInferencing Samples:  38%|███▊      | 28/74 [00:04<00:07,  6.47 Batches/s]\rInferencing Samples:  39%|███▉      | 29/74 [00:04<00:06,  6.44 Batches/s]\rInferencing Samples:  41%|████      | 30/74 [00:04<00:06,  6.37 Batches/s]\rInferencing Samples:  42%|████▏     | 31/74 [00:04<00:06,  6.35 Batches/s]\rInferencing Samples:  43%|████▎     | 32/74 [00:05<00:06,  6.38 Batches/s]\rInferencing Samples:  45%|████▍     | 33/74 [00:05<00:06,  6.42 Batches/s]\rInferencing Samples:  46%|████▌     | 34/74 [00:05<00:06,  6.42 Batches/s]\rInferencing Samples:  47%|████▋     | 35/74 [00:05<00:06,  6.37 Batches/s]\rInferencing Samples:  49%|████▊     | 36/74 [00:05<00:06,  6.27 Batches/s]\rInferencing Samples:  50%|█████     | 37/74 [00:05<00:05,  6.28 Batches/s]\rInferencing Samples:  51%|█████▏    | 38/74 [00:06<00:05,  6.30 Batches/s]\rInferencing Samples:  53%|█████▎    | 39/74 [00:06<00:05,  6.34 Batches/s]\rInferencing Samples:  54%|█████▍    | 40/74 [00:06<00:05,  6.36 Batches/s]\rInferencing Samples:  55%|█████▌    | 41/74 [00:06<00:05,  6.39 Batches/s]\rInferencing Samples:  57%|█████▋    | 42/74 [00:06<00:05,  6.33 Batches/s]\rInferencing Samples:  58%|█████▊    | 43/74 [00:06<00:04,  6.29 Batches/s]\rInferencing Samples:  59%|█████▉    | 44/74 [00:07<00:04,  6.31 Batches/s]\rInferencing Samples:  61%|██████    | 45/74 [00:07<00:04,  6.35 Batches/s]\rInferencing Samples:  62%|██████▏   | 46/74 [00:07<00:04,  6.34 Batches/s]\rInferencing Samples:  64%|██████▎   | 47/74 [00:07<00:04,  6.37 Batches/s]\rInferencing Samples:  65%|██████▍   | 48/74 [00:07<00:04,  6.38 Batches/s]\rInferencing Samples:  66%|██████▌   | 49/74 [00:07<00:03,  6.37 Batches/s]\rInferencing Samples:  68%|██████▊   | 50/74 [00:07<00:03,  6.33 Batches/s]\rInferencing Samples:  69%|██████▉   | 51/74 [00:08<00:03,  6.27 Batches/s]\rInferencing Samples:  70%|███████   | 52/74 [00:08<00:03,  6.28 Batches/s]\rInferencing Samples:  72%|███████▏  | 53/74 [00:08<00:03,  6.25 Batches/s]\rInferencing Samples:  73%|███████▎  | 54/74 [00:08<00:03,  6.22 Batches/s]\rInferencing Samples:  74%|███████▍  | 55/74 [00:08<00:03,  6.23 Batches/s]\rInferencing Samples:  76%|███████▌  | 56/74 [00:08<00:02,  6.25 Batches/s]\rInferencing Samples:  77%|███████▋  | 57/74 [00:09<00:02,  6.28 Batches/s]\rInferencing Samples:  78%|███████▊  | 58/74 [00:09<00:02,  6.34 Batches/s]\rInferencing Samples:  80%|███████▉  | 59/74 [00:09<00:02,  6.32 Batches/s]\rInferencing Samples:  81%|████████  | 60/74 [00:09<00:02,  6.31 Batches/s]\rInferencing Samples:  82%|████████▏ | 61/74 [00:09<00:02,  6.30 Batches/s]\rInferencing Samples:  84%|████████▍ | 62/74 [00:09<00:01,  6.30 Batches/s]\rInferencing Samples:  85%|████████▌ | 63/74 [00:10<00:01,  6.32 Batches/s]\rInferencing Samples:  86%|████████▋ | 64/74 [00:10<00:01,  6.31 Batches/s]\rInferencing Samples:  88%|████████▊ | 65/74 [00:10<00:01,  6.32 Batches/s]\rInferencing Samples:  89%|████████▉ | 66/74 [00:10<00:01,  6.32 Batches/s]\rInferencing Samples:  91%|█████████ | 67/74 [00:10<00:01,  6.30 Batches/s]\rInferencing Samples:  92%|█████████▏| 68/74 [00:10<00:00,  6.32 Batches/s]\rInferencing Samples:  93%|█████████▎| 69/74 [00:10<00:00,  6.31 Batches/s]\rInferencing Samples:  95%|█████████▍| 70/74 [00:11<00:00,  6.30 Batches/s]\rInferencing Samples:  96%|█████████▌| 71/74 [00:11<00:00,  6.28 Batches/s]\rInferencing Samples:  97%|█████████▋| 72/74 [00:11<00:00,  6.24 Batches/s]\rInferencing Samples:  99%|█████████▊| 73/74 [00:11<00:00,  6.16 Batches/s]\rInferencing Samples: 100%|██████████| 74/74 [00:11<00:00,  6.70 Batches/s]\rInferencing Samples: 100%|██████████| 74/74 [00:11<00:00,  6.30 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 31.61 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.37 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.57 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 32.62 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.27 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.65 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.07 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.24 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.76 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 32.37 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.77 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 34.07 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 31.14 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 27.35 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\rInferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 26.21 Batches/s]\n",
            "\rInferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]\rInferencing Samples:  25%|██▌       | 1/4 [00:00<00:00,  6.61 Batches/s]\rInferencing Samples:  50%|█████     | 2/4 [00:00<00:00,  6.55 Batches/s]\rInferencing Samples:  75%|███████▌  | 3/4 [00:00<00:00,  6.53 Batches/s]\rInferencing Samples: 100%|██████████| 4/4 [00:00<00:00,  7.09 Batches/s]\rInferencing Samples: 100%|██████████| 4/4 [00:00<00:00,  6.95 Batches/s]\n",
            "2020-08-16 18:03:18.655 NumExpr defaulting to 2 threads.\n",
            "2020-08-16 18:06:50.384 Init retriever using embeddings of model deepset/sentence_bert\n",
            "2020-08-16 18:06:50.384 device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
            "2020-08-16 18:06:50.384 Could not find `deepset/sentence_bert` locally. Try to download from model hub ...\n",
            "2020-08-16 18:06:53.877 Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "2020-08-16 18:06:59.502 device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
            "\rInferencing Samples:   0%|          | 0/74 [00:00<?, ? Batches/s]\rInferencing Samples:   1%|▏         | 1/74 [00:00<00:13,  5.22 Batches/s]\rInferencing Samples:   3%|▎         | 2/74 [00:00<00:13,  5.41 Batches/s]\rInferencing Samples:   4%|▍         | 3/74 [00:00<00:12,  5.63 Batches/s]\rInferencing Samples:   5%|▌         | 4/74 [00:00<00:12,  5.82 Batches/s]\rInferencing Samples:   7%|▋         | 5/74 [00:00<00:11,  5.93 Batches/s]\rInferencing Samples:   8%|▊         | 6/74 [00:01<00:11,  5.99 Batches/s]\rInferencing Samples:   9%|▉         | 7/74 [00:01<00:11,  5.91 Batches/s]\rInferencing Samples:  11%|█         | 8/74 [00:01<00:11,  5.88 Batches/s]\rInferencing Samples:  12%|█▏        | 9/74 [00:01<00:10,  5.96 Batches/s]\rInferencing Samples:  14%|█▎        | 10/74 [00:01<00:10,  6.04 Batches/s]\rInferencing Samples:  15%|█▍        | 11/74 [00:01<00:10,  6.08 Batches/s]\rInferencing Samples:  16%|█▌        | 12/74 [00:01<00:10,  6.13 Batches/s]\rInferencing Samples:  18%|█▊        | 13/74 [00:02<00:10,  6.08 Batches/s]\rInferencing Samples:  19%|█▉        | 14/74 [00:02<00:10,  5.97 Batches/s]\rInferencing Samples:  20%|██        | 15/74 [00:02<00:09,  5.97 Batches/s]\rInferencing Samples:  22%|██▏       | 16/74 [00:02<00:09,  6.04 Batches/s]\rInferencing Samples:  23%|██▎       | 17/74 [00:02<00:09,  6.09 Batches/s]\rInferencing Samples:  24%|██▍       | 18/74 [00:02<00:09,  6.11 Batches/s]\rInferencing Samples:  26%|██▌       | 19/74 [00:03<00:09,  6.05 Batches/s]\rInferencing Samples:  27%|██▋       | 20/74 [00:03<00:09,  5.99 Batches/s]\rInferencing Samples:  28%|██▊       | 21/74 [00:03<00:08,  5.95 Batches/s]\rInferencing Samples:  30%|██▉       | 22/74 [00:03<00:08,  5.89 Batches/s]\rInferencing Samples:  31%|███       | 23/74 [00:03<00:08,  5.92 Batches/s]\rInferencing Samples:  32%|███▏      | 24/74 [00:04<00:08,  5.95 Batches/s]\rInferencing Samples:  34%|███▍      | 25/74 [00:04<00:08,  5.86 Batches/s]\rInferencing Samples:  35%|███▌      | 26/74 [00:04<00:08,  5.86 Batches/s]\rInferencing Samples:  36%|███▋      | 27/74 [00:04<00:08,  5.83 Batches/s]\rInferencing Samples:  38%|███▊      | 28/74 [00:04<00:07,  5.84 Batches/s]\rInferencing Samples:  39%|███▉      | 29/74 [00:04<00:07,  5.82 Batches/s]\rInferencing Samples:  41%|████      | 30/74 [00:05<00:07,  5.88 Batches/s]\rInferencing Samples:  42%|████▏     | 31/74 [00:05<00:07,  5.80 Batches/s]\rInferencing Samples:  43%|████▎     | 32/74 [00:05<00:07,  5.82 Batches/s]\rInferencing Samples:  45%|████▍     | 33/74 [00:05<00:07,  5.76 Batches/s]\rInferencing Samples:  46%|████▌     | 34/74 [00:05<00:06,  5.74 Batches/s]\rInferencing Samples:  47%|████▋     | 35/74 [00:05<00:06,  5.82 Batches/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqhXsfo7T_jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "publ_url = ngrok.connect(port='8503')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfBe34a9UKDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "321aee5c-4132-4af7-d7c2-4a00302dd73b"
      },
      "source": [
        "publ_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://635eda040288.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}