{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import tqdm\n",
    "import bs4\n",
    "import feedparser\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_publications = [\n",
    "    'the-artificial-impostor',\n",
    "    'pytorch',\n",
    "    'dair.ai',\n",
    "    'towards-artificial-intelligence',\n",
    "    'swlh',\n",
    "    '@ODSC',\n",
    "    'doctrine',\n",
    "    'paperswithcode'\n",
    "]\n",
    "\n",
    "\n",
    "medium_url_template = 'https://medium.com/feed/{}'\n",
    "medium_url = medium_url_template.format(medium_publications[0])\n",
    "medium_urls = [medium_url_template.format(publication) for publication in medium_publications]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    'MachineLearning',\n",
    "    'deeplearning',\n",
    "    'datascience',\n",
    "    'cognitivelinguistics',\n",
    "    'TopOfArxivSanity',\n",
    "    'kaggle'\n",
    "]\n",
    "\n",
    "reddit_url_template = 'https://www.reddit.com/r/{}/.rss'\n",
    "reddit_url = reddit_url_template.format(subreddits[0])\n",
    "reddit_urls = [reddit_url_template.format(subreddit) for subreddit in subreddits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = feedparser.parse(reddit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feed['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = feed['entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(article):\n",
    "    article_html_content = article['content'][0]['value']\n",
    "    article_text = bs4.BeautifulSoup(article_html_content).text\n",
    "    return article_text\n",
    "\n",
    "\n",
    "def get_feed_article_texts(feed):\n",
    "    return [get_article_text(article) for article in feed['entries'] if 'content' in article.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:07<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "feeds = [\n",
    "    feedparser.parse(feed_url)\n",
    "    for feed_url in tqdm.tqdm(medium_urls + reddit_urls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_texts = [\n",
    "    text \n",
    "    for feed in feeds\n",
    "    for text in get_feed_article_texts(feed)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Generate questions and answers from both real and synthetic contextsPhoto Credit(This post was originally published on my personal\\xa0blog.)Preamble“Training Question Answering Models From Synthetic Data” is an NLP paper from Nvidia that I found very interesting. Question and answer(QA) data is expansive to obtain. If we can use the data we have to generate more data, that will be a huge time saver and create a lot of new possibilities. This paper shows some promising results in this direction.Some caveats:We need big models to be able to get decent results. (The paper reported question generation models with the number of parameters from 117M to 8.3B. See the ablation study in the following sections.)Generated QA data is still not at the same level as the real data. (At least 3x+ more synthetic data is needed to reach the same level of accuracy.)There are a lot of contents in this paper, and it can be a bit overwhelming. I wrote down parts of the paper that I think is most relevant in this post, and hopefully, it can be helpful to you as\\xa0well.MethodComponentsThere are three or four stages in the data generation process. Each stage requires a separate\\xa0model:Stage 0 [Optional]\\u200a—\\u200aContext generation: The SQuAD 1.1 training data were used to train the following three stages (Figure 2 below). But when testing/generating, we can choose to use real Wikipedia data or use a model to generate Wikipedia-like data.Stage 1\\u200a—\\u200aAnswer Generation: A BERT-style model to do answer extraction from the given context. The start and the end of the token span are jointly\\xa0sampled.Stage 2\\u200a—\\u200aQuestion Generation: Fine-tuned GPT-2 model to generation question from the context and the\\xa0answer.Stage 3\\u200a—\\u200aRoundtrip Filtration: A trained extractive QA model to get the answer from the context and the generated question. If the predicted answer matches the generated answer, we keep this triplet (context, answer, and question). Otherwise, the triplet is discarded.The last step seems to be very strict. Any deviation from the generated answer will not be tolerated. However, given the EM(exact match) of the model trained on SQuAD 1.1 alone is already 87.7%, it’s reasonable to expect that the quality of answer predicted by the filtration model to be quite accurate. The paper also proposes an over-generation technique (generate two questions for each answer and context pair) to compensate for those valid triplets being discarded.(Taken from the source\\xa0paper)More DetailsContext GenerationBeside using Wikipedia documents as contexts, this paper also generates completely synthetic contexts using an 8.3B GPT-2\\xa0model:This model was first trained with the Megatron-LM codebase for 400k iterations before being fine-tuned on only Wikipedia documents for 2k iterations. This allows us to generate high-quality text from a distribution similar to Wikipedia by using top-p (p = 0.96) nucleus sampling.Answer GenerationThis paper train the answer generation model to match the exact answer in the training data. This naturally ignores the other possible answers from the context but seems to be a more generalizable way to do\\xa0it.The joint modeling of the starts and the ends of the answer span, which is reported to perform better, creates more candidates in the denominator in the calculation of the likelihood.(Taken from the source\\xa0paper)(I’m not very sure about the complexity and performance impact of this joint approach.)Question GenerationThis paper uses token type ids to identify the components in the triplets. The answer span in the context are also marked by the answer token id. Special tokens is also added to the start and the end of the questions.(Taken from the source\\xa0paper)Number of Triplets GeneratedAs explained in the previous section, the paper uses an over-generation technique to compensate for the model precision problem. Two questions are generated for each answer and context pair (a.k.a. answer candidate). Answer candidates of the context are generated by top-k sampling within a nucleus of p = 0.9 (that means we take the samples with the highest likelihoods until we either get K samples or the cumulative probabilities of the samples taken reaches\\xa00.9).(Taken from the source\\xa0paper)In the ablation study(which will be covered in the following sections), the models in stage 1 to 3 are trained with half of the SQuAD 1.1 training data, and the other half is used to generate synthetic data. The performance of the QA model trained on synthetic data is used to evaluate the quality of synthetic data.From the table above (Table 4), we can see that the smaller model on average generated 2.75 valid triplets per context, and the larger model generated 4.36 triplets. Those synthetic datasets are already bigger than the SQuAD 1.1 training\\xa0set.ExperimentsModel ScaleTable 4 (in the previous section) shows that larger models in stage 1 to 3 create better data for the downstream model, but it is not clear whether it was the quality of the data or the quantity of the data that\\xa0helped.(Taken from the source\\xa0paper)Table 5 shows that the quality of questions generated does increase as the model scales\\xa0up.(Taken from the source\\xa0paper)To test the quality of the generated answers, the paper used the 1.2B question generator (see Table 5) to generate questions without filtration from the generated answers, fine-tune a QA model and test against the dev set. Table 6 shows that bigger model increases the quality of generated answers, but only marginally.(I am not sure how they obtained the benchmark for BERT-Large, though. I think BERT-Large expects a context and question pair to generate an answer, but here we want to generate answers from only the context. Maybe they take the pre-trained BERT-Large model and fine-tune it like the other\\xa0two.)(Taken from the source\\xa0paper)In Table 7 we can see that filtration does improve the performance of the downstream models (compare to Table 5). When using real answers to generate questions, less than 50% of the triplets generated by the 345M model were rejected, while about 55% by the 1.2B model was rejected. Note that all the models in this set under-performed to the model trained with only human-generated data (SQuAD training\\xa0set).The additional triplets from using generated answers are quite helpful, the 1.2B model finally surpassed the baseline model (human-generated data), but it used 3x+ more\\xa0data.To sum up, the ablation study shows that scaling up the model improved the quality of the generated data, but the increase in the quantity of the data also played a\\xa0part.Fully Synthetic DataIn this part of the paper, they trained the models for stage 1 to 3 using the full SQuAD 1.1 training set, and use the deduplicated Wikipedia documents as contexts to generate answers and questions. They also fine-tune an 8.3B GPT-2 model on Wikipedia documents to generate synthetic contexts.(Taken from the source\\xa0paper)Table 2 shows that synthetic contexts can be as good as the real ones. Also, further fine-tuning on the real SQuAD 1.1 data can further improve the performance, which might imply that there is still something missing in the fully or partially synthetic triplets.However, using 200x+ more data to get less 1% more accuracy seems wasteful. We want to know how much synthetic data we need to reach the baseline accuracy. The next section answers this question.The Quantity of Data Required to Beat the\\xa0Baseline(Taken from the source\\xa0paper)(The “data labeled” seems to mean the size of the corpus used to generate the triplets, not the size of generated triplets.)Figure 3 shows that we need at least 50 MB of text labeled to reach the baseline accuracy (without fine-tuning with the real data), 100 MB to surpass. That’s 2.5x+ and 7x+ more than the real one used by the baseline. Considering there are multiple triplets generated by one context, the number of triplets required is estimated (by me) to be around 20x and 40x\\xa0more.The silver lining is that only 10 MB of text is needed to be labeled if we fine-tune the model with the real SQuAD data to surpass the baseline. That roughly translates to 3 to 4 times more triplets used than the baseline. So real plus synthetic data is probably the way to go for\\xa0now.Wrapping UpThere are quite a few more details I did not cover in this post. Please refer to the source paper if you want to know\\xa0more.All in all, very interesting results in this paper. Unfortunately, the amount of compute needed to synthesize the data and the amount of synthetic data needed to reach good results are still staggering. But on the other hand, it is essentially trade compute for the human labors required by the annotation process, and it might not be a bad\\xa0deal.[Notes] Training Question Answering Models From Synthetic Data was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Speeding up inference for models trained with mixed precisionPhoto Credit(This post was originally published on my personal\\xa0blog.)This is a short post describing how to use half precision in TorchScript. This can speed up models that were trained using mixed precision in PyTorch (using Apex Amps), and also some of the model trained using full precision (with some potential degradation of accuracy).TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. sourceThis repository (NVIDIA/apex) holds NVIDIA-maintained utilities to streamline mixed precision and distributed training in PyTorch. Some of the code here will be included in upstream PyTorch eventually. sourceOverviewOne thing that I managed to forget is that PyTorch itself already supports half precision computation. I wanted to speed up inference for my TorchScript model using half precision, and I spent quite some time digging around before it came to me. It doesn’t need Apex Amp to do that. What Amp does for you is patching some of the PyTorch operation so only they run in half precision (O1 mode), or keep master weights in full precision and run all other operations in half (O2 mode, see the diagram below). It also handles the scaling of gradients for you. These are all essential in mixed precision training.Mixed Precision Training\\xa0[source]But when you finished training and wants to deploy the model, almost all the features provided by Apex Amp are not useful for inference. So you don’t really need the Amp module anymore. Besides, you can not use Apex Amp in TorchScript, so you don’t really have a choice. Simply convert the model weights to half precision would\\xa0do.ExamplesBelow I give two examples of converting a model weights and then export to TorchScript.BiT-M-R101x1 Model:from bit_models import KNOW_MODELSmodel = KNOWN_MODELS[\"BiT-M-R101x1\"](head_size=100)model.eval()model.half()model.load_state_dict(torch.load(    \"../cache/BiT-M-R101x1.pth\")[\"model_states\"])with torch.jit.optimized_execution(True):    model  = torch.jit.script(model)model.save(\"../cache/BiT-M-R101x1.pt\")EfficientNet-B4 Model:import geffnetgeffnet.config.set_scriptable(True)model = geffnet.tf_efficientnet_b4_ns(pretrained=False, as_sequential=True)model.load_state_dict(torch.load(    \"../cache/b4.pth\")[\"model\"].cpu().state_dict())model.eval()with torch.jit.optimized_execution(True):    model  = torch.jit.script(model)model.save(\"../cache/b4.pt\")You’ll need to convert the input tensors. I also convert the logits back to full precision before the Softmax as it’s a recommended practice. This is what I do in the evaluation script:TL;DR version:if half:    input_tensor = input_tensor.half()probs = F.softmax(model(input_tensor).float(), -1)Full version:def collect_predictions(model, loader, half: bool):    model.eval()    outputs, y_global = [], []    with torch.no_grad():        for input_tensor, y_local in tqdm(loader, ncols=100):            batch_size = input_tensor.size(0)            aug_size = input_tensor.size(1)            if half:                input_tensor = input_tensor.half()            input_tensor = input_tensor.view(                -1, *input_tensor.size()[2:]).to(\"cuda:0\")            tmp = F.softmax(model(input_tensor).float(), -1).cpu()            probs = torch.mean(                tmp.view(batch_size, aug_size, -1),                dim=1            ).clamp(1e-5, 1-1e-5)            outputs.append(probs)            y_global.append(y_local.cpu())        outputs = torch.cat(outputs, dim=0)        y_global = torch.cat(y_global, dim=0)    return outputs, y_globalSimple Benchmarks(The model were evaluated on a private image classification dataset. The model were trained in Apex O2\\xa0mode.)Remarks:Apex (O2) and TorchScript (fp16) got exactly the same loss, as they should. The feed-forward computation are exactly the same in these two\\xa0modes.Apex (O3) is surprisingly slow. Not sure\\xa0why.Bonus: TRTorchTRTorch is a new tool developed by NVIDIA and converts a standard TorchScript program into an module targeting a TensorRT engine. With this, we will not need to export the PyTorch model to ONNX format to run model on TensorRT and speed up inference.However, TRTorch still does not support at lot of operations. Both the BiT-M-R101x1 model and the EfficientNet-B4 model failed to be compiled by TRTorch, making it’s not very useful for now. But I really like this approach, and wish this projects gain more momentum\\xa0soon.[Tip] TorchScript Supports Half Precision was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Syntax almost as friendly as Python, while running up to 100x\\xa0fasterPhoto Credit(This post was originally published on my personal\\xa0blog.)IntroductionIn my last post, [Failure Report] Distill Fine-tuned Transformers into Recurrent Neural Networks, I tried to distill the knowledge of a fine-tuned BERT model into an LSTM or GRU model without any data augmentation and failed to achieve satisfiable results. In the follow-up works, I tried to replicate the easies-to-implement augmentation method\\u200a—\\u200amasking\\u200a—\\u200aused in [1] and see its effect. The masking described in [1] is called “whole word masking” [2], that is, masking the whole word instead of just masking a single word\\xa0piece.It is non-trivial to implement whole word masking, as it would require the sampling process to be aware of which word piece is itself a whole word, and which is part of a word. As you may know, doing text processing in pure Python is quite slow comparing to other compiled languages. I recently picked up the Julia programming language, which promises the flexibility of scripting languages and the speed of compiled languages, and thought that it was a good opportunity to test Julia in the\\xa0field.This post describes the Julia code I wrote for this task and shows that for this specific task the Julia code is as simple to write as Python, while runs up to 100x faster than its pure Python counterpart.The AlgorithmThis is the algorithm I used to do whole word masking (given that the examples are already tokenized to word\\xa0pieces):For each example, mark all the word pieces that are either a whole word or the first piece of a word (by using a\\xa0mask).Randomly sample N marked pieces for each example (N is a hyper-parameter).Replacing the selected pieces with “[MASK]“.Check if the next piece is a part of this word (tokens start with “##” in BERT tokenizer). If so, also replace it with “[MASK]“.Repeat step 4 until the condition is false or the end of the example is\\xa0reached.BenchmarksNotebook used in this\\xa0section:PythonJuliaSummary(Comparing the mean run time here as the %timeit magic doesn’t provide the median run\\xa0time.)Tokenizing examples:15 seconds (shared by both Python and Julia Pipeline)Adding Special\\xa0Tokens:Python: 42 ms (estimated)Julia: 41\\xa0msMarking First\\xa0PiecesPython: 326\\xa0msJulia: 47 ms (single-threaded)Julia: 39 ms (multi-threaded)Sample One Word to\\xa0MaskPython: 8.2 s (using Numpy.random.choice)Julia: 69\\xa0msMaskingPython: 725 ms (copying the examples)Julia: 426 ms (copying the examples)Python: 300 ms (estimated)Julia: 10\\xa0msRemarksThe most time-consuming part is tokenizing the examples. So in reality optimizing the tokenizer has the most potential (That’s why huggingface has re-implemented the word-piece tokenizers in\\xa0Rust).But the eight seconds saved on sampling by switching to Julia is also a significant improvement, and just took a few lines to implement.Copying the examples takes around 300 to 500 ms, and is the most expensive operation besides tokenization. So try to avoid it if possible. (If you need the augment the same dataset multiple times, you have no choice to copy the examples.)Adding Special\\xa0TokensA simple operation that adds “[CLS]” to the head and “[SEP]” to the tail. Python and Julia are equally fast in this\\xa0one.Pythondef add_special_tokens(sentence):    sentence.insert(0, \"[CLS]\")    sentence.append(\"[SEP]\")tmp = deepcopy(sentences)for sentence in tmp:    add_special_tokens(sentence)Juliafunction add_special_tokens!(sentence)    pushfirst!(sentence, \"[CLS]\")    push!(sentence, \"[SEP]\")endtmp = deepcopy(sentences)results = add_special_tokens!.(tmp)Marking First\\xa0PiecesCreate binary masks to filter out word piece that is not the first word piece of a word. Julia is starting to outperform Python.Pythondef is_first_piece(tokens):    return [not token.startswith(\"##\") for token in tokens]first_piece_masks = [is_first_piece(sent) for sent in sentences]JuliaVectorized (single-thread) version:function is_first_piece(arr::Array{String,1})    return .!startswith.(arr, \"##\")endresults = is_first_piece.(sentences)A multi-thread version is also provided, which can sometimes be faster depending on your hardware:results = [Bool[] for _ in 1:length(sentences)]Threads.@threads for i in 1:length(sentences)   results[i] = is_first_piece(sentences[i])endSamplingRandomly sample one word from each example to be masked. Since I can’t think of any simple way to vectorized this in Python, a naive for-loop approach is used. Vectorizing in Julia, on the other hand, is fairly straight-forward. As a result, the Julia version is vastly faster (100x) than the Python\\xa0one.Note: I used Numpy in the Python implementation, so it’s not really “pure python” in this\\xa0case.Pythondef sample(first_piece_masks, n=1):    results = []    for mask in first_piece_masks:        if sum(mask) <= n:            results.append([])            continue        probabilities = np.asarray(mask) / float(sum(mask))        results.append(np.random.choice(np.arange(len(mask)), size=n, p=probabilities))    return resultsmasking_points =  sample(first_piece_masks)Juliausing StatsBasefunction sample_mask_position(first_piece_mask, n=1)    if sum(first_piece_mask) <= n        return Int64[]    end    return sample(1:length(first_piece_mask), Weights(first_piece_mask), n, replace=false)endmasking_points = sample_mask_position.(first_piece_masks)MaskingFull word masking. This one inevitably has to use some loop to scan the example. For loops are not a problem for Julia, so the Julia version is much faster (30x) than\\xa0Python.The implementation presented here copies the examples inside the function so the original examples can be augmented multiple\\xa0times.Pythondef masking(rows, first_piece_masks, masking_points):    augmented_rows = deepcopy(rows)    for idx in range(len(masking_points)):        for pos in masking_points[idx]:            augmented_rows[idx][pos] = \"[MASK]\"            while pos +1 < len(first_piece_masks[idx]) and first_piece_masks[idx][pos + 1] == 0:                pos += 1                augmented_rows[idx][pos] = \"[MASK]\"    return augmented_rowsaugmented_sentences = masking(sentences, first_piece_masks, masking_points)Juliafunction masking(rows::Vector{Vector{String}}, first_piece_masks::Vector{Vector{Bool}}, masking_points::Vector{Vector{Int64}})    augmented_rows = deepcopy(rows)    for idx in 1:length(masking_points)        for pos in masking_points[idx]            augmented_rows[idx][pos] = \"[MASK]\"            while pos + 1 <= length(first_piece_masks[idx]) && first_piece_masks[idx][pos + 1] == 0                pos += 1                augmented_rows[idx][pos] = \"[MASK]\"            end        end    end    return augmented_rowsendaugmented_sentences = masking(sentences, first_piece_masks, masking_points)ConclusionThis is the first time I integrate Julia in an NLP pipeline, and the results are encouraging. The easy of development of Julia is on the same level as Python, but the is on a totally different level. In this example, the most improvement in speed comes from the sampling process, but it only represents less than 40 % of the total run time. And the total run time in Python is relatively short. I look forward to seeing what kind of speedup Julia can bring in bigger datasets or more complicated tasks.(The notebook actually used in the pipeline).ReferencesTang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., & Lin, J. (2019). Distilling Task-Specific Knowledge from BERT into Simple Neural Networks.BERT: New May 31st, 2019: Whole Word Masking\\xa0ModelsUsing Julia to Do Whole Word Masking was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'A Case\\xa0StudyPhoto Credit(This post was originally published on my personal\\xa0blog.)IntroductionAWS recently released TorchServe, an open-source model serving library for PyTorch. The production-readiness of Tensorflow has long been one of its competitive advantages. TorchServe is PyTorch community’s response to that. It is supposed to be the PyTorch counterpart of Tensorflow Serving. So far, it seems to have a very strong\\xa0start.This post from the AWS Machine Learning Blog and the documentation of TorchServe should be more than enough to get you started. But for advanced usage, the documentation is a bit chaotic and the example code suggests sometimes conflicting ways to do\\xa0things.This post is not meant to be a tutorial for beginners. Instead, it uses a case study to show the readers what a slightly more complicated deployment looks like, and saves the readers’ time by referencing relevant documents and example\\xa0code.In this post,we will deploy an EfficientNet model from the rwightman/gen-efficientnet-pytorch repo. The server accepts images as arrays in Numpy binary format and returns the corresponding class probabilities. (The reason for using Numpy binary format is that in this use case the images are already read into memory on the client-side, the network bandwidth is cheap and we don’t have strict latency requirements, so re-encoded it into JPEG or PNG format doesn’t make\\xa0sense.)Preparing the EfficientNet ModelTorchServe can load models from PyTorch checkpoints (.state_dict()) or exported TorchScript programs. I’d recommend using TorchScript when possible, as it doesn’t require you to install extra libraries (e.g., gen-efficientnet-pytorch) and provide a model definition file.Luckily, rwightman/gen-efficientnet-pytorch already provides an easy API to create TorchScript-compatible models. In this case, the model has already been trained and saved via torch.save(model). We need to load it using torch.load, create an untrained TorchScript-compatible model, and transfer the\\xa0weights:geffnet.config.set_scriptable(True)model_old = torch.load(    \"cache/b4-checkpoint.pth\")[\"model\"].cpu()model, _ = get_model(    arch=\"b4\", n_classes=6)model.load_state_dict(model_old.state_dict())del model_oldwith torch.jit.optimized_execution(True):    model  = torch.jit.script(model)model.save(\"cache/b4.pt\")(The above code was inspired by this script in the gen-efficientnet-pytorch repo.)Please note that the geffnet.config.set_scriptable(True) line is essential. Without it the model won’t be able to be compiled with TorchScript.The Custom\\xa0HandlerTorchServe comes with four default handlers that define the input and output of the deployed service. We are deploying an image classification model in this example, and the corresponding default handler is image_classifier. If you read its source code, you’ll find that it accepts a binary image input, resize, center crop, and normalize it, and returns the top 5 predicted classes. Most of these don’t fit our use case, so we’ll have to write our own handler. You can refer to this documentation on how to create non-standard services.Batch InferenceIn this example, we’ll have thousands of images per minute to predict, so batch processing is essential. For more information on batch inference with TorchServe, please refer to this documentation.Final ResultThe code is based on the resnet_152_batch example, with some simplification (e.g., we don’t need to handle PyTorch checkpoints). By the way, the MNIST example used a confusing way to load model and model file, the one in resnet_152_batch makes much more sense (by using the manifest[\\'model\\'][\\'serializedFile\\'] and manifest[\\'model\\'][\\'modelFile\\'] property).Highlights:Load the model from TorchScript program (Line\\xa030).Load the image from an array in numpy binary format: input_image = Image.fromarray(np.load(io.BytesIO(image))) (Line\\xa059).Test Time Augmentation (TTA) in Line 48 to 53, Line 60 to 61, and Line 84 to 87 (horizontal flip).https://medium.com/media/f23c027c15cb27d352c0617dbed8be41/hrefThis handler cannot handle malformed inputs (as all the example handlers I’ve seen). If that’s inevitable in your use case, you’ll probably need to find some way to identify those inputs, ignore that in the inference method, and return proper error messages in the postprocess method.DeploymentCreate the Model\\xa0ArchiveTorchServe requires the user to package all model artifacts into a single model archive file. It’s fairly straight-forward in our case. Please refer to the documentation if in\\xa0doubt.mkdir model-storetorch-model-archiver --model-name b4 --version 1.0 \\\\    --serialized-file cache/b4.pt --handler handler.py \\\\    --export-path model-storeStart the TorchServe serviceI use a shell script to start the TorchServe server and register the\\xa0model:torchserve --start --model-store model-store --ts-config config.properties > /dev/nullsleep 3curl -X DELETE http://localhost:8081/models/b4curl -X POST \"localhost:8081/models?model_name=b4&url=b4.mar&batch_size=4&max_batch_delay=1000&initial_workers=1&synchronous=true\"batch_size = 4: because of the TTA, the effective batch size is actually 8. I’ve noticed that the maximum batch size is smaller in TorchServe than directly do the inference in a Python script. I’m not sure the reason why this is the\\xa0case.max_batch_delay=1000: wait at most 1 second for the batch to be filled. You can adjust this according to your latency requirements.My config.properties file contains:async_logging=truevmargs=-Dlog4j.configuration=log4j.propertiesFor now, log4j.properties is an exact copy of the one used by\\xa0default.At this point, your TorchServe service should be up and\\xa0running.If you updated your model, create the model archive at the same path, and rerun the shell script will automatically reload the model on the\\xa0server.Stop the server by running torchserve --stop.Client Requests\\xa0ExampleHere’s an example of making multiple requests to the server via\\xa0asyncio:async def predict_batch(cache):    buffer = []    for img in cache[\"images\"]:        output = io.BytesIO()        np.save(output, img)        output.seek(0, 0)        buffer.append(output)    loop = asyncio.get_event_loop()    executor = concurrent.futures.ThreadPoolExecutor(max_workers=8)    responses = await asyncio.gather(*[        loop.run_in_executor(            executor, requests.post, INFERENCE_ENDPOINT, pickled_image        ) for pickled_image in buffer    ])    probs = [res.json() for res in responses]    return probsConclusionThanks for reading! I hope this post makes it easier for you to understand and use TorchServe. TorchServe creates an API for your model and does most of the heavy-lifting involved in handling HTTP requests. It shows great promise in the production environment support of\\xa0PyTorch.If you have any suggestions, please feel free to leave a\\xa0comment.Deploying EfficientNet Model using TorchServe was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Analyze and Optimize TensorFlow Performance on\\xa0GPUPhoto Credit(This article was first published on my personal\\xa0blog.)IntroductionThe Tensorflow Profiler in the upcoming Tensorflow 2.2 release is a much-welcomed addition to the ecosystem. For image-related tasks, often the bottleneck is the input pipeline. But you also don’t want to spend time optimizing the input pipeline unless it is necessary. The Tensorflow Profiler makes pinpointing the bottleneck of the training process much easier, so you can decide where the optimization effort should be put\\xa0into.An Input-Bound Example.\\xa0SourceThe official documentation demonstrates how to use the profiler with the Keras interface via a callback(tf.keras.callbacks.TensorBoard). However, there are no mentions of custom training loops. I did some research and came up with a working solution, which will be described in this post, along with some obstacles I had met and how I overcame\\xa0them.PreparationInstall the Latest Tensorflow and the Profiler\\xa0PluginThis comes directly from the documentation:# Uninstall twice to uninstall both the 1.15.0 and 2.1.0 version of TensorFlow and TensorBoard.pip uninstall -y -q tensorflow tensorboardpip uninstall -y -q tensorflow tensorboardpip install -U -q tf-nightly tb-nightly tensorboard_plugin_profile(This will no longer be required once Tensorflow and TensorBoard 2.2 are released)NVIDIA GPU Libraries(This section is for training on a single GPU. For training on multiple GPUs, please refer to this\\xa0guide.)You’ll need to install NVIDIA GPU drivers and CUDA Toolkit as you normally do when training models on\\xa0GPU.The next step is more specifically for the profiler. First, make sure that CUPTI 10.1 exists on the path (source):/sbin/ldconfig -N -v $(sed \\'s/:/ /g\\' <<< $LD_LIBRARY_PATH) | grep libcuptiIf not, update the LD_LIBRARY_PATH environment variable:export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATHTroubleshoot: CUPTI_ERROR_INSUFFICIENT_PRIVILEGESYou’ll likely see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES and CUPTI_ERROR_INVALID_PARAMETER errors in the log when trying to profile your model. This is because NVIDIA GPU performance counters, when running on one of the newer drivers, is only available to system administrators.Please read this document from NVIDIA to find a solution to your\\xa0system.For my Linux system, the recommended modprobe nvidia NVreg_RestrictProfilingToAdminUsers=0 does not work. An alternative solution, which writes a file to /etc/modprobe.d, works for me. It is also offered in this Github\\xa0thread:Adding options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\" to /etc/modprobe.d/nvidia-kernel-common.conf and reboot should resolve the permission issue.Profile the Training\\xa0LoopThis guide(Profile Tensorflow performance) describes four ways to collect performance data. One of them is specific to Keras interface. Another one(sampling mode) is interactive through Tensorboard web UI. I’ll describe the two that works programmatically and are compatible with custom training\\xa0loops.Using tf.profiler Function\\xa0API:tf.profiler.experimental.start(\\'logdir\\')# Train the model heretf.profiler.experimental.stop()2. Using Context\\xa0Manager:with tf.profiler.experimental.Profile(\\'logdir\\'):    # Train the model here    passThere is one additional way. By reading the source code of the Keras Tensorboard callback, I reconstructed the Tensorflow Profiler part in the callback\\xa0as:from tensorflow.python.profiler import profiler_v2 as profilerprofiler.warmup()profiler.start(logdir=\\'logdir\\')# Train the model hereprofiler.stop()A Working\\xa0ExampleHere is an example that trains an Efficientnet-B3 model and collect performance data using two different ways(with no obvious differences in results):overview_pageNotice that the Device Compute Precisions indicates that 87.6% of the GPU time was spent in 16-bit computation, showing that the mixed-precision training is configured correctly. Judging from the graph, the GPU is well fed with basically no time spent on waiting for input (I enabled prefetch in the data pipeline, so this tells us that it hadn’t run out of the prefetched batches).The input_pipeline_analyzer page shows that most time on host(CPU side) is spent on data preprocessing, so disk IO doesn’t seem to be a\\xa0problem:input_pipeline_analyzerThe kernel_stats page shows that 25% of the time is spent on SwapDimension1And2InTensor3UsingTiles. I’m not sure swapping dimensions should take up that much time (doesn’t seem so). Some more research is required to answer that. The page also provides a helpful indication of whether an Op is Tensor Core eligible and whether Tensor Cores were actually\\xa0used:kernel_statsThe notebook used (I used my library tf-helper-bot to wrap my custom training loop in a Fast.ai-inspired API.\\xa0):https://medium.com/media/50af2c437d3da78615887a96ce5f916b/hrefConclusionThanks for reading! Hopefully this post shows to you that Tensorflow Profiler is a powerful and easy-to-use tool (once you overcome the installation hurdles) that can potentially save you tons of\\xa0time.This post only covers part of the profiler capabilities. There are a lot of things I don’t fully understand yet. The profiling report should give you some sense of where to look. I’d love to know if you found any other interesting resources on this topic (leave a comment!).Tensorflow Profiler with Custom Training Loop was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Case Study: Google QUEST Q&A Labeling CompetitionPhoto Credit(This is the first half of this article on my personal\\xa0blog.)Executive SummaryTensorFlow has become much easier to use: As an experience PyTorch developer who only knows a bit of TensorFlow 1.x, I was able to pick up TensorFlow 2.x in my spare time in 60 days and do competitive machine learning.TPU has never been more accessible: The new interface to TPU in TensorFlow 2.1 works right out of the box in most cases and greatly reduces the development time required to make a model TPU-compatible. Using TPU drastically increases the iteration speed of experiments.We present a case study of solving a Q&A labeling problem by fine-tuning the RoBERTa-base model from huggingface/transformer library:CodebaseColab TPU training\\xa0notebookKaggle Inference KernelHigh-level library TF-HelperBot to provide more flexibility than the Keras interface.(TensorFlow 2.1 and TPU are also a very good fit for CV applications. A case study of solving an image classification problem will be published in about a\\xa0month.)AcknowledgmentI was granted free access to Cloud TPUs for 60 days via TensorFlow Research Cloud. It was for the TensorFlow 2.0 Question Answering competition. I chose to do this simpler Google QUEST Q&A Labeling competition first but unfortunately couldn’t find enough time to go back and do the original one (sorry!).I was also granted $300 credits for the TensorFlow 2.0 Question Answering competition and had used those to develop a PyTorch baseline. They also covered the costs of Cloud Compute VM and Cloud Storage used to train models on\\xa0TPU.IntroductionGoogle was handing out free TPU access to competitors in the TensorFlow 2.0 Question Answering competition, as an incentive for them to try out the newly added TPU support in TensorFlow 2.1 (then RC). Because the preemptible GPUs on GCP are barely usable at the time, I decided to give it a shot. It all began with this\\xa0tweet:\\u200a—\\u200a@ceshine_enTurns out that the TensorFlow model in huggingface/transformers library can work with TPU without modification! I then proceeded to develop models using TensorFlow(TF) 2.1 for a simpler competition Google QUEST Q&A Labeling.I missed the post-processing trick in the QUEST competition because I spent most of my limited time wrestling with TF and TPU. After applying the post-processing trick, my final model would be somewhat competitive at around 65th place (silver medal) on the final leaderboard. The total training time of my 5-fold models using TPUv2 on Colab was about an hour. This is a satisfactory result in my opinion, given the time constraint.TensorFlow 2.xThe TensorFlow 2.x has become much more approachable, and the customizable training loops provide a swath of opportunities to do creative things. I think I’ll be able to re-implement top solutions of the competition in TensorFlow without banging my head on the door (at least less frequently).On the other hand, TF 2.x is still not as intuitive as PyTorch. Documentation and community support still have much to be desired. Many of the search results still point to TF 1.x solutions that do not apply to TF\\xa02.x.As an example, I ran into this problem in which the CuDNN failed to initialize:\\u200a—\\u200a@ceshine_enOne of the solutions is to limit the GPU memory usage, and here’s a confusingly long thread on how to do\\xa0so:\\u200a—\\u200a@ceshine_en(I know that PyTorch has its own TPU support now, but it is still quite hard to use last time I checked, and it is not supported in Google Colab. Maybe I’ll take another look in the next few\\xa0weeks.)Case Study and Code\\xa0SnippetsThis section will briefly describe my solution to the QUEST Q&A Labeling competition, and discuss some parts of the code that I think are most helpful for those to come from PyTorch as I did. This section assumes that you already have a basic understanding of TensorFlow 2.x. If you’re not sure, please refer to the official tutorial Effective TensorFlow 2.Source CodeCodebaseColab TPU training\\xa0notebookKaggle Inference KernelHigh-level library TF-HelperBot to provide more flexibility than the Keras interface.RoadmapTF-Helper-Bot: this is a simple high-level wrapper of TensorFlow I wrote to improve code reusability.Input Formulation and TFRecords Preparation.TPU-compatible Data\\xa0Loading.The Siamese Encoder\\xa0Network.There’s more! Read the full post on my personal\\xa0blog.TensorFlow 2.1 with TPU in Practice was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Create a Customized Text Annotation Tool in Two Days\\u200a—\\u200aPart\\xa02Building a React Front-end InterfacePhoto Credit(This is a republication of this post on my personal\\xa0blog.)IntroductionIn Part 1 of this series, we’ve discussed why building your own annotation tool can be a good idea, and demonstrated a back-end API server based on FastAPI. Now in this Part 2, we’re going to build a front-end interface that interacts with the end-user (the annotator). The front-end needs to do mainly three\\xa0things:Fetch a batch of sentence/paragraph pairs to be annotated from the back-end\\xa0server.Present the pairs to the annotator and provide a way for them to adjust the automatically generated labels.Send the annotated results to the back-end\\xa0server.Disclaimer: I’m relatively inexperienced in front-end development. The code here may seem extremely amateur to professionals. However, I hope this post can serve as a reference or starting point for those with similar requirements.PrerequisitesThis post assumes you have basic understandings of Javascript, React, HTML, and CSS/Sass. If you don’t, please refer to the learning resources mentioned in Part\\xa01.You need to install npm on your system (one recommended way is via nvm), and create a new React project using the Create React\\xa0App.We’ll be using the Bulma CSS framework. The way we used to integrate Bulma into React is to install the bulma, node-sass, and react-bulma-components package (via npm). We’ll also be using the immutable package to store the states in\\xa0React.The following section will be built on the created\\xa0project.Code-throughLet’s take another look at what we’ll be getting at the end of this\\xa0section:The source code for this post can be found at veritable-tech/text-annotation-react-frontend.veritable-tech/text-annotation-react-frontendPage LayoutThe overall page layout is defined in the render method in src/App.js:render() {  return (    <section className=\"section\">      <div className=\"container\">        <section className=\"section\">        <Button color=\"primary\" onClick={this.fetchBatch}>            Fetch Page        </Button>        {this.state.page !== null ? (            <Button color=\"info\" onClick={this.submitBatch}>            Submit Changes            </Button>        ) : (            \"\"        )}        {this.state.page !== null ? (            <span className=\"subtitle\">Page {this.state.page + 1}</span>        ) : (            \"\"        )}        </section>        <Entries        page={this.state.page}        pairs={this.state.pairs}        changeScore={this.changeScore}        />      </div>    </section>  );}There are two buttons, one for fetching the page and one for submitting the changes. The submit button will only be displayed when a batch/page has been\\xa0fetched.We’ll create an Entries React component that is responsible for displaying the pairs and also collecting the annotations. The two states in the App component\\u200a—\\u200apage and pairs\\u200a—\\u200aare passed to the Entries component as properties. There is also a function changeScore that is passed to handle the changes in\\xa0labels.The App ComponentThis is the main React component that every workflow will go through. We start by initializing the application state:class App extends Component {  constructor() {    super();    this.state = {      page: null,      pairs: null    };    // These two methods will be added soon in the following sections    this.fetchBatch = this.fetchBatch.bind(this);    this.submitBatch = this.submitBatch.bind(this);  }}Fetching a\\xa0BatchWe use the Fetch\\xa0API:async function getBatch() {  const res = await fetch(SERVER_ENDPOINT + \"batch/\", {    method: \"GET\", // *GET, POST, PUT, DELETE, etc.    mode: \"cors\", // no-cors, cors, *same-origin    cache: \"no-cache\", // *default, no-cache, reload, force-cache, only-if-cached    redirect: \"follow\", // manual, *follow, error    referrer: \"no-referrer\", // no-referrer, *client    credentials: \"include\"  });  if (!res.ok) {    throw Error(res.statusText);  }  return res;}The credentials: \"include\" is necessary to make sure fetch sends the cookies to the back-end, since the where the front-end is hosted is not always the same as the back-end (therefore will have different origins). The mode: \"cors\" part is for the same\\xa0reason.The error handling in this function is very bare-bone, and will require you to use the developer console in browser to detect the errors. Nonetheless, this interaction is fairly simple and will almost never go wrong if your server and browser is on the same machine or in the same network. You can improve the error handling if you’re dealing with more complicated scenarios.The following method of the App component call the getBatch function and set the application states afterwards:async fetchBatch() {  const res = await getBatch();  const data = await res.json();  const pairs = data.pairs.map(x => [    x[0], // text_1    x[1], // text_2    x[2], // raw score    x[3], // adjusted score    Math.round(x[3] * 4) / 4 // transformed label  ]);  this.setState({    page: data.page,    pairs: fromJS(pairs)  });}The Math.round(x[3] * 4) / 4 part automatically transform the continuous predictions (with range [0, 1]) from the model into discrete labels {0, 0.25, 0.5, 0.75, 1.}, which will be displayed as {1, 2, 3, 4, 5} in the Entires component.The Entries and Entry ComponentThe Entries component is fairly simple. It just goes through the pairs property and creates one Entry component for each\\xa0pair:class Entries extends Component {  render() {    if (this.props.pairs === null) {      return <div></div>;    }    return this.props.pairs.map((value, idx) => (      <Entry        row={value}        idx={idx}        key={idx}        changeScore={this.props.changeScore}      />    ));  }}The Entry component is where the main UI\\xa0lives:const Entry = props => {  return (    <div className=\"columns\">      <div className=\"column\">{props.row.get(1)}</div>      <div className=\"column\">{props.row.get(2)}</div>      <div className=\"column\">        <Button          className={props.row.get(4) === 0 ? \"is-dark\" : \"is-light\"}          onClick={props.changeScore(props.idx, 0)}          disabled={props.row.get(4) === 0}        >          1        </Button>        {/* Code for other buttons is ignored to save space */}        <span>({props.row.get(3).toFixed(4)})</span>      </div>    </div>  );};The code should be quite straightforward. Let me know in the comment if any of the above is unclear to\\xa0you.The props.row.get(3).toFixed(4) part is the adjusted similarity score from the model and is displayed for reference (mostly for debugging).The only “moving part” in the sub-section is the changeScore function/method that is called when a user clicks on one of the activated label\\xa0buttons:changeScore = (i, score) => () => {  this.setState({    pairs: this.state.pairs.set(i, this.state.pairs.get(i).set(4, score))  });};What it does is updating the label of the ith pair. Because we’re using immutable objects, what actually happens is a new pairs List object is created to replace the old\\xa0one.Submitting the\\xa0ResultsNow we’re at the final piece of the puzzle. The postBatch function is very similar to the getBatch function, except for the method and body parameters:async function postBatch(batch) {  const res = await fetch(SERVER_ENDPOINT + \"batch/\", {    method: \"POST\", // *GET, POST, PUT, DELETE, etc.    mode: \"cors\", // no-cors, cors, *same-origin    cache: \"no-cache\", // *default, no-cache, reload, force-cache, only-if-cached    redirect: \"follow\", // manual, *follow, error    referrer: \"no-referrer\", // no-referrer, *client    credentials: \"include\",    body: JSON.stringify(batch)  });  if (!res.ok) {    throw Error(res.statusText);  }  return res;}And the submitBatch method of the App component prepares the payload for the fetch request, and display an alert to the user when the submission has been successfully accepted by the back-end\\xa0server.async submitBatch() {  const payload = {    page: this.state.page,    pairs: this.state.pairs.map(x => [x.get(0), x.get(4)])  };  const res = await postBatch(payload);  const data = await res.json();  console.log(data);  alert(`Submit Success: ${data.success} ${data.message}`);}This is just the BeginningHere we conclude our journey of building a customized annotation tool. As you can see, it’s not as hard as you might think. Almost all the changes I’ve made to the base React project have been fit inside this single blog post! And the 2,000 annotations I’ve made via this tool can testify that it works well enough (the actual number at the point of writing is approaching 3,000).There is a lot of space for improvement, of course. In a lot of cases, you can just use the modifiers of Bulma to make your UI look better. You can also write your own CSS like I briefly did in the App.scss file. You can add a page selector as I mentioned in Part 1. You can implement an account-based user management system. It’s all up to your specific use case and imagination.I hope this series has been helpful to you, and thank you for reading all the way to this point. If you have any specific questions or recommendations, please let me know in the comment\\xa0section.Create a Customized Text Annotation Tool in Two Days\\u200a—\\u200aPart 2 was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Create a Customized Text Annotation Tool in Two Days\\u200a—\\u200aPart\\xa01Building a Back-end API Server with Multi-user SupportPhoto Credit(This is a republication of this post on my personal\\xa0blog.)IntroductionIn my previous post, Fine-tuning BERT for Similarity Search, I mentioned that I annotated 2,000 pair of sentence pairs, but did not describe how I did it and what tool I used. Now in this two-part series, we’ll see how I created a customized text annotation tool that greatly speeds up the annotation process.The entire stack was developed in two days. You can probably do it a lot faster if you are familiar with the technology (the actual time I spent on it is about 6 hours\\xa0top).Why Build Your Own Annotation ToolYou might ask, why build your own tool? Why not just use Excel or open-source/proprietary tools? There are several\\xa0reasons:The Excel UI is not necessarily the best fit for test reading and annotation.You have to do all the data management inside Excel, including shuffling, sampling, or splitting the dataset. All of them are tedious manual labor and prone to\\xa0errors.Open-source tools can be overly complicated for your use case, and also requires a lot of work to customize.You might not want to pay for the proprietary tool if the scale of your annotation work is relatively small.Learning\\u200a—\\u200ayou can see this as an opportunity to practice and improve your web development skills. (More details in the next section.)The customized annotation tool I built does these\\xa0things:Manage user sessions\\u200a—\\u200ause signed cookies to identify users, provide different sets of pairs for annotation, and keep track of annotated pairs.Editable annotations\\u200a—\\u200ausers can submit the results, make some changes afterward, and submit them again. No duplicated entries will be generated.Each submission is stored in a CSV file, with a timestamp field\\u200a—\\u200ayou can easily pick out annotations from a specific time range that could be problematic.Automatically pick a score based on the model prediction\\u200a—\\u200athe annotator only needs to adjust the model prediction. Also, it gives you an idea of how the model performs on unseen\\xa0entries.Our Annotation Interface. Need some work aesthetically, but useful enough\\xa0IMO.Inspiration and Learning ResourcesI don’t remember exactly where I got the idea of a React front-end working with a Python back-end from. I think it was from a tweet by Ines Montani, but here’s a more recent reminder by Joel\\xa0Grus:\\u200a—\\u200a@joelgrusHaving React in your arsenal is very empowering. You don’t need to attend a boot camp or spend a couple of months. Just learn the basics and start building stuffs. Granted, the results can be very ugly (in terms of both presentation and code), but as long as it works reasonably well, it’s still better than nothing. (You’ll still probably need professional help if you want to build any customer-facing applications.)The learning resources in the following Twitter thread by Ines Montani are very\\xa0helpful:\\u200a—\\u200a@_inesmontaniThe API\\xa0ServerWe’re going to use FastAPI to build our back-end API server. It’s created by Sebastián Ramírez (tiangolo) and is built on top of Starlette. The other popular alternative is to use Flask. I had experience using both, and IMO FastAPI is better if you just want to build an API server (without any no front-end logic).The source code can be found at veritable-tech/text-annotation-api-server/server.py. The codebase could use some refactoring, but this state represents how I quickly put together all the necessary parts in the first\\xa0place.veritable-tech/text-annotation-api-serverCreating Session(I’m going to skip the non-essential lines here to save space. Please refer to the source code for the full implementation).First, we need to create the app object and add a session middleware:from starlette.middleware.sessions import SessionMiddlewarefrom fastapi import FastAPIAPP = FastAPI()APP.add_middleware(    SessionMiddleware,    secret_key=\"PUT_YOUR_SECRET_KEY_HERE\")The secret_key is used to sign the cookies, which makes sure their contents are not tampered\\xa0with.The next step is to create a GET endpoint for retrieving a batch/set of sentence pairs to annotate. At the start of the function, we check if we’ve seen this user. If not, we create a new user ID for them and save the ID to the associated session:@APP.get(\"/batch/\", response_model=BatchForAnnotation)def get_batch(request: Request):    if not request.session or request.session[\"uid\"] not in GLOBAL_CACHE:        print(\"Creating new indices...\")        request.session[\"uid\"] = str(uuid.uuid4())        # Shuffle the dataset (via shuffling the index)        indices = np.arange(len(DATASET))        np.random.shuffle(indices)        # Initialize the user states        GLOBAL_CACHE[request.session[\"uid\"]] = {            \"indices\": indices,            \"submitted\": {}        }Since I only run the annotation tool locally and on a small scale, I keep all the application states in a global dictionary object GLOBAL_CACHE. For a more robust solution, you can consider using something like Redis to store the\\xa0states.The following is an example of how we use the user ID in session data and the application states to reject the POST request from users who have not fetched any batch\\xa0yet:@APP.post(\"/batch/\", response_model=SubmitResult)def submit_batch(batch: BatchAnnotated, request: Request):    if (        not request.session.get(\"uid\") or        not request.session.get(\"uid\") in GLOBAL_CACHE    ):        return SubmitResult(            success=False,            overwrite=False,            message=\"You haven\\'t fetched any batches yet.\"        )Loading the\\xa0DatasetThis is how I prepare the dataset: I have 8,000+ very short paragraphs (in Traditional Chinese), and an existing baseline model (Multilingual Universal Sentence Encoder or a fine-tuned model). I use the model to collect the 20 ~ 30 most similar paragraphs and another 20 ~ 30 random paragraphs for each paragraph. I save the result into a CSV file with four fields\\u200a—\\u200a“text_1”, “text_2”, “similarity”, “similarity_raw”. (The “similarity_raw” field is the score before the optional linear transformation. It is there just for reference.)The CSV file is loaded before launching the application:if __name__ == \\'__main__\\':    parser = argparse.ArgumentParser()    arg = parser.add_argument    arg(\\'--data-path\\', type=str, default=\"data/dataset.csv\")    args = parser.parse_args()    DATASET = pd.read_csv(args.data_path)    print(f\"Listening to port {PORT}\")    uvicorn.run(APP, host=\\'0.0.0.0\\', port=PORT)Preparing a\\xa0BatchThis is how each batch is prepared:def find_page_to_annotate(cache_entry: Dict):    for page in range(int(math.ceil(len(DATASET) / PAIRS_PER_PAGE))):        if page not in cache_entry[\"submitted\"]:            return page    return Noneclass BatchForAnnotation(BaseModel):    page: int    pairs: List[Tuple[int, str, str, float]] = []@APP.get(\"/batch/\", response_model=BatchForAnnotation)def get_batch(request: Request):    # Skipped ...    cache_entry = GLOBAL_CACHE[request.session[\"uid\"]]    page = find_page_to_annotate(cache_entry)    if page is None:        return BatchForAnnotation(            page=-1,            pairs=[]        )    batch = DATASET.loc[        cache_entry[\"indices\"][            page*PAIRS_PER_PAGE:(page+1)*PAIRS_PER_PAGE        ]    ]    pairs = list(batch[        [\"text_1\", \"text_2\", \"similarity\"]    ].itertuples(index=True, name=None))    print(f\"Page: {page} Items: {len(pairs)}\")    return BatchForAnnotation(        page=page,        pairs=pairs    )The GLOBAL_CACHE[uid][\"submitted\"] stores page -> output_path key/value pairs. The output_path points to a CSV file where a user-submitted batch of annotations has been saved\\xa0to.The find_page_to_annotate function finds the first page that hasn’t been annotated yet.Accepting a Batch SubmissionWhen a user finished annotating the received batch, they submit the results via a POST request. Only the ID of the pair and the annotated similarity score is submitted (as we already have the corresponding text in the\\xa0memory):class SubmitResult(BaseModel):    success: bool    overwrite: bool    message: strclass BatchAnnotated(BaseModel):    page: int    pairs: List[Tuple[int, float]] = []@APP.post(\"/batch/\", response_model=SubmitResult)def submit_batch(batch: BatchAnnotated, request: Request):    # Skipped...    page = batch.page    batch_orig = DATASET.loc[        GLOBAL_CACHE[request.session[\"uid\"]][\"indices\"][            page*PAIRS_PER_PAGE:(page+1)*PAIRS_PER_PAGE        ]    ].copy()    indices, similarities = list(zip(*batch.pairs))    # Skipped the data validation block...    batch_orig.loc[indices, \"similarity\"] = similarities    batch_orig[\"timestamp\"] = int(datetime.now().timestamp())    overwrite = False    if page in GLOBAL_CACHE[request.session[\"uid\"]][\"submitted\"]:        # Overwrite a submitted batch        output_path = GLOBAL_CACHE[request.session[\"uid\"]][\"submitted\"][page]        overwrite = True    else:        # This is a new batch        output_path = OUTPUT_DIR / \\\\            f\"{datetime.now().strftime(\\'%Y%m%d_%H%M\\')}_{page}.csv\"        GLOBAL_CACHE[request.session[\"uid\"]][\"submitted\"][page] = output_path    batch_orig.to_csv(output_path, index=False)    return SubmitResult(        success=True,        overwrite=overwrite,        message=\"\"    )The code should be quite straightforward to read. We create a copy of a slice of the Pandas data frame, do some data validation to make sure the pair IDs and the page are matched, and update the slice with the submitted labels. If the submitted page already has been submitted before, we overwrite the previous output; if not, we create a new output\\xa0file.Potential ImprovementsWe now already have a functioning back-end server with just about 150 lines of code. There are many potential improvements, including:Specify which page to fetch in the GET request. (Currently, we can only fetch the pages that have not received any submissions.)Store the user ID in the output\\xa0file.Use a persistent store for the application states, so an application restart will not erase previous\\xa0records.Customizable page size at the start of the\\xa0session.Some of these are relatively easy to implement. Readers are encouraged to implement them as exercises.To Be ContinuedWe’ve covered the back-end API server in this post. In the next post, we’ll describe how to write a fairly basic React front-end to interact with both the user and the back-end\\xa0server.Read the Part 2\\xa0here.Create a Customized Text Annotation Tool in Two Days\\u200a—\\u200aPart 1 was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Fine-tuning BERT for Similarity SearchBERT as Sentence Encoder is Surprisingly Sample-EfficientPhoto Credit(This is a republication of this post on my personal\\xa0blog.)SynopsisI have the task of finding similar entries among 8,000+ pieces of news, using their title and edited short descriptions in Traditional Chinese. I tried LASER[1] first but later found Universal Sentence Encoder[2] seemed to work slightly better. Results from these unsupervised approaches are already acceptable, but still have occasional confusion and\\xa0hiccups.Not entirely satisfied with the unsupervised approaches, I collected and annotated 2,000 pairs of news and fine-tuned the BERT model on this dataset. This supervised approach is visibly better than the unsupervised one. And it’s also quite sample-efficient. Three hundred and fifty training example is already enough to beat Universal Sentence Encoder by a large\\xa0margin.Collecting domain-specific training examples, albeit small in size, can be crucial in improving the model performance. The result in this post provides some evidence to this argument.BackgroundThe Sentence-BERT paper[3] demonstrated that fine-tune the BERT[4] model on NLI datasets can create very competitive sentence embeddings. Further fine-tuning the model on STS (Semantic Textual Similarity) is also shown to perform even better in the target\\xa0domain.from Reimers et\\xa0al.[3]I reviewed BERT and Sentence-BERT in this previous post: Zero Shot Cross-Lingual Transfer with Multilingual BERT. Readers are also advised to read that post for more information about the advantages of using sentence embeddings instead of directly feeding sentence pairs to\\xa0BERT.ExperimentsDatasetEach pair of news is annotated with one of these scores\\u200a—\\u200a{0, 0.25, 0.5, 0.75, 1.0}, according to these general\\xa0rules:0\\u200a—\\u200aThese two pieces of news are completely unrelated.0.25\\u200a—\\u200aThese two pieces of news share a very general topic (e.g. diplomatic incidences) or a larger region (e.g. South-East Asia).0.5\\u200a—\\u200aThese two pieces of news share a topic (e.g. health tips), a country (e.g. South Korea), or a set of countries (e.g. Japan and the\\xa0U.S.A.).0.75\\u200a—\\u200aThese two pieces of news share a topic AND a country / a set of countries (e.g. trade negotiation between China and the\\xa0U.S.A.).1\\u200a—\\u200aThese two pieces of news cover the same events (e.g. the meeting of Trump and Kim) or shares the very narrow topic (e.g. the impact of Brexit on consumers).The 2,000 annotated pairs were split into three parts: training (1,400 pairs), validation (300 pairs), and test (300\\xa0pairs).Supervised ModelThe model setup is basically the same as in [3]. The cosine similarity between the sentence embeddings is used to calculate the regression loss (MSE is used in this\\xa0post).Since we only care about relative rankings, I also tried applying a learnable linear transformation to the cosine similarities to speed up training. The results are more or less the same. To reduce the hyper-parameters involved, The results reported below were all from trained without the linear transformation.The model with the best validation result is picked as the final\\xa0model.ResultsThe BERT baseline is formulated as in bert-as-service[5]. Using the second-to-last layer does not have better performance for this\\xa0dataset.Interestingly, converting the text from Traditional Chinese to Simplified Chinese boosts the performance of the unsupervised BERT and USE models. This might indicate the relative lack of Traditional Chinese resources during pre-training. In contrast, doing the same in supervised training creates slightly worse\\xa0models.I fixed the validation and test set, and sampled the training set to see how the supervised performs in low resource settings. The experiments show that it can already beat the unsupervised models with 350 examples, and more examples steadily improve the performance.Things that didn’t\\xa0workUsing Sentence-BERT fine-tuned on XNLI\\xa0dataset.Using Sentence-BERT fine-tuned on LCQMC dataset[6].Using Sentence-BERT fine-tuned on a news classification dataset.The news classification dataset is created from the same 8,000+ pieces of news used in the similarity dataset. The models are further fine-tuning on the similarity dataset.Among these three, the model fine-tuned on news classification dataset is the best one, but still inferior to directly fine-tuning on the similarity dataset.The failure of this approach to improve the performance can probably be attributed to the relatively large domain mismatch and the low quality of the machine-translated XNLI training\\xa0set.FinAs stated in the synopsis, I think that collecting domain-specific training examples, albeit small in size, can be crucial in improving the model performance. Although dull and boring at times, collecting and annotating your own dataset can be a very rewarding experience. Knowing your data is one of the most underrated aspects of data science\\xa0IMHO.ReferencesArtetxe, M. (2018). Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and\\xa0Beyond.Yang, Y., Cer, D., Ahmad, A., Guo, M., Law, J., Constant, N.,\\xa0… Kurzweil, R. (2019). Multilingual Universal Sentence Encoder for Semantic Retrieval.Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.hanxiao/bert-as-service: Mapping a variable-length sentence to a fixed-length vector using BERT\\xa0model.Liu, X. U., Chen, Q., Deng, C., Zeng, H., Chen, J. J., Li, D., & Tang, B. (2018). LCQMC: A Large-scale Chinese Question Matching Corpus. Proceedings of the 27th International Conference on Computational Linguistics, 1952–1962.Finetuning BERT for Similarity Search was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Reading the Article by Christopher Tong on The American Statistician Volume 73,\\xa02019Photo Credit(This is a republication of this post on my personal\\xa0blog.)This article by Christopher Tong has got a lot of love from people I followed on Twitter, so I decided to read it. It was very enlightening. But to be honest, I don’t fully understand quite a few arguments made by this article, probably because I lack the experience of more rigorous scientific experiments and research. Nonetheless, I think writing down the parts I find interesting and put it into a blog post would be beneficial for myself and other potential readers. Hopefully, it makes it easier to reflect on these materials later.This article argues that instead of relying on the statistical inference on an isolated study, we should use guide scientific research of all kinds by statistical thinking, and validate claims by replicating and predicting finds in new data and new settings.Replicating and predicting findings in new data and new settings is a stronger way of validating claims than blessing results from an isolated study with statistical inferences.Let’s see the reasoning behind this\\xa0claim.IntroductionFirst, Tong makes clear what “statistical inferences” are:Statistical inferences are claims made using probability models of data generating processes, intended to characterize unknown features of the population(s) or process(es) from which data are thought to be sampled. Examples include estimates of parameters such as the population mean (often attended by confidence intervals), hypothesis test results (such as p-values), and posterior probabilities.Some of the widely used tools in statistical inference has come under fire recently for being misused or abused (as in The ASA’s Statement on p-Values: Context, Process, and\\xa0Purpose)Among these criticisms, McShane and Gelman (2017) succinctly stated that null hypothesis testing “was supposed to protect researchers from over-interpreting noisy data. Now it has the opposite\\xa0effect.”Tong tries to distinguish exploratory and confirmatory objectives of a study. He argues that most scientific research tends to be exploratory and flexible, but the statistical inference is only suitable in a confirmatory setting where study protocol and statistical model are fully prespecified.We shall argue that these issues stem largely from the Optimism Principle (Picard and Cook 1984)that is an inevitable byproduct of the necessarily flexible data analysis and modeling work that attends most scientific research.And the lack of this distinction in the current use of inferential methods in science has enabled biased statistical inference and encouraged a Cult of the Isolated Study that short-circuits the iterative nature of research.Statistical Inference and the Optimism PrincipleAs Efron and Hastie stated in their new book “Computer-Age Statistical Inference: Algorithms, Evidence, and Data Science”:It is a surprising, and crucial, aspect of statistical theory that the same data that supplies an estimate can also assess its accuracy.I had similar doubts when receiving traditional statistics education. The bias and variance tradeoff is mentioned but it is generally up to use to decide where to draw the line. The cross-validation is clearly a more principled and objective approach. (See the Breiman’s classic paper “Statistical Modeling: The Two Culture”)As Harrell, F. E., Jr. (2015) observed:Using the data to guide the data analysis is almost as dangerous as not doing\\xa0so.Essentially, when researchers devise their analysis approach based on the data, it creates a chance to overfit the data. Simmons, Nelson, and Simonsohn (2011) called these opportunities researcher degrees of freedom, and when abused to fish for publishable p-values, p-hacking.The resulting inferences from the final model tend to be biased, with uncertainties underestimated, and statistical significance overestimated, a phenomenon dubbed the Optimism Principle by Picard andCook\\xa0(1984).In extreme cases, nonsense data can still seem to make\\xa0sense.In other words, it is possible to obtain a seemingly informative linear model, with decent R² and several statistically significant predictor variables, from data that is utter nonsense. This finding was later dubbed “Freedman’s paradox” (Raftery, Madigan, and Hoeting\\xa01993).This kind of bias would lead to an underestimation of the uncertainty because we picked the model that has fit the training data\\xa0best.Chatfield (1995) used the term model selection bias to describe the distorted inferences that result when using the same data that determines the form of the final model to also produce inferences from that\\xa0model.Exploratory and Confirmatory Objectives in Scientific ResearchThe obvious way to avoid the difficulties of overfitting and produce valid statistical inferences is to completely prespecify the study design and statistical analysis plan prior to the start of data collection.Tong uses the phased experimentation of medical clinical trials as an example of scientific research where exploratory/confirmatory distinction is clearly\\xa0made.This framework helps to separate therapeutic exploratory (typically Phase II) with therapeutic confirmatory (typically Phase III) objectives.It doesn’t prevent the expensive clinical dataset to be used for further exploratory work\\u200a—\\u200ato generate hypotheses for further testing in later experiments.A succinct perspective on such inferences is given by Sir Richard Peto, often quoted (e.g., Freedman 1998) as saying “you should always do subgroup analysis and never believe the results.”And it doesn’t mean the result from exploratory studies shouldn’t be published.If the result is important and exciting, we want to publish exploratory studies, but at the same time make clear that they are generally statistically underpowered, and need to be reproduced.From the Cult of the Isolated Study to TriangulationThe treatment of statistical inferences from exploratory research as if they were confirmatory enables what Nelder (1986) called The Cult of the Isolated Study, so that The effects claimed may never be checked by painstaking reproduction of the study elsewhere, and when this absence of checking is combined with the possibility that the original results would not have been reported unless the effects could be presented as significant, the result is a procedure which hardly deserves the attribute ‘scientific.Simple replication is usually not sufficient. Tong uses the Wright Brothers as a demonstrative example.Munafo and Davey Smith (2018) define triangulation as “the strategic use of multiple approaches to address one question. Each approach has its own unrelated assumptions, strengths, and weaknesses. Results that agree across different methodologies are less likely to be artifacts.”The notorious example of the report by the OPERA collaboration shows the importance of triangulation to uncover systematic errors.A particular weakness of the Isolated Study is that systematic errors may contaminate an entire study but remain hidden if no further research is\\xa0done.Technical Solutions and Their DeficienciesThe most widely known class of such methods is based on adjusting for multiple inferences. These range from the simple Bonferroni inequality to the modern methods of false discovery rate and false coverage rate (e.g., Dickhaus\\xa02014).A second class of methods incorporates resistance to overfitting into the statistical modeling process, often through an optimization procedure that penalizes model complexity, an approach sometimes called regularization.Tong also indicates that random splitting is still not the perfect solution.Unfortunately, such procedures (or their variants) are still vulnerable to the Optimism Principle, because random splitting implies that “left-out” samples are similar to the “left-in” samples (Gunter and Tong\\xa02017).So it’s better to collect more data to overcome model uncertainty:Obtaining “more than one set of data, whenever possible, is a potentially more convincing way of overcoming model uncertainty and is needed anyway to determine the range of conditions under which a model is valid” (Chatfield 1995).Tong also discussed another widely advocated solution\\u200a—\\u200amodel averaging. Those who are familiar with Kaggle competitions should already have a firm grasp on\\xa0this.Only through the iterative learning process, using multiple lines of evidence and many sets of data, can systematic error be discovered, and model refinement be continually guided by new\\xa0data.More Thoughtful SolutionsOne strategy requires preregistering both the research hypotheses to be tested and the statistical analysis plan prior to data collection, much as in a late-stage clinical trial (e.g., Nosek et al.\\xa02018).However, the fact that most scientific research cannot fit the above paradigm is a big problem. A more realistic approach is preregistered replication.A variation on this theme is preregistered replication, where a replication study, rather than the original study, is subject to strict preregistration (e.g., Gelman 2015). A broader vision of this idea (Mogil andMacleod 2017) is to carry out a whole series of exploratory experiments without any formal statistical inference, and summarize the results by descriptive statistics (including graphics) or even just disclosure of the raw\\xa0data.Enabling Good\\xa0ScienceTong adapts a taxonomy of statistical activity by Cox (1957) and Moore\\xa0(1992):Data production. The planning and execution of a study (either observational or experimental).Descriptive and exploratory analysis. Study the data at\\xa0hand.Generalization. Make claims about the world beyond the data at\\xa0hand.The first step of statistical thinking is to understand the objective of the study, its context, and its constraints, so that planning for study design and analysis can be fit for\\xa0purpose.Data ProductionFeller (1969) pronounced that “The purpose of statistics in laboratories should be to save labor, time, and expense by efficient experimental designs” rather than null hypothesis significance testing.Tong discusses a few experiment design techniques that should already be familiar to those who have taken formal statistics education. He also raises some practical concerns when conducting the experiment and its analysis.Data acquisition and storage systems should have appropriate resolution and reliability. (We once worked with an instrument that allowed the user to retrieve stored time series data with a choice of time-resolution. Upon investigation, we found that the system was artificially interpolating data, and reporting values not actually measured, if the user chose a high resolution.)And other research degrees of freedom that is related to decisions around experiment design:Other researcher degrees of freedom can affect study design and execution. An instructive example for the latter is the decision to terminate data collection. Except in clinical trials, where this decision is tightly regulated and accounted for in the subsequent analysis (e.g., Chow and Chang 2012), many researchers have no formal termination rule, stopping when funding is exhausted, lab priorities shift, apparent statistical significance is achieved (or becomes clearly hopeless), or for some other arbitrary reason, often involving unblinded interim looks at the\\xa0data.Data DescriptionMoses (1992)warned us that Good statistical description is demanding and challenging work: it requires sound conceptualization, and demands insightfully organizing the data, and effectively communicating the results; not one of those tasks is easy. To mistakenly treat description as ‘routine’ is almost surely to botch the\\xa0job.Theory of Description:Mallows (1983) provided an interesting perspective on a Theory of Description. He noted that “A good descriptive technique should be appropriate for its purpose; effective as a mode of communication, accurate, complete, and resistant.”Something like Tukey’s (1977) five number summary (the minimum, first quartile, median, third quartile, and maximum) can be helpful to describe the variability of the\\xa0data.Though we might not quantify uncertainty using probability statements, we can attempt to convey the observed variability of the data at hand, while acknowledging that it does not fully capture uncertainty… However, the use of such data summaries is not free of assumptions (e.g., unimodality, in some cases symmetry), so they are descriptive only in relation to these assumptions, not in an absolute\\xa0sense.Disciplined Data ExplorationAccording to Tukey (1973), exploratory analysis of the data is not “just descriptive statistics,” but rather an “actively incisive rather than passively descriptive” activity, “with a real emphasis on the discovery of the unexpected.”An example of how exploratory analysis may be essential for scientific inquiry is in the detection of and adjustment for batch\\xa0effects.Leek et al. (2010) defined batch effects as “sub-groups of measurements that have qualitatively different behavior across conditions and are unrelated to the biological or scientific variables in a\\xa0study.”Tong also cites the warning of Diaconis (1985) about the danger of undisciplined exploratory analysis.If such patterns are accepted as gospel without considering that they may have arisen by chance, he considers it magical thinking, which he defines as“our inclination to seek and interpret connections and events around us, together with our disinclination to revise belief after further observation.”Statistical ThinkingStatistical thinking begins with a relentless focus on fitness for purpose (paraphrasing Tukey 1962: seeking approximate answers to the right questions, not exact answers to the wrong ones), sound attitudes about data production and its pitfalls, and good habits of data display and disciplined data exploration.Statistical thinking also involves a keen awareness of the pitfalls of data analysis and its interpretation, including:The correlation versus causation fallacy.The distinction between interpolation and extrapolation.The distinction between experimental and observational data.Regression to the\\xa0mean.Simpson’s paradox, and the ecological fallacy.The curse of dimensionalityDiscussionThere is no scientifically sound way to quantify uncertainty from a single set of data, in isolation from other sets of data comprising an exploratory/learning process. This brings to mind an observation made about certain research in materials science: “Even if the studies had reported an error value, the trustworthiness of the result would not depend on that value alone” (Wenmackers and Vanpouke 2012). By emphasizing principles of data production, data description, enlightened data display, disciplined data exploration, and exposing statistical pitfalls in interpretation, there is much that statisticians can do to ensure that statistics is “a catalyst to iterative scientific learning” (Box\\xa01999).[Notes] “Statistical Inference Enables Bad Science; Statistical Thinking Enables Good Science” was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Author: Chris Padwick, Director of Computer Vision and Machine Learning at Blue River TechnologyHow did farming affect your day today? If you live in a city, you might feel disconnected from the farms and fields that produce your food. Agriculture is a core piece of our lives, but we often take it for\\xa0granted.A 2017 prototype of See & Spray, Blue River Technology’s precision weed control\\xa0machineFarmers today face a huge challenge\\u200a—\\u200afeeding a growing global population with less available land. The world’s population is expected to grow to nearly 10 billion by 2050, increasing the global food demand by 50%. As this demand for food grows, land, water, and other resources will come under even more pressure. The variability inherent in farming, like changing weather conditions, and threats like weeds and pests also have consequential effects on a farmer’s ability to produce food. The only way to produce more food while using less resources is through smart machines that can help farmers with difficult jobs, offering more consistency, precision, and efficiency.Alex Marsh, one of our Field Operations Specialists, pictured with a Self Propelled Sprayer. We work with big machines at Blue River\\u200a—\\u200aAlex is 6’4” tall and is about level with the top of the\\xa0tire.Agricultural roboticsAt Blue River Technology, we are building the next generation of smart machines. Farmers use our tools to control weeds and reduce costs in a way that promotes agricultural sustainability. Our weeding robot integrates cameras, computer vision, machine learning and robotics to make an intelligent sprayer that drives through fields (using AutoTrac to minimize the load on the driver) and quickly targets and sprays weeds, leaving the crops\\xa0intact.The machine needs to make real-time decisions on what is a crop and what is a weed. As the machine drives through the field, high resolution cameras collect imagery at a high frame rate. We developed a convolutional neural network (CNN) using PyTorch to analyze each frame and produce a pixel-accurate map of where the crops and weeds are. Once the plants are all identified, each weed and crop is mapped to field locations, and the robot sprays only the weeds. This entire process happens in milliseconds, allowing the farmer to cover as much ground as possible since efficiency matters. Here is a great See & Spray Video that explains the process in more\\xa0detail.To support the Machine Learning (ML) and robotics stack we built an impressive compute unit, based on the NVIDIA Jetson AGX Xavier Edge AI platform. Since all our inference happens in real time, uploading to the cloud would take too long, so we bring the server farms to the field. The total compute power on board the robot just dedicated to visual inference and spray robotics is on par with IBM’s super computer, Blue Gene (2007). This makes this a machine with some of the highest compute capacity of any moving machine machinery in the\\xa0world!Building weed detection modelsMy team of researchers and engineers is responsible for training the neural network model that identifies crops and weeds. This is a challenging problem because many weeds look just like crops. Professional agronomists and weed scientists train our labeling workforce to label the images correctly\\u200a—\\u200acan you spot the weeds\\xa0below?You are looking at cotton plants and some weeds. Can you tell the difference?In the image below, the cotton plants are in green and the weeds are in\\xa0red.The cotton plants are in green and the weeds are in\\xa0red.Machine learning\\xa0stackOn the machine learning front, we have a sophisticated stack. We use PyTorch for training all our models. We have built a set of internal libraries on top of PyTorch which allow us to perform repeatable machine learning experiments. The responsibilities of my team fall into three categories:Build production models to deploy onto the\\xa0robotsPerform machine learning experiments and research with the goal of continually improving model performanceData analysis / data science related to machine learning, A/B testing, process improvement, software engineeringWe chose PyTorch because it’s very flexible and easy to debug. New team members can quickly get up to speed, and the documentation is thorough. Before working with PyTorch, our team used Caffe and Tensorflow extensively. In 2019, we made a decision to switch to PyTorch and the transition was seamless. The framework gives us the ability to support production model workflows and research workflows simultaneously. For example we use the torchvision library for image transforms and tensor transformations. It contains some basic functionality and it also integrates really nicely with sophisticated augmentation packages like imgaug. The transforms object in torchvision is a piece of cake to integrate with imgaug. Below is a code example using the Fashion MNIST dataset. A class called CustomAugmentor initializes the iaa.Sequential object in the constructor, then calls augment_image() in the __call__ method. CustomAugmentor() is then added to the call to transforms.Compose(), prior to ToTensor(). Now the train and val data loaders will apply the augmentations defined in CustomAugmentor() when the batches are loaded for training and validation.https://medium.com/media/fb45b468be83d1466047055fb6c243ad/hrefAdditionally, PyTorch has emerged as a favorite tool in the computer vision ecosystem (looking at Papers With Code, PyTorch is a common submission). This makes it easy for us to try out new techniques like Debiased Contrastive Learning for semi-supervised training.On the model training front, we have two normal workflows: production and research. For research applications, our team runs PyTorch on an internal, on-prem compute cluster. Jobs being executed on the on-premise cluster are managed by Slurm, which is an HPC batch job based scheduler. It is free, easy to set up and maintain, and provides all the functionality our group needs for running thousands of machine learning jobs. For our production based workflows we utilize an Argo workflow on top of a Kubernetes (K8s) cluster hosted in AWS. Our PyTorch training code is deployed to the cloud using\\xa0Docker.Deploying models on field\\xa0robotsFor production deployment, one of our top priorities is high-speed inference on the edge computing device. If the robot needs to drive more slowly to wait for inferences, it can’t be as efficient in the fields. To this end, we use TensorRT to convert the network to an NVIDIA Jetson AGX Xavier optimized model. TensorRT doesn’t accept JIT models as input so we use ONNX to convert from JIT to ONNX format, and from there we use TensorRT to convert to a TensorRT engine file that we deploy directly to the device. As the toolstack evolves, we expect this process to improve as well. Our models are deployed to Artifactory using a Jenkins build process and they are deployed to remote machines in the field by pulling from Artifactory.To monitor and evaluate our machine learning runs, we have found the Weights & Biases platform to be the best solution. Their API makes it fast to integrate W&B logging into an existing codebase. We use W&B to monitor training runs in progress, including live curves of the training and validation loss.SGD vs Adam\\xa0ProjectAs an example of using PyTorch and W&B, I will run an experiment and compare the results of using different solvers in PyTorch. There are a number of different solvers in PyTorch\\u200a—\\u200athe obvious question is which one should you pick? A popular choice of solver is Adam. It often gives good results without needing to set any parameters and is our usual choice for our models. In PyTorch, this solver is available under torch.optim.adam. Another popular choice of solver for machine learning researchers is Stochastic Gradient Descent (SGD). This solver is available in PyTorch as torch.optim.SGD. If you’re not sure of the differences between the two, or if you need a refresher, I suggest reviewing this write up. Momentum is an important concept in machine learning, as it can help the solver to find better solutions by avoiding getting stuck in local minima in the optimization space. Using SGD and momentum the question is this: Can I find a momentum setting for SGD that beats\\xa0Adam?The experimental setup is as follows. I use the same training data for each run, and evaluate the results on the same test set. I’m going to compare the F1 score for plants between different runs. I set up a number of runs with SGD as the solver and sweeping through momentum values from 0–0.99 (when using momentum, anything greater than 1.0 causes the solver to diverge). I set up 10 runs with momentum values from 0 to 0.9 in increments of 0.1. Following that I performed another set of 10 runs, this time with momentum values between 0.90 and 0.99, with increments of 0.01. After looking at these results, I also ran a set of experiments at momentum values of 0.999 and 0.9999. Each run was done with a different random seed, and was given a tag of “SGD Sweep” in W&B. The results are shown in Figure\\xa01.Figure 1: On the left hand side the f1 score for crops is shown on the x-axis, and the run name is shown on the y axis. On the right hand side the f1 score for plants as a function of momentum value is\\xa0shown.It is very clear from Figure 1 that larger values of momentum are increasing the f1 score. The best value of 0.9447 occurs at momentum value of 0.999, and drops off to a value of 0.9394 at a momentum value of 0.9999. The values are shown in the table\\xa0below.Table 1: Each run is shown as a row in the table above. The last column is the momentum setting for the run. The F1 score, precision, and recall for class 2 (crops) is\\xa0shown.How do these results compare to Adam? To test this I ran 10 identical runs using torch.optim.Adam with just the default parameters. I used the tag “Adam runs” in W&B to identify these runs. I also tagged each set of SGD runs for comparison. Since a different random seed is used for each run, the solver will initialize differently each time and will end up with different weights at the last epoch. This gives slightly different results on the test set for each run. To compare them I will need to measure the spread of values for the Adam and SGD runs. This is easy to do with a box plot grouped by tag in\\xa0W&B.Figure 2: The spread of values for Adam and SGD. The Adam runs are shown in the left of the graph in green. The SGD runs are shown as brown (0.999), teal (0–0.99), blue (0.9999) and yellow\\xa0(0.95).The results are shown in graph form in Figure 2, and in tabular form in Table 2. The full report is available online too. You can see that I haven’t been able to beat the results for Adam by just adjusting momentum values with SGD. The momentum setting of 0.999 gives very comparable results, but the variance on the Adam runs is tighter and the average value is higher as well. So Adam appears to be a good choice of solver for our plant segmentation problem!Table 2: Run table showing f1 score, optimizer and momentum value for each\\xa0run.PyTorch VisualizationsWith the PyTorch integration, W&B picks up the gradients at each layer, letting us inspect the network during training.W&B experiment tracking also makes it easy to visualize PyTorch models during training, so you can see the loss curves in real time in a central dashboard. We use these visualizations in our team meetings to discuss the latest results and share\\xa0updates.As the images pass through our PyTorch model, we seamlessly log predictions to Weights & Biases to visualize the results of model training. Here we can see the predictions, ground truth, and labels. This makes it easy to identify scenarios where model performance isn’t meeting our expectations.The ground truth, predictions and the difference between the two. Crops are shown in green, while weeds are shown in\\xa0red.Here we can quickly browse the ground truth, predictions and the difference between the two. We’ve labeled the crops in green and the weeds in red. As you can see, the model is doing a pretty reasonable job of identifying the crops and the weeds in the\\xa0image.Here is a short code example of how to work with data frames in\\xa0W&B:https://medium.com/media/32ada41235b3678efea15cbb5fc58b5c/hrefReproducible modelsReproducibility and traceability are key features of any ML system, and it’s hard to get right. When comparing different network architectures and hyperparameters, the input data needs to be the same to make runs comparable. Often individual practitioners on ML teams save YAML or JSON config files\\u200a—\\u200ait’s excruciating to find a team member’s run and wade through their config file to find out what training set and hyperparameters were used. We’ve all done it, and we all hate\\xa0it.A new feature that W&B just released solves this problem. Artifacts allow us to track the inputs and outputs of our training and evaluation runs. This helps us a lot with reproducibility and traceability. By inspecting the Artifacts section of a run in W&B I can tell what datasets were used to train the model, what models were produced (from multiple runs), and the results of the model evaluation.A typical use case is the following. A data staging process downloads the latest and greatest data and stages it to disk for training and test (separate data sets for each). These datasets are specified as artifacts. A training run takes the training set artifact as input and outputs a trained model as an output artifact. The evaluation process takes the test set artifact as input, along with the trained model artifact, and outputs an evaluation that might include a set of metrics or images. A directed acyclic graph (DAG) is formed and visualized within W&B. This is helpful since it is very important to track the artifacts that are involved with releasing a machine learning model into production. A DAG like this can be formed\\xa0easily:One of the big advantages of the Artifacts feature is that you can choose to upload all the artifacts (datasets, models, evaluations) or you can choose to upload only references to the artifacts. This is a nice feature because moving lots of data around is time consuming and slow. With the dataset artifacts, we simply store a reference to those artifacts in W&B. That allows us to maintain control of our data (and avoid long transfer times) and still get traceability and reproducibility in machine learning.https://medium.com/media/56b63553a5b4abaefb1037221b2b9cb5/hrefLeading ML\\xa0teamsLooking back on the years I’ve spent leading teams of machine learning engineers, I’ve seen some common challenges:Efficiency: As we develop new models, we need to experiment quickly and share results. PyTorch makes it easy for us to add new features fast, and Weights & Biases gives us the visibility we need to debug and improve our\\xa0models.Flexibility: Working with our customers in the fields, every day can bring a new challenge. Our team needs tools that can keep up with our constantly evolving needs, which is why we chose PyTorch for its thriving ecosystem and W&B for the lightweight, modular integrations.Performance: At the end of the day, we need to build the most accurate and fastest models for our field machines. PyTorch enables us to iterate quickly, then productionize our models and deploy them in the field. We have full visibility and transparency in the development process with W&B, making it easy to identify the most performant models.I hope you have enjoyed this short tour of how my team uses PyTorch and Weights and Biases to enable the next generation of intelligent agricultural machines!About the\\xa0authorI am the Director of Computer Vision and Machine Learning at Blue River Technology. We build robots that distinguish crop from weed in an agricultural field and then only spray the weeds. I’ve worked at Blue River for 4 and a half years. My background is in Physics and Astronomy and in grad school I helped build a telescope to measure the Cosmic Background Radiation. Check out our careers page, we are\\xa0hiring!AI for AG: Production machine learning for agriculture was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'A tutorial for serving models cost-effectively at scale using Azure Functions and ONNX\\xa0RuntimeAuthored by: Gopi Kumar, Principal Program Manager at Microsoft. (@zenlytix)Recent advances in deep learning and cloud-based infrastructure have led to innovations in models for various domains like natural language processing, computer vision, recommendations. Of course, developing the model is only half the story. Your models are mostly useful once they are served up for making predictions for consumption in in AI-driven scenarios from the end applications. It is important to do it in a cost-effective and reliable manner. However, managing infrastructure for hosting your models is challenging as it involves several aspects like maintaining your fleet, ensuring reliability, scaling, security and ongoing monitoring and management. Can we leverage serverless technologies for our model\\xa0hosting?Serverless DeploymentAzure provides serverless infrastructure with the Azure Functions service that offloads many of these infrastructure management tasks and simplifies the rest. Azure Functions operates the hosting instances to run your models as small functions without the developer or operator being aware of the specific virtual machine or fleets. Depending on your application needs and cost budget, you can use a choice of several hosting plans within Azure Functions from basic instances with a consumption plan, to premium instances and dedicated hosting. We will use the “consumption plan” which is usually the most cost effective (often free upto 1 million monthly requests per subscription ) option for relatively low volume scenarios.You are only charged for the duration that the function actually runs in a consumption plan. Also, as your needs change, you can easily upgrade to premium or dedicated hosting plans as the underlying technology and methodology is still the same. Details on pricing for various hosting options are here. For efficient model serving we will use the ONNX Runtime, a highly optimized, low memory footprint execution engine. With ONNX Runtime, the deployment package footprint can be upto 10x lower, allowing us to use the more cost effective plan. More details are in the section below titled “Optimizing the runtime footprint”.A Step-by-step walkthroughWe will walk through the steps to take a PyTorch model and deploy it into the Azure Functions serverless infrastructure, running the model prediction in the highly efficient ONNX Runtime execution environment. While the steps illustrated below are specific to a model that was built using the popular fast.ai (a convenience library built on PyTorch), the pattern itself is quite generic and can be applied to deploying any PyTorch\\xa0model.The main steps to get your models into production on Azure serverless infrastructure using the ONNX Runtime execution engine (after you have trained your model\\xa0are):Export modelTest model deployment locallyDeploy model to the Azure FunctionsStep 1: Export\\xa0modelThe model illustrated as an example is the Bear Detector model which is one of the popular examples in fast.ai. We won’t go into the actual training process here as it is the same method you normally use. The end result of the training process is a PyTorch model object in your Python environment. PyTorch provides a built-in mechanism to export your model object in the format needed by ONNX Runtime with the following code:dummy_input = torch.randn(1, 3, 224, 224, device=\\'cuda\\')onnx_path =  \"./model.onnx\"torch.onnx.export(learn.model, dummy_input, onnx_path, verbose=False)The parameters for the dummy_input depends on the shape of the tensors in your model. The output will be the model written to a file called model.onnx.You also need to create a label file (labels.json) since this is a classification model. In the Bear Detector example, the model is classifying across the three classes of bear, and hence the label file looks like\\xa0this:[\"black\",\"grizzly\",\"teddy\"]The classes names must match the vocabulary you used during the training.Step 2: Test model deployment locallyOne of the best practices for productive development is to be able to test your deployment on your development machine before you deploy it to the cloud. I use a Windows laptop with Windows Subsystems for Linux (WSL2) as my development-test environment. The instructions should also apply to development environments like a local Linux machine, Azure Cloud Shell or a Virtual machine on the cloud like the Data Science Virtual Machine or Azure Machine Learning Compute Instances.Setting up the environment and\\xa0toolsYou need to have the following tools installed on your development machine.The Azure Functions Core\\xa0ToolsThe Azure\\xa0CLIPython 3.7Create an Azure Function\\xa0ProjectFirst, you need to create a project for your Azure Functions locally, which is just a directory on your\\xa0machine.mkdir << Your projectname >>cd << Your projectname>>]mkdir startcd startNext, you must initialize the Function App and specify the runtime. We use Python runtime and the Azure Functions whose execution is triggered through a HTTP request. This means that to get a prediction from your model you send a HTTP request from the client with the desired parameter (to be described later).func init --worker-runtime pythonfunc new --name classify --template \"HTTP trigger\"Create your inferencing codeWe have a convenience inference code template that is published on GitHub that you can use as boiler plate and update to your needs. Here are steps to clone the code template and adapt it for your Azure Function App project to deploy your\\xa0model.git clone https://github.com/Azure-Samples/functions-deploy-pytorch-onnx.git /tmp/deploy-onnx-template# Copy the deployment sample to function appcp -r /tmp/deploy-onnx-template/start ..The main source files are __init__.py and predictonnx.py in start/classify directory. In the Bear detector example, it takes input from the HTTP GET request in the “img” parameter which is a URL to an image which will be run through the model for prediction of the type of bear. You can adapt the same easily for deploying other\\xa0models.The predictonnx.py which does the actual prediction function, expects the model file and labels file in the current directory. In this example, we also need to do the pre-processing of the input image by normalizing it and scaling it to the desired size before it can be passed onto ONNX Runtime to run the inference operation. This file contains both the pre-processing code and the code to get the model prediction with ONNX\\xa0Runtime.Copy the model.onnx and labels.json files (created in the earlier step) to the directory.Install the dependent Python libraries locally in a virtual environment.python -m venv .venvsource .venv/bin/activatepip install --no-cache-dir -r requirements.txtDeploy Azure Functions App locally and\\xa0testNow you are ready to test your Azure Functions App locally. The Azure Functions Core Tools makes this super simple. Literally you just run one command from the “start” directory:func startThis will start an environment very similar to what would be in the cloud-based Azure Functions on your local machine. It listens on port 7071 and is ready for your request. For testing the Azure functions all you need is to visit the following URL in a browser or use tools like curl or invoke a Web request from your client application where you want to consume the\\xa0model.http://localhost:7071/api/classify?img=[[URL of the image to classify]]Effectively you are pass an URL to an image that the Azure functions in the “img” parameter of the web request to receive predictions from the model on the type of bear with the above example\\xa0model.After you have tested with a few sample images and are satisfied that your model works fine, you are now ready to deploy it to the cloud where a client or application from anywhere is able to consume predictions from the\\xa0model.Pro Tip: If the only consumer for the model is an app running on your development machine this can be an end\\xa0state.Step 3: Deploy Model to the Azure FunctionsWe will use the Azure CLI to create an Azure Function App and a Storage account and put all these in a resource group for easy management.az group create --name [[YOUR Function App name]]  --location westus2az storage account create --name [[Your Storage Account Name]] -l westus2 --sku Standard_LRS -g [[YOUR Function App name]]az functionapp create --name [[YOUR Function App name]] -g [[YOUR Function App name]] --consumption-plan-location westus2 --storage-account [[Your Storage Account Name]] --runtime python --runtime-version 3.7 --functions-version 3 --disable-app-insights --os-type LinuxNote: If you have not logged into Azure CLI. you must first run “az login” and follow instructions to log into to Azure with your credentials. In the example above, we are deploying the resources in westus2. You can choose another Azure data center/ region if that is more convenient for you. Here in this example, we set a flag to disable Application Insights on this Azure Functions App. Application Insights is a service Azure provides to help you monitor your Azure Functions and other Azure services. We recommend enabling Application Insights for production deployment and refer you to the documentation on Functions Monitoring for more information on its\\xa0usage.Finally, you run the command to publish your Azure Function App project into\\xa0Azure.pip install  --target=\"./.python_packages/lib/site-packages\"  -r requirements.txt# Publish Azure function to the cloudfunc azure functionapp publish [[YOUR Function App name] --no-buildAfter a few minutes, your model is deployed to the cloud. The last command also output the URL base include a key that can be used to make HTTP request and get predictions from the model. In case you missed the output, you can go back and fetch the URL by running the command “func azure functionapp list-functions [[YOUR Function App name]\\u200a—\\u200ashow-keys”. For this Bear detector app, you can append “&img=[Your Image URL]” to the InvokeUrl from above command to invoke the Azure functions and receive predictions from the\\xa0model.You can visit the Azure portal and search for your Azure Function\\xa0App.Azure Functions provides additional deployment modes. For simplicity I used the local zip deployment which essentially packages up the local project directory (including the dependent python libraries) into a zip file which is then deployed to the Azure Functions App in the cloud. Azure Functions also supports container deployment on premium and dedicated hosting.Optimizing the runtime footprintOne challenge with the consumption plan is that the instance sizes are relatively small with a maximum of 1.5GB of main memory per instance. A native PyTorch model has a bigger footprint both from an App on-disk size and the working memory size perspective. The default runtimes in popular deep learning frameworks are more optimized for model development experience as opposed to serving. Microsoft developed the ONNX Runtime, a highly optimized, low memory footprint and open source execution engine for inferencing.Using ONNX Runtime as the execution runtime in Azure Functions helps lower the footprint of hosting your PyTorch model and enables you to deploy models on the cheaper consumption plan hosting mode of Azure Functions. The total deployment package for our example was about 75MB (including the model file, Python library dependencies). In contrast, if you are using standard PyTorch runtime, the deployment package is almost 10X bigger for the same model since the PyTorch library and dependencies has to be bundled with your model. This often requires deploying to a larger instance type for hosting. In our experience in deploying numerous models within applications in Microsoft, the ONNX Runtime is on an average 2X faster enabling to serve models at low latency and high throughput. So, ONNX Runtime is a great option to deploy your PyTorch models in most scenarios especially in low cost / low resource environments such as the Azure Functions Consumption plan instances. Hosting models in Azure Functions with HTTP interface enables you to consume the same from cross platform\\xa0clients.Advanced Deployment ConsiderationsIt should be noted that there are other technologies you can use to deploy models on Azure. Many customers use Kubernetes clusters to run their applications and host their models. Azure offers a managed Azure Kubernetes Service (AKS) that can be used to host your models. Azure Machine Learning service provides out of the box support to deploy your models to\\xa0AKS.Some of the other considerations in deploying models into production include having a streamlined development and deployment processes. This is where end-to-end machine learning services like Azure Machine Learning addresses these challenges by effectively bridging the experimentation world of data scientists who are iterating on new models and the operational world of machine learning (also known as ML Ops) where the models are served in a production environment with the appropriate SLAs, ensuring model reproducibility, versioning, monitoring and feedback loops to improve models over time. We don’t cover these here but provide pointers in the “Learn More” section\\xa0below.SummaryWe have seen how it is quite easy to deploy PyTorch models cost-effectively to the Azure serverless infrastructure and get the benefits of offloading operational concerns like scaling, security, monitoring and infrastructure management. The tooling provided by Azure Functions enables a good local development, debugging and deployment experience. ONNX Runtime enhances PyTorch with optimized inferencing and a fast execution engine in a small footprint, making your PyTorch model inferencing highly performant. We would to love to hear your experience with serverless deployment of your models and how we can improve our tools and processes further.Learn MoreAzure FunctionsONNX RuntimeAzure Machine Learning\\xa0ServiceGithub sample model deployment to Azure FunctionsMachine Learning OperationsEfficient Serverless deployment of PyTorch models on Azure was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Falcon 9 SpaceX\\xa0launchHi, I am Sergey, the author of the Catalyst\\u200a—\\u200aPyTorch library for deep learning research and development. In our previous blog posts, we covered an introduction to the Catalyst and our advanced pipeline for NLP on BERT distillation. In this post, I would like to share with you our development progress for the last month. Let’s check what features we have added to the framework in such a short\\xa0time.tl;drTraining Flow improvements: BatchOverfitCallback, PeriodicLoaderCallback, ControlFlowCallbackMetric Learning features: InBatchSampler, AllTripletsSampler, HardTripletsSampler, tutorialFixes and acknowledgmentsNew integrations: MONAI &\\xa0CatalystEcosystem update\\u200a—\\u200aAlchemyYou can find all examples from this blog post on this Google\\xa0Colab,Google ColaboratoryTraining Flow improvementsBatchOverfitCallbackFor better user experience with deep learning, you need to think not only about cool engineering features like distributed support, half-precision training, and metrics (we already have them). You also have to think about common difficulties that occur during experimentations.Imagine a typical research situation: you wrote your fancy pipeline, got the dataset, and try to fit this data into your model. But something goes wrong and you can’t get desired\\xa0results.One of the potential causes\\u200a—\\u200athere is a problem with pipeline convergence. You could subsample your data and check that model easily overfits only on this subset. But do it again and again along all your projects? Looks like we need a general solution for this problem. And here comes our BatchOverfitCallback (contributed by Scitator). The idea behind it is straightforward— let’s take only a requested number of batches from your dataset and use only them for training.So, let’s check some deep learning pipeline,https://medium.com/media/edfd7a546a931a2b07f35e8f57154acf/hrefYou can run it\\xa0withhttps://medium.com/media/e2ebbbc014b1cb96bb1f53dd927b62d4/hrefThanks to the update, you can check your pipeline convergence with only one extra\\xa0linehttps://medium.com/media/0f8ed1a7625c898270c2c54cac10073b/hrefThis way you can easily debug your experiment without extra code refactoring. You could also redefine the required number of batches per\\xa0loader.https://medium.com/media/322f6a21cc63a7b8a650e8caa313686c/hrefWhat is even cooler, we have integrated this feature into our Config API. You can use it\\xa0withcatalyst-dl run --config=/path/to/config --overfitPeriodicLoaderCallbackDuring your research practices, you could find yourself in the situation, when you have only a few train samples and a huge test set to check your model performance. Alternatively, you could have computational heavy validation (for example, during the NMS stage on anchor-box object detection) that takes too much time of your training pipeline. You can increase the train set for each epoch with BalanceClassSampler, but what if you want to keep your training data unchanged? Try our new PeriodicLoaderCallback (contributed by\\xa0Ditwoo).For the example above you can set a validation run every 2\\xa0epochs:https://medium.com/media/e047ade1f3d783ffd052469c560b7a67/hrefThanks to Catalyst design, we could extend it for any number of your data\\xa0sources:https://medium.com/media/c99697e7b65ca526718d6060ab205dd3/hrefControlFlowCallbackAfter PeriodicLoaderCallback we asked ourselves: “If you can enable/disable data sources, why can’t you do the same with metrics and entire Callbacks?”. For example, you have a metric you don’t want to compute during the training or validation stage. With ControlFlowCallback (contributed by Ditwoo) it could be done\\xa0easily:https://medium.com/media/d55a840ced82dc7b11192f625e093a77/hrefNow you can define with which loaders and epochs you would like to use Callback, or ignore\\xa0it.Metric Learning\\xa0featuresI also want to make a preview of extra updates during this release. For the last month we were working hard developing a foundation for Metric Learning research. We have prepared several InBatchTripletsSamplers (contributed by AlekseySh)\\u200a—\\u200ahelper modules for online triplets mining during training,AllTripletsSampler to select all possible triplets for the\\xa0anchorsHardTripletsSampler to select the hardest triplets based on distances between\\xa0samplesWe hope these abstractions would help in your research. We are working on Metric Learning minimal example now to create a starting benchmark for this case. Stay in touch for the upcoming tutorial.FixesLast but not least, as with every release, this one was with a few\\xa0fixes,thanks to Oleksii Sliusarenko we fix our “first epoch” issue with EarlyStoppingCallbackwith Lokesh Nandanwar support we make our OneCycleLRWithWarmup great\\xa0againa number of Github and catalyst-codestyle improvements by Yauheni\\xa0KachanIntegrations\\u200a—\\u200aMONAI segmentation exampleIn collaboration with the MONAI team, we have prepared an introduction tutorial on 3D image segmentation with the MONAI and Catalyst framework.Google ColaboratoryPlansWe still have a lot of\\xa0plans:TPU support\\u200a—\\u200awith current cpu, gpu, and Slurm support, we want to push the frontiers and get Catalyst to the fancy\\xa0TPUkornia integration\\u200a—\\u200awe already have a native integration with the famous albumentations library, but… why should not we make a fair comparison between alternatives and take the best for our customers? Stay in touch for an upcoming benchmark on image augmentation libraries benchmark by Catalyst-Teammodel auto-pruning\\u200a—\\u200aas far as Catalyst is a framework for deep learning research and development, and we already support model auto-tracing, we want introduce framework support for models auto-pruning.Ecosystem release\\u200a—\\u200aAlchemyDuring this Catalyst release, we also have another great new\\u200a—\\u200awe are moving our ecosystem powered monitoring tools to the global MVP release. Feel free to use it and share your feedback with\\xa0us.AlchemyWe help researchers to accelerate pipilines with Catalyst and to find insights with Alchemy along the whole R&D process: these ecosystem tools are available for you to train, share and collaborate more effectively.Photograph by Robert\\xa0OrmerodAfterwordOur goal is to build a foundation for fundamental breakthroughs in deep learning and reinforcement learning areas. Nevertheless, it is really hard to build an Open Source Ecosystem with only a few motivated people. If you are a company that is deeply committed to using open source technologies in deep learning, and want to support our initiative, feel free to write us at catalyst.team.core@gmail.com. For details about Ecosystem, check our vision and manifesto.Catalyst dev blog - 20.07 release was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Authors: Miquel Àngel Farré, Anthony Accardo, Marc Junyent, Monica Alfaro, Cesc Guitart at\\xa0DisneyDisney’s Content\\xa0GenomeThe long and incremental evolution of the media industry, from a traditional broadcast and home video model, to a more mixed model with increasingly digitally-accessible content, has accelerated the use of machine learning and artificial intelligence (AI). Advancing the implementation of these technologies is critical for a company like Disney that has produced nearly a century of content, as it allows for new consumer experiences and enables new applications for illustrators and writers to create the highest-quality content.In response to this industry shift, Disney’s Content Genome was built by a team of R&D engineers and information scientists within Disney’s Direct-to-Consumer & International Organization (DTCI) to power new applications for digital product innovation, and create a comprehensive digital archive of Disney’s unique and unmatched library of content. The platform populates knowledge graphs with content metadata, which powers AI applications across search, personalization, and production operations, all of which are critical components of digital video platforms. This metadata improves tools that are used by Disney storytellers to produce content; inspire iterative creativity in storytelling; power user experiences through recommendation engines, digital navigation and content discovery; and enable business intelligence.In order to bring the Content Genome to life, a significant investment in manual and automated content annotation, computer vision and machine learning techniques was necessary\\u200a—\\u200aand PyTorch helped us meet this challenge.Tagging Disney\\xa0contentAs a first step toward powering the Content Genome, DTCI Technology engineers created the first automated tagging pipeline. Tagging content is an important component of DTCI’s use of supervised learning, which is regularly employed in custom use cases that require specific detection. Tagging is also the only way to identify a lot of highly contextual story and character information from structured data, like storylines, character archetypes or motivations.This automated tagging pipeline was equipped with face detection and recognition modules that are applied to Disney’s library of content (shows, films, etc.). They are based on traditional machine learning approaches and performed well enough to recognize real human faces from characters in shows and feature films. Part of this success relied on the combination of machine learning methods, like HOG+SVM, and the DTCI knowledge graph, which specifically defines relations between particular entities of Disney content, such as the link between a specific episode of a series and the subset of locations or characters that appear in that episode. This initial success in facial recognition was then extended to the classification of other entities, such as locations.Recent advancements in deep learning helped extend our models beyond facial recognition of characters within content, largely due to pre-trained models based on new architectures that can be fine-tuned to create custom models aligned to our intellectual property. PyTorch allows us to have state-of-the-art pre-trained models accessible as a starting point to fulfill our needs expediently, and make our archiving process more efficient.The next step toward more advanced facial detection and recognition, and one of the major technical challenges for Disney’s content catalogue, pertains to tagging animated content. The first question we approached was: How do we move from facial detection in live-action content to facial detection in animated\\xa0content?The natural first approach was to try our live action face detection against animated content; while this worked in some cases, it wasn’t a consistent solution. After some analysis, we determined that methods like HOG+SVM are robust against color, brightness or texture changes, but the models used could only match human features (i.e. two eyes, one nose and a mouth) in animated characters with human proportions. In comparison, a human brain can identify faces even when they appear on the front of a car or on an alien body\\u200a—\\u200aand for Disney, we need to ensure we can detect characters that fall into this category, like Lightning McQueen from Cars or Mike Wazowski from Monsters, Inc., to build the most robust archive possible.Animated face detectionOur first experiment was to validate if the same HOG+SVM pipeline that worked for animated faces of characters with human proportions could work with animated faces of characters that are not\\xa0humans.We manually annotated a few samples using two Disney Junior animated shows, Elena of Avalor and The Lion Guard, drawing squares around faces for a couple hundred frames per show. With the manually annotated dataset, we validated that a HOG+SVM pipeline performed poorly in animated faces, specifically, human-like faces and animal-like faces, and knew we needed a technique able to learn more abstract concepts of\\xa0faces.Disney Junior’s Princess Elena of Avalor and magical flying creature, Migs, with manually annotated facial detection methods\\xa0applied.Disney Junior’s The Lion Guard character, Bunga, demonstrates the complexity of animated, non-human facial detection.If we wanted to apply deep learning approaches, we would either have to collect thousands of different faces from animated content or apply transfer learning from another domain to animated content. The second solution had the advantage of requiring a much smaller training\\xa0set.We continued our experimentation using the samples that we had for the HOG+SVM experiment to fine tune a Faster-R CNN Object Detection architecture trained over the COCO dataset with the single goal of identifying animated\\xa0faces.Even with the small number of samples we used in this transfer learning solution, we obtained satisfactory results testing with images in the dataset. However, when we ran the detector against images that didn’t contain animated faces, we often found false positive detections.The relevance of negative\\xa0samplesFalse positive detections are a common problem with transfer learning on custom datasets, largely due to the limited context in the training\\xa0images.In our particular case, during training, every object that appears in the image that is not the object of interest is considered as a negative sample. The background of animated content usually has flat regions and few details. Hence, the Faster-RCNN model was incorrectly learning that everything that stood out against a simple background was an animated face. For example, any text clearly in the foreground was considered a positive detection. Although our dataset had enough positive images to detect animated faces, it didn’t have rich negative samples from detailed backgrounds.We decided to increase our initial dataset with images not containing animated faces but with other objects from animated series or features.In order to make this technically possible, we extended Torchvision’s Faster-RCNN implementation to allow the load of negative images during the training process without annotations. This is a new feature that we contributed to Torchvision 0.6 with the guidance of the Torchvision core developers.Adding negative examples in the dataset drastically reduced false positives at inference time, providing outstanding results.Speeding up a video processing pipeline with\\xa0PyTorchWith an animated character face detector performing properly, our next goal was to accelerate our video analysis pipeline. For this task and thanks to the PyTorch team, we discovered that PyTorch is more than a framework to run neural net architectures and can be used to parallelize and speedup other\\xa0tasks.Running the new animated face detector on each frame is time-consuming and ineffective, so we combined our face detection model with other algorithms like a bounding box tracker and a shot detector (a shot is defined as the continuous sequence of frames between two edits or cuts). This allowed us to accelerate the processing, as fewer detections are required, and we can propagate the detected faces to all the frames. Furthermore, it provided us temporal information; instead of only detecting independent frames, they are contextualized as segments of the video where a character appears.These relationships between models expose dependencies that impact our implementation of classifiers. For example, we choose the frames to send to the detection model depending on the output of the shot boundaries classifiers. Our pipeline has to take these dependencies into account and remove redundant computations to be as fast as possible.Reading and decoding the video is also time-consuming so that’s the first thing we optimized. We use a custom PyTorch IterableDataset that, in combination with PyTorch’s DataLoader, allows us to read different parts of the video with parallel CPU\\xa0workers.The video is split in chunks based on its I-frames and each worker reads different chunks. This provides batches of contiguous ordered frames, although batches might not be\\xa0ordered.Video batching strategy.Even though this video reading approach is very fast, we try to do all our computations with a single video read. To do this we implemented most of our pipeline in PyTorch with GPU execution in mind. Each frame is sent only once to the GPU and there we apply all our algorithms on each batch, reducing the communication between CPU and GPU to a\\xa0minimum.We also use PyTorch to implement more traditional algorithms such as our shot detector, which doesn’t use neural nets and primarily performs operations such as color space changes, histograms and singular value decomposition (SVD). Using PyTorch to implement even the more traditional algorithms in our pipeline allows us to move computations to GPU with minimal cost and to easily recycle intermediate results shared between multiple algorithms.By moving our CPU parts of the pipeline to GPU with PyTorch and speeding up our video reading with DataLoader, we were able to speed up our processing time by a factor of 10, taking full advantage of our hardware.The right, community-driven philosophyPyTorch has been present in our animated character detection R&D from the initial neural net architecture experimentations to the latest efficiency improvements in our production environment.From a discovery perspective, the well-maintained popular datasets and model architectures in Torchvision, combined with its popularity across academia, allowed us to compare state-of-the-art approaches and validate which ones better fit our needs, accelerating our\\xa0R&D.Digging into PyTorch core components such as IterableDataset, DataLoader and the common image transformations for computer vision in Torchvision, enabled us to increase data loading and algorithm efficiency in our production environment, growing our use of PyTorch from the inference or model training resource to a full pipeline optimization toolset.We have also had the opportunity to meet the community behind PyTorch, which encouraged us to propose changes to the framework, hold discussions to find together the best approach for the community and eventually make the agreed solution part of the framework. This guidance was key for us to contribute. It was also paramount to understand that each addition to the framework is examined from different perspectives and to ensure it is the correct move in terms of performance and functionality.How Disney uses PyTorch for animated character recognition was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Authors: Narine Kokhlikyan, Research Scientist at Facebook AI, Ankur Taly, Head of Data Science at Fiddler Labs, Aalok Shanbhag, Data Scientist at Fiddler\\xa0LabsWe are excited to announce that Fiddler and Captum are collaborating to push the boundaries of Explainable AI. The goals of this partnership are to help the data science community to improve model understanding and its applications, as well as to promote the usage of Explainable AI in the ML workflow.Fiddler is an Explainable AI Platform that seamlessly enables business and technical stakeholders to explain, monitor, and analyze AI models in production. Users get access to deep model level actionable insights to understand problem drivers using explanations and efficiently root cause and resolve issues. Captum is a model interpretability suite for PyTorch developed by Facebook AI. It offers a number of attribution algorithms that allow users to understand the importance of inputs features, and hidden neurons and\\xa0layers.Need for attributions in\\xa0MLML methods have made remarkable progress over the last decade, achieving super human performance on a variety of tasks. Unfortunately much of the recent progress in machine learning has come at the cost of the models becoming more opaque and “black box” to us humans. With the increasing use of ML models in high stakes domains such as hiring, credit lending, and healthcare, the impact of ML methods on society can be far reaching. Consequently, today, there is a tremendous need for explainability methods from ethical, regulatory, end-user, and model developer perspectives. An overarching question that arises is: why did the model make this prediction? This question is of importance to developers in debugging (mis-)predictions, regulators in assessing the robustness and fairness of the model, and end-users in deciding whether they can trust the\\xa0model.One concrete formulation of this why question is the attribution problem. Here, we seek to explain a model’s prediction on an input by attributing the prediction to features of the input. The attributions are meant to be commensurate with each feature’s contribution to the prediction. Attributions are an effective tool for debugging mis-predictions, assessing model bias and unfairness, generating explanations for the end-user, and extracting rules from a model. Over the last few years, several attribution methods have been proposed to explain model predictions. The diagram below shows all attribution algorithms available in the Captum library divided into two groups. The first group, listed on the left side of the diagram, allows us to attribute the output predictions or the internal neurons to the inputs of the model. The second group, listed on the right side, includes several attribution algorithms that allow us to attribute the output predictions to the internal layers of the model. Some of those algorithms in the second group are different variants of the ones in the first\\xa0group.A prominent method among them is Integrated Gradients (IG), which has recently become a popular tool for explaining predictions made by deep neural networks. It belongs to the family of gradient-based attribution methods, which compute attributions by examining the gradient of the prediction with respect to the input feature. Gradients essentially capture the sensitivity of the prediction with respect to each\\xa0feature.IG operates by considering a straight line path, in feature space, from the input at hand (e.g., an image from a training set) to a certain baseline input (e.g., a black image), and integrating the gradient of the prediction with respect to input features (e.g., image pixels) along this path. The highlight of the method is that it is proven to be unique under a certain set of desirable axioms. As a historical note, the method is derived from the Aumann-Shapley method from cooperative game theory. We refer the interested reader to the paper and this blog post for a thorough analysis of the method. IG is attractive as it is broadly applicable to all differentiable models, is easy to implement in popular ML frameworks, e.g., PyTorch, TensorFlow, and is backed by an axiomatic theory. IG is also much faster than combinatorial calculation based Shapley value methods due the use of gradients. For instance, IG is one of the key explainability methods made available by Captum for PyTorch\\xa0models.In the rest of the post, we will demonstrate how IG explanations, enabled by the Captum framework, can be leveraged within Fiddler to explain a toxicity\\xa0model.Example\\u200a—\\u200aThe toxicity\\xa0modelDetecting conversational toxicity is an important yet challenging task that involves understanding many subtleties and nuances of human language.Over the last years various different classifiers have built that aim to address these problems and increase the prediction accuracy of machine learning\\xa0models.Although prediction accuracy is an important metric to measure, understanding the root causes of how those models reason and whether they are capable of capturing the semantics and unintended bias are crucial for those\\xa0tasks.In this case study we fine-tuned BERT classification model on conversational toxicity dataset, performed predictions on a subset of sentences and computed each token’s importance for the predicted samples using integrated gradients. Integrated gradients is a gradient-based attribution algorithm that assigns an importance score to each token embedding by integrating the gradients along the path from a sentence that has missing toxic features to the one that is classified as\\xa0toxic.For training purposes we used English wikipedia talk dataset (source: https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) that contains 160k labeled discussion comments.About 60% of that dataset was used for training, 20% for development and another 20% for testing purposes.We reached overall 97% training and 96% test accuracy by fine-tuning vanilla Bert binary classification model on the dataset described above.Side Note: This Unintended ML Bias Analysis presentation contains interesting insights on bias in text classification.We will be sharing the training script and other analysis notebooks shortly.Importing the model into\\xa0FiddlerHere we give a general run down of the steps to import a PyTorch model, using the example of the above\\xa0model.As an Explainable AI Platform a core feature in Fiddler’s Platform is to generate explanations and predictions for the models that are loaded in it. With the Fiddler Python package, uploading models into Fiddler is simple. Once you initialize the Fiddler API object, you can programmatically harness the power of the Fiddler Platform to interpret and explain your\\xa0models.It can be done in four simple\\xa0steps:Step 1: Create a project\\u200a—\\u200amodels live inside a project. Think of it as a directory for\\xa0models.Step 2: Upload the dataset that the model uses\\u200a—\\u200aa dataset can be linked to multiple\\xa0models.Step 3: Edit our package.py template\\u200a—\\u200apackage.py is the interface by which the Fiddler Platform figures out how to load your model, run predictions on it and explain model predictions. More detailed instructions for doing and testing this code will be shared along with the demo notebook for this example. Rest assured that it’s quite easy and well laid\\xa0out.Step 4: Upload the model and associated files using our custom_model_import function\\u200a—\\u200aall the files must be inside a folder, which is provided to the function.With four simple steps, the model is now part of Fiddler. You can now use Fiddler’s model analysis and debugging tools, in addition to using it to serve explanations at scale. If this is a production model, you can use Fiddler’s Explainable Monitoring tools to monitor live traffic for this model, analyze data drift, outliers, data integrity of your pipeline, and general service\\xa0health.Analyzing the model in\\xa0FiddlerHere we show a brief example of how we can use Fiddler’s NLP explain tools to debug and test this model. The comment in question (not from the dataset) is “that man is so silly” We can see that this is a toxic comment. And the word silly is the term that should get the most blame for the toxicity.From the attributions above, we can see that the term ‘silly’ indeed gets the most blame. Now we can see that ‘man’ also gets a positive attribution. Is it because the model believes that ‘man’ is a toxic word, or is it because ‘man’ in this case is the subject of discussion\\xa0? We can use the text edit tool to try and change it to boy and see if it also gets a high positive attribution‘Boy’ in fact gets an even higher positive attribution, telling us that the model likely thinks both terms to be toxic, and ‘boy’ more so. It’s quite possible that it’s because the word ‘boy’ occurs in more toxic comments than does ‘man’. We can use Fiddler’s model analysis feature to check if this is\\xa0true.This is indeed the case, as the very rough SQL query shows us. A sentence containing boy is toxic roughly a third of the time, as opposed to almost a seventh for ‘man’. Do note that this does not constitute hard proof for the model’s behavior, and neither is it necessarily a fact that ‘boy’ will always get a higher toxic attribution than ‘man’. That will need much deeper analysis to prove, which we plan to address in a subsequent post, along with a thorough investigation of the biases of the model and dataset in\\xa0general.ConclusionBoth teams are very excited about the potential of this collaboration on furthering model explainability research and applications. As a first step we’ve made the two interoperable to make it easy for Fiddler users to upload PyTorch models. We plan to share the results of our work regularly, so stay\\xa0tuned.Fiddler & Captum join hands to enhance Explainable AI offerings was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Authors:By Wenqi Li, Guotai Wang and Wentao\\xa0ZhuThe Medical Open Network for AI (MONAI), is a freely available, community-supported, PyTorch-based framework for deep learning in healthcare imaging. It provides domain-optimized, foundational capabilities for developing a training workflow.Building upon the GTC 2020 alpha release announcement back in April, MONAI has now released version 0.2 with new capabilities, examples, and research implementations for medical imaging researchers to accelerate the pace of innovation for AI development. For more information, see NVIDIA and King’s College London Announce MONAI Open Source AI Framework for Healthcare Research.Why MONAI research?MONAI research is a submodule in the MONAI codebase. The goal is to showcase the implementation of research prototypes and demonstrations from recent publications in medical imaging with deep learning. The research modules are regularly reviewed and maintained by the core developer team. Reusable components identified from the research submodule are integrated into the MONAI core module, following good software engineering practices.Along with the flexibility and usability of MONAI, we envision MONAI research as a suitable venue to release the research code, increase the research impact, and promote open and reproducible research. Like all the other submodules in MONAI, we welcome contributions in the forms of comments, ideas, and\\xa0code.In this post, we discuss the research publications that now have been included with a MONAI-based implementation, addressing advanced research problems in medical image segmentation. MONAI is not intended for clinical\\xa0use.COPLE-Net: COVID-19 Pneumonia Lesion Segmentation NetworkLAMP: Large Deep Nets with Automated Model Parallelism for Image SegmentationCOPLE-Net: COVID-19 Pneumonia Lesion Segmentation NetworkSegmentation of pneumonia lesions from CT scans of COVID-19 patients is important for accurate diagnosis and follow-up. In a recent paper, the leading author, Guotai Wang from University of Electronic Science and Technology of China, and the team propose to use deep learning to automate this task. For more information, see A Noise-robust Framework for Automatic Segmentation of COVID-19 Pneumonia Lesions from CT\\xa0Images.Figure 1. Complex appearances of pneumonia lesions in CT scans of COVID19 patients. Scans (a-c) are from three different patients, where red arrows highlight some lesions. Scan (d) shows the annotations of © given by different observers.Acquiring a large set of accurate pixel-level annotations of the pneumonia lesions during the outbreak of COVID-19 is challenging. This research mainly deals with learning from noisy labels for the segmentation task.One of the key innovations of the research is an enhanced deep convolutional neural network architecture. This architecture has the following features:It uses a combination of max-pooling and average pooling to reduce information loss during downsampling.It employs bridge layers to alleviate the semantic gap between features in the encoder and\\xa0decoder.It employs an ASPP module at the bottleneck to better deal with lesions at multiple\\xa0scales.Figure 2. The proposed COPLE-Net architecture.The novel architecture is made available in MONAI. The key network components, such as MaxAvgPool and SimpleASPP, could be conveniently integrated into other deep learning pipelines:https://medium.com/media/2654541da9e06054acd47b7263f07c0c/hrefThe image preprocessing pipeline and pretrained model loading could be done in a few Python commands with\\xa0MONAI:https://medium.com/media/1162ed16c28b328e545b8ec7d9636f46/hrefThe PyTorch users would benefit from the MONAI medical image preprocessors and domain-specific network blocks. At the same time, the code shows the compatibility of MONAI modules and the PyTorch native objects such as torch.utils.data.DataLoader, thus facilitating the easy adoption of MONAI modules in general PyTorch workflows.Figure 3. Visual comparison of segmentation performance of the COPLE-Net with different loss functions.In the scenario of learning from noisy labels for COVID-19 pneumonia lesion segmentation, the experimental results of the COPLE-Net demonstrate that the new architecture can achieve higher performance than state-of-the-art CNNs.LAMP: Large Deep Nets with Automated Model Parallelism for Image SegmentationDeep learning models are becoming larger because increases in model size can offer significant accuracy gain. Through automated model parallelism, it is feasible to train large deep 3D ConvNets with a large input patch, even the whole image. For more information about the possibility of the automated model parallelism for 3D U-Net for medical image segmentation tasks, see LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation.Figure 4. (Top) The long-range skip-connection hinders the parallelism in the U-Net. (Bottom) We explicitly construct a variant of U-Net to remove the long-range dependency in the U-Net. The parallel U-Net has higher parallel efficiency and throughput.Figure 5. Partitioning model.In Figure 5, a deep model is partitioned across three GPUs (a). F_k is the forward function of the k-th cell. B_k is the back-propagation function which relies on both B_k+1 from the upper layer and feature F_k. Conventional model parallelism has low device utilization because of the dependency of the model (b). The pipeline parallelism splits the input minibatch to smaller micro-batches (c) and enables different devices to run micro-batches simultaneously. Synchronized gradient calculation can be applied\\xa0last.The MONAI research implementation shows straightforward implementations by using preprocessing modules such as the following:AddChannelDictComposeRandCropByPosNegLabeldRand3DelasticdSpatialPaddIt also uses network modules, such as Convolution, and the layer factory to easily handle 2D or 3D inputs using the same module interface. The loss and metrics modules make the model training and evaluation simple. This implementation also includes a working example of training and validation pipelines.Figure 6. Segmentation accuracy (Dice coefficient, %) and inference time (s) comparisons among 3D U-Net and 3D SEU-Net of different sizes (#filters in the first convolutional layer: 32, 64, 128) and different input sizes (64×64×64, 128×128×128, whole image or 192×192×192) on Head and Neck nine organ auto-segmentation and decathlon liver and tumor segmentation datasets.This research demonstrates the following:A large model and input increase segmentation accuracy.The large input reduces inference time significantly. LAMP can be a useful tool for medical image analysis tasks, such as large image registration, detection, and neural architecture search.SummaryThis post highlights how deep learning research for medical imaging could be built with MONAI. Both research examples use the representative features from MONAI v0.2.0, which allows for fast prototyping of research\\xa0ideas.For more information about MONAI v0.2.0, see the following resources:https://monai.ioProject-MONAI/MONAI GitHub\\xa0repoModules in\\xa0v0.2.0Getting started with\\xa0MONAIAuthors Headshots &\\xa0BioGuotai Wang is an Associate Professor at University of Electronic Science and Technology of China. He holds a Ph.D. in medical and biomedical imaging from University College London. His interests include medical image analysis, computer vision, and deep learning.Wenqi Li is a senior applied research scientist at NVIDIA, focusing on computer vision and machine learning techniques for medical image analysis. Wenqi received his Ph.D. in applied computing from The University of Dundee (Scotland, UK) in\\xa02015.Personal webpage: https://wyli1072.appspot.com/Wentao Zhu joined NVIDIA as a research scientist in 2019. His research spans machine learning applications in healthcare and computer vision. In particular, he has spearheaded research in object detection, segmentation, image registration, action recognition, weakly supervised learning and detection, deep probabilistic graphical models.Wentao received his B.Sc in Computational Mathematics from Shandong University in 2012, his M.Sc in Computer Science from Chinese Academy of Sciences in 2015 and his PhD in Computer Science from U.C. Irvine in\\xa02019.Personal Webpage: https://wentaozhu.github.io/Accelerating Deep Learning Research in Medical Imaging Using MONAI was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " \"Authored by Dan Malowany at Allegro\\xa0AIThis blog post is a first of a series on how to leverage PyTorch’s ecosystem tools to easily jumpstart your ML/DL project. The first part of this blog describes common problems appearing when developing ML/DL solutions, and the second describes a simple image classification example demonstrating how to use Allegro Trains and PyTorch to address those problems. In the coming week, we’ll release blog posts as a part of this series, focusing on examples for different use cases and data types, such as, text and audio. However, the principles demonstrated in this post and the others are all completely data agnostic.Jump start your ML/DL project with PyTorch and Allegro\\xa0TrainsAllegro Trains is an open-source machine learning and deep learning experiment manager and ML-Ops solution developed at Allegro AI that boosts GPU utilization and the effectiveness and productivity of AI teams. This solution provides powerful, scalable experiment management and ML-Ops tools with zero integration effort. As a part of the PyTorch ecosystem, Allegro Trains helps PyTorch researchers and developers to manage complex machine learning projects more easily. Allegro Trains is data agnostic and can be applied to structured data, image processing, audio processing, text processing and\\xa0more.Challenges in Deep Learning\\xa0ProjectsEvery data scientist knows that machine and deep learning projects include way more tasks than just choosing the model architecture and training the model on your data. Machine learning projects include ongoing trial and error efforts that resemble lab scientific experiments more than a software development workflow. This is the reason that training sessions are commonly called experiments and a platform that helps manage these sessions as an experiment manager.This unique workflow gives rise to several challenges that might substantially complicate and prolong the time a machine learning project converges to the required\\xa0results.Since a project includes numerous experiments, you need a simple and easy way to track these experiments, log all the hyperparameters values being used and enable reproducing an experiment that was successful even weeks and months later, when new data arrives. To do so you need to integrate an experiment manager to your workflow.Another known challenge are the DevOps efforts that can consume a big chunk of the data scientist’s time if not managed properly. Training models is a process that takes time\\u200a—\\u200ahours and even days. As such, it is common practice to have training machines\\u200a—\\u200alocal and cloud machines\\u200a—\\u200aallocated to perform the training sessions, while the data scientists’ own machines are reserved for further development of the project’s codebase. Managing all these machines, requires an ongoing DevOps effort: Setting up the machines, introducing a queue management system to manage the stream of experiments being sent to the machines and monitoring the status (GPU, CPU, memory) of the machines.Furthermore, with time, different data scientists will use different versions of deep learning frameworks, such as PyTorch, that require different versions of Cuda, Cudnn and other packages. In addition, the different Python package versions will change from one person to another. Creating and spinning up adequate containers for each machine based on the experiment being sent can be frustrating.To solve these challenges and more, you are encouraged to use Allegro Trains and other tools in the PyTorch ecosystem.Let’s take a simple image processing example to demonstrate how these tools can help solve these challenges and many more. The full code can be found\\xa0here.Allegro Trains to the\\xa0RescueThe first tool we will discuss is the Allegro Trains experiment manager and ML-Ops solution. Trains supports experiment tracking, analysis, comparison, hyperparameter tuning, automation, reproducibility and a variety of additional features. It is a suite of three open source components:Trains Python package\\u200a—\\u200aThis Python package allows you to integrate Allegro Trains into your Python code, with only two lines of\\xa0code.Trains Server\\u200a—\\u200aThis is Allegro Trains’ backend infrastructure. You can deploy your own self-hosted Trains Server in a variety of formats, including pre-built Docker images for Linux, Windows 10, macOS, pre-built AWS EC2 AMIs, and Kubernetes.Trains Agent\\u200a—\\u200aTrains Agent is the DevOps component of Allegro Trains that enables remote experiment execution, resource control and automation (like Trains’ built-in Bayesian hyperparameter optimization).Allegro Trains architectureOne of the great things about Allegro Trains is that there is zero integration effort to use with PyTorch. You only need to add the following two lines of code at the top of your main script and you are set to\\xa0go:from trains import Tasktask = Task.init(project_name='Image Classification', task_name='image_classification_CIFAR10')Allegro Trains organizes your PyTorch developments into projects, so once you will execute your code your experiment will be logged under the relevant project in the web app. Now that we integrated our code to Allegro Trains, we can enjoy all the benefits that come with\\xa0it:Reproducibility\\u200a—\\u200aAll the execution data for each experiment is\\u200a—\\u200aautomagically\\u200a—\\u200alogged on the Trains Server: git repository, branch, commit id, uncommitted changes and all used Python packages (including their specific versions at time of execution). This ensures that it is possible to reproduce the experiment at any time. We are all familiar with cases where a package version changes and our script simply doesn’t work anymore. This feature helps us avoid having to troubleshoot such frustrating cases.Improved teamwork\\u200a—\\u200aWith Allegro Trains there is continuous sharing of what each team member is doing, which enables visually brainstorming the results, effectively debating issues and possible remediations with teammates and sharing an experiment that performed well on one use case to easily be applied to another use case by a teammate working on another\\xa0project.Effortless experiments tracking and analysis\\u200a—\\u200aAllegro Trains web app includes a variety of analysis and comparison tools, such as creating a leaderboard ranking all the team’s experiments based on a chosen metric, parallel coordinates and\\xa0more.There are many more features of and benefits from using Allegro Trains, some of which we will discuss later on. For now let’s go back to our\\xa0code.Next you want to make sure all the parameters are reflected in the experiment manager web app and that there are no “magic numbers” hidden in the code. You can use the well known argparse package and Allegro Trains will automatically pick it up. Or you can just define a configuration dictionary and connect it to the Allegro Trains task\\xa0object:configuration_dict = {'number_of_epochs': 3, 'batch_size': 4, 'dropout': 0.25, 'base_lr': 0.001}configuration_dict = task.connect(configuration_dict)Now it is time to define our PyTorch Dataset object. It is always useful to use the built-in datasets that come with PyTorch’s domain libraries. Here we will use the CIFAR10 dataset that can be easily loaded with the popular torchvision. We will also use PyTorch’s DataLoader that represents a Python iterable over a dataset. The DataLoader supports single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory\\xa0pinning:https://medium.com/media/f14e4c0632e6c3b72cc8640d40edc083/hrefNext we will define our model. PyTorch enables easy and flexible model configuration that makes researching model architecture as easy as possible. In our example, we will create a simple\\xa0model:https://medium.com/media/f3986c52e4addeea8f17e51695f1d5a1/hrefIt is important to note that torchvision also includes a useful set of popular pretrained models that can be a great backbone basis for every model you want to build. You can have a look at the torchvision model zoo list\\xa0here.Although TensorBoard is part of the TensorFlow framework, it is fully integrated into PyTorch as well. This enables an easy way to report scalar values (such as loss, accuracy, etc.), images (for example the images we fed into our model for debug purposes) and more. Here comes another automagical feature of Allegro Trains\\u200a—\\u200ait will automatically pick up all the reports sent to TensorBoard and will log them under your experiment in the web app. So let’s add TensorBoard to our code and use it to report the loss value during the training\\xa0session:https://medium.com/media/a3bb0b58286a69e0adb9ec6d8517bcd7/hrefDuring the test session that comes at the end of each epoch, we can report some debug images with their ground truth labels and the prediction made by the model, as well as the accuracy per label and the total accuracy:https://medium.com/media/e588946a9e9f1362ae3c54a9b9da74e2/hrefFinally, at the end of the experiment, we want to save the model. So we will finish our code\\xa0with:torch.save(net.state_dict(), './cifar_net.pth')Allegro Trains will automatically detect that you are saving the model and will register the model location to the experiment in the Allegro Server. It is recommended to upload a copy of the model to a centralized storage (local or cloud), and Allegro Trains can do that for you, if such storage is available. Just easily add the centralized storage location when initializing Allegro Trains task object, as explained here, and the snapshot will be stored in the designated storage and will be linked to your experiment, so you will have access to it from any of your machines.Now all we have to do is run the experiment and watch its progress in the webapp. Allegro Trains not only logs all your reports, but also adds status reports on the machine and its\\xa0GPU.Note: As we didn’t install a Trains-server in this blog, the experiment will be logged on the Allegro Trains demo server. This demo server is meant for easy and fast experimentation with the Allegro Trains suite. Allegro Trains Python package automatically works with the demo server, if a self-hosted Trains-server wasn’t installed.Part 1 - Snapshot of the scalars reports in the Allegro Trains\\xa0webappPart 2— Snapshot of the scalars reports in the Allegro Trains\\xa0webappSnapshot of the debug images reports in the Allegro Trains\\xa0webappNow comes one of the best features of Allegro Trains: Once your experiments are logged into Trains Server, you can enjoy ML-Ops capabilities. All you need to do is install Trains Agent on each machine designated for executing the experiments. Trains Agent is a daemon that spins a container for you and runs your code. Whether it is an office computer or cloud machine, Trains Agent will make it available to the team members, with full queue management and machine diagnosis capabilities.The installation of trains-agent is as simple\\xa0as:pip install trains-agentOnce installed, you execute a simple configuration command and follow the instructions:trains-agent initIn order to register the machines into Trains Server, you simply create queues in the webapp and set the machine to listen to a specific\\xa0queue:trains-agent daemon -queue my_queueAll you have got left to do is to choose the task in the webapp, change the hyper parameters and trains-agent will take care of the rest. You can also name a container from DockerHub you want Trains Agent to use, when running your experiment. For example, just state that the requested docker is “nvidia/cuda:10.0-cudnn7-runtime” and your experiment will be executed within this docker. Now you can stop harassing the DevOps team every time you need a new Cuda version in your container. Python packages versions can also be updated from the webapp, so you don’t need a new container everytime you want to update the version of the numpy\\xa0package.SummaryMachine learning is an exciting field that has numerous applications. However, managing machine learning projects includes addressing many unique challenges. Companies tend to either neglect taking the time and effort to form an infrastructure that helps cope with those challenges, or try to build it themselves. In both cases, they end up diverting too much effort and focus from the core machine & deep learning research and development. The PyTorch ecosystem includes a set of open source tools that once integrated into your workflow, will boost the productivity of your machine learning\\xa0team.In this tutorial we demonstrated how using Allegro Trains, Torchvision and PyTorch built-in support for TensorBoard enables a more simple and productive machine and deep learning workflow. With zero integration efforts and no cost, you get an experiment management system and an ML-Ops solution.To learn more about Allegro Trains, reference its documentation. In the next blog post of this series we will demonstrate how to leverage the tools discussed here to initiate an automatic bayesian optimization hyperparameter search on your experiment.ML/DL Engineering Made Easy with PyTorch’s Ecosystem Tools was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\n",
       " \"PyTorch Lightning: MetricsWith PyTorch Lightning 0.8.1 we added a feature that has been requested many times by our community: Metrics. This feature is designed to be used with PyTorch Lightning as well as with any other PyTorch based code. In this blog post, we’ll explain what Metrics is and how you can get\\xa0started.What’s a metric and why do I need\\xa0this?The purpose of metrics in general is to allow you to monitor and quantify the training process. If you are using more advanced techniques like learning-rate scheduling or early stopping, you are probably using metrics for this. While you could also use losses for this, metrics are preferred since they represent the training goal better. This means, that while the loss (like cross-entropy) pushes the network’s parameters into the correct direction, metrics can show some additional insights on network behavior.In opposite to losses, they also don’t have to be differentiable at all(in fact many of them aren’t), but some of them are. If the metric itself is differentiable and it is implemented using pure PyTorch it can also be used to backpropagate over it and use it for whatever fancy research you want to\\xa0do.In the mathematical sense, a metric is defined as a distance between each pair of elements in a set. While this definition also holds for the deep-learning understanding of a metric, some of the mathematical constraints of a metric must not be fulfilled here, since we don’t need them to be symmetric in all cases and they also don’t necessarily have to hold the triangle inequality.How to use\\xa0itNow we all have a basic understanding of metrics, let me explain how you can use them in PyTorch Lightning.Getting started steps are very simple. There are two different ways to get started: a functional and a module based way. Let’s first have a look at the functional way, since this one is easier\\xa0one.Functional MetricsBasically you only write your metrics as a function using only torch operations.https://medium.com/media/e413b7db3be093a8556dc93abb5ed1d8/hrefOkay, that was really easy, right? This is plain PyTorch code and this does nothing special. But this also worked before before we introduced the Metrics package. So why do we need this package at\\xa0all?Creating metrics like a function this way works but you need to manually make sure the inputs and outputs are all tensors, are all on correct devices and are all of the correct type. This is where PyTorch Lightning’s automation approach\\xa0starts.Take a look at the following example:https://medium.com/media/a0546d24e7050357fe2cea7ed0d6b619/hrefYou can notice the @tensor_metric() decorator. This actually converts all inputs to tensors and all outputs to tensors as well (in case you somehow change the type of result here). Additionally it makes sure to synchronize the Metric's output across all DDP nodes if DDP was initialized.Besides tensor_metric, there are two other decorators: numpy_metric and tensor_collection_metric. While tensor_collection metric only converts all occurences of numbers and numpy arrays (to avoid errors due to the fact, that some collections (like lists of lists with different lengths) cannot be converted to tensors) but also syncs across DDP nodes, numpy_metric converts all inputs to numpy arrays and all outputs to torch tensors. This enables you to basically use your favorite numpy code as a Metric as well and still get automated DDP\\xa0syncing.Note: We strongly recommend to use/write native tensor implementations whenever possible, since for numpy Metrics each call requires a GPU synchronization and thus may slow down your training substantiallyModule Metrics InterfaceThe easiest way to provide a module interface for your Metric if you already have a functional interface is as\\xa0follows:You just derive a class from our base class and call your functional Metric within the\\xa0forward:https://medium.com/media/0cb158f1bdda2f413a5879fdb090a3cb/hrefin this example TensorMetric already deals with the DDP sync and input/output conversion.There are 3 more metric base classes besides TensorMetric:NumpyMetric: wrapper for metric functions implemented with\\xa0numpyTensorCollectionMetric: wrapper for metric whose outputs cannot be converted to torch.Tensor's completely (like list of lists with different length)Metric: The most basic class. Does not do DDP sync and no input/output conversion. This class should be used for functional metrics, that already handle conversion on their own (for example if they are decorated with @tensor_metric)Now you may ask, what’s the advantage of these modular interfaces over functional interfaces?The first one is quite obvious: Metric is a class derived from torch.nn.Module. That means, you also gain all the advantages from them like registering buffers whose device and dtype can be easily changed, by changing the metrics device or dtype. So you don't have to take care of this yourself.The next one: We extended torch.nn.Module by a device property. So you can always access on which device your tensors should be (and we also use this for automated device changes if necessary).Third, In case you want a different ddp reduction than the standard one or you only want to reduce across a specific group of processes, you can just specify this upon initalization of these classes, while on functionals, the decorator specifies these arguments, meaning you cannot change them that\\xa0easily.Fourth: We plan to introduce additional things for these classes like automated metric specific aggregation across batches, automated metric evaluation etc, which will also come built-in with these\\xa0classes.This also brings me to my next and almost last\\xa0point:Future Plans with this\\xa0packageCurrently, all of these Metrics can be used within or without PyTorch Lightning. We will also make sure to keep it that way, BUT we will introduce some convenience features like the ones mentioned above, that probably will be deeply integrated with PyTorch Lightning, but not that easily usable without\\xa0it.Already Available MetricsI spent a lot of time to explain how metrics work in PyTorch Lightning, but our aim with this package is to collect and consolidate common metrics with a single interface for more standardized usage and research. Therefore we also have support for all the SciKit-Learn Metrics.Note: since these Metrics are implemented with numpy, they can also slow down your training substantiallyWe also started implementing a growing list of native Metrics like accuracy, auroc, average precision and about 20 others (as of\\xa0today!).You can see the documentation of the Metrics’ package\\xa0here.DDP SyncingWhenever you write some fancy application and want to run it in a distributed fashion, you had to sync metrics/results manually. That’s over now! The module interface synchronises the Metric’s outputs across a specific process group with a specific operation (per default it’s sum). We are already working on a separate version that includes Metric-specific reductions.Usage without PyTorch LightningAs already mentioned, these metrics (even the already implemented ones!) can also be used without anything else from PyTorch Lightning!Let’s have a look at this short\\xa0example:https://medium.com/media/4628c5e2253ed3a0b03bec319eba5501/hrefAs you see, you can use it by just importing the Metric without any changes required!At the very end, I also have one thing to ask you for: If you have an implementation of a Metric, we did not yet implement, please consider opening an issue and ping me (@justusschock) for discussion and (guidance on) implementation. I’ll try my best to answer as soon as possible to make sure, we all get those Metrics standardized.PyTorch Lightning: metrics was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\n",
       " \"ProgrammingThe objective of this article is to read data from the Oracle DB table and push the records in JSON format to Kafka Broker and then read messages from Kafka Broker and insert the JSON messages to MongoDB collection.The blog contains a fundamental ETL messaging system build using Oracle as a source, Kafka as middleware, and MongoDB as the\\xa0target.This is my third blog of Kafka series, previous two blog links contain conceptual details of Kafka, link as\\xa0below:https://medium.com/towards-artificial-intelligence/getting-started-with-apache-kafka-beginners-tutorial-d38e3634706c?source=friends_link&sk=2b98454c001fb88527fff8c2217947e2https://medium.com/towards-artificial-intelligence/diving-deep-into-kafka-29160f32d408?source=friends_link&sk=0cf78cc0ee1ef12f0bac1634e71d1550Kafka ETL ecosystemConnect to Oracle and Extract\\xa0Data:#import required librariesfrom json import dumpsimport jsonfrom kafka import KafkaProducerimport cx_Oracleconn = cx_Oracle.connect('scott/scott@oracle')cursor=conn.cursor()query='select empno,ename,sal from emp'result=cursor.execute(query)data=[]#convert data into JSON formatfor row in result:    data.append({'empno':row[0], 'ename':row[1], 'sal':row[2]})conn.close()for records in data:    print(records)Oracle dataAs MongoDB accepts data in JSON format, we will convert the data into JSON key-value format.2. Create a Producer and push messages to Kafka\\xa0Broker:producer = KafkaProducer(bootstrap_servers=[‘localhost:9092’], value_serializer=lambda x:json.dumps(x).encode(‘utf-8’))#push message to kafka topic mongo_poctry: for val in data: producer.send(‘mongo_poc’,val)except Exception as e: print(e)Kafka messagesmessages are in key-value pair\\xa0format3. Create a Consumer and insert records to\\xa0MongoDB:#import required librariesfrom kafka import KafkaConsumerfrom json import loadsimport jsonfrom pymongo import MongoClient#mongodb connection detailsclient = MongoClient(‘localhost:27017’)db=client.mydb#write Kafka consumerconsumer = KafkaConsumer(‘mongo_poc’, bootstrap_servers=[‘localhost:9092’], auto_offset_reset=’earliest’, enable_auto_commit=True, group_id=’my-group’, consumer_timeout_ms=1000, value_deserializer=lambda x: loads(x.decode(‘utf-8’)))#read messages from Kafka and insert into mongodb collection named kafka_mongofor message in consumer: msg = message.value print(msg) db.kafka_mongo.insert(msg) consumer.commit()MongoDB DocumentsMongoDB collection can be construed as an RDBMS Table. Each record in MongoDB is termed as Document. So an Oracle record is MongoDB document.find() method is used to query the collection- kafka_mongoVisit my MongoDB blog to understand basic CRUD operations, link as below: https://mongocrud.blogspot.com/2020/03/mongodb-crud-operations.htmlHurray, we have successfully read the messages from Kafka Broker and inserted them into MongoDB collection.Summary:· Python Oracle connection using cx_Oracle· JSON data\\xa0creation· Push messages using Kafka\\xa0Producer· Kafka Consumer Insert Data into MongoDB collectionThanks to all for reading my blog, and If you like my content and explanation, please follow me on medium and share your feedback, which will always help all of us to enhance our knowledge.Kafka Python Data processing was originally published in Towards AI\\u200a—\\u200aMultidisciplinary Science Journal on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\n",
       " 'The field of data science is flexible and adaptable, making it a path that will remain relevant regardless of industry or circumstance. Given the COVID-19 crisis, many individuals are questioning their ability to find a new position, however, there are still many companies looking to hire a data scientist right\\xa0now.[Related article: Companies Hiring Data Scientists: Summer\\xa02020]Companies Hiring Data ScientistsThere’s a wide range of industries hiring data scientists right now. It shouldn’t be a surprise that health insurance and biopharmaceutical companies are looking to ramp up their workforce to cope with this new dynamic. Tech giants are always looking to innovate, and are therefore still hiring in troves. While some retail stores have hit a slump, some are excelling. Companies like Target are thriving as they sell food and other essential items, and computer giants like Staples are still hiring to help people with their work from home\\xa0needs.Here’s a look at a few companies hiring data scientists summer\\xa02020:Technology: CGI Group, Apeiro Technologies, Apple, Spotify, Twitter,\\xa0IBMFinance: Discover, Capital\\xa0OneHealthcare & Insurance: Cenpatico, Humana, Libert\\xa0MutualAvailable Data Science\\xa0JobsHere are a few active job opportunities that you can apply for now. Please note that these jobs may close at any time, so apply quickly! The salary figures are estimates and are not guaranteed.Quantum Data Scientist (Industrial-Process sector)\\u200a—\\u200aIBM | $78k\\u200a—\\u200a$125kStaff Machine Learning Engineer, Health ML\\u200a—\\u200aTwitter | ~$115k\\u200a—\\u200a$164kData Scientist, Analytics, WhatsApp Privacy\\u200a—\\u200aWhatsApp Inc | $148k\\u200a—\\u200a$159kLead Data Scientist, Customer Journey Product\\u200a—\\u200aCimpress/Vistaprint | $117k\\u200a—\\u200a$186kSr. Manager, Data Science\\u200a—\\u200aSlack | $144k\\u200a—\\u200a$158kSenior / Data Scientist, Ads and Messaging R&D\\u200a—\\u200aSpotify | $121k\\u200a—\\u200a$182kManager, Data Science (Machine Learning)\\u200a—\\u200aUber | $170k\\u200a—\\u200a$180kData Science Manager/R&D\\u200a—\\u200aZS | $171k\\u200a—\\u200a$206kResearch Data Scientist\\u200a—\\u200aFacebook | $106k\\u200a—\\u200a$206kData&AI Cloud Solution Architect\\u200a—\\u200aMicrosoft | $91k\\u200a—\\u200a$198kWe update our jobs board frequently with other companies hiring data scientists during COVID-19\\u200a—\\u200abe sure to follow jobs.opendatascience.com for more data science\\xa0jobs!Original post\\xa0here.Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday.',\n",
       " 'The internet and the World Wide Web (WWW), is probably the most prominent source of information today. Most of that information is retrievable through HTTP. HTTP was invented originally to share pages of hypertext (hence the name Hypertext Transfer Protocol), which eventually started the\\xa0WWW.This process occurs every time we request a web page through our devices. The exciting part is we can perform these operations programmatically to automate the retrieval and processing of information.This article is an excerpt from the book Python Automation Cookbook, Second Edition by Jamie Buelta, a comprehensive and updated edition that enables you to develop a sharp understanding of the fundamentals required to automate business processes through real-world tasks, such as developing your first web scraping application, analyzing information to generate spreadsheet reports with graphs, and communicating with automatically generated emails.In this article, we will learn how to leverage the Python language to fetch HTTP. Python has an HTTP client in its standard library. Further, the fantastic request modules make obtaining web pages very convenient.[Related article: Web Scraping News Articles in\\xa0Python]Interacting with\\xa0formsA common element present in web pages is forms. Forms are a way of sending values to a web page, for example, to create a new comment on a blog post, or to submit a purchase.Browsers present forms so you can input values and send them in a single action after pressing the submit or equivalent button. We’ll see how to create this action programmatically in this\\xa0recipe.https://odsc.com/Getting readyWe’ll work against the test server https://httpbin.org/forms/post, which allows us to send a test form and sends back the submitted information.The following is an example form to order a\\xa0pizza:Figure 1 Rendered\\xa0formYou can fill the form in manually and see it return the information in JSON format, including extra information such as the browser being\\xa0used.The following is the frontend of the web form that is generated:Figure 2: Filled-in formThe following screenshot shows the backend of the web form that is generated:Figure 3: Returned JSON\\xa0contentWe need to analyze the HTML to see the accepted data for the form. The source code is as\\xa0follows:Figure 4: Source\\xa0codeCheck the names of the inputs, custname, custtel, custemail, size (a radio option), topping (a multiselection checkbox), delivery (time), and comments.How to do\\xa0it…1. Import the requests, BeautifulSoup, and re\\xa0modules:>>> import requests >>> from bs4 import BeautifulSoup >>> import re2. Retrieve the form page, parse it, and print the input fields. Check that the posting URL is /post (not /forms/post): >>> response = requests.get(‘https://httpbin.org/forms/post’)>>> page = BeautifulSoup(response.text) >>> form = page.find(\\'form\\') >>> {field.get(\\'name\\') for field in form.find_all(re. compile(\\'input|textarea\\'))} {\\'delivery\\', \\'topping\\', \\'size\\', \\'custemail\\', \\'comments\\', \\'custtel\\', \\'custname\\'}3. Note that textarea is a valid input and is defined in the HTML format. Prepare the data to be posted as a dictionary. Check that the values are as defined in the\\xa0form:>>> data = {\\'custname\\': \"Sean O\\'Connell\", \\'custtel\\': \\'123-456- 789\\', \\'custemail\\': \\'sean@oconnell.ie\\', \\'size\\': \\'small\\', \\'topping\\': [\\'bacon\\', \\'onion\\'], \\'delivery\\': \\'20:30\\', \\'comments\\': \\'\\'}4. Post the values and check that the response is the same as returned in the\\xa0browser:>>> response = requests.post(\\'https://httpbin.org/post\\', data) >>> response <Response [200]> >>> response.json() {\\'args\\': {}, \\'data\\': \\'\\', \\'files\\': {}, \\'form\\': {\\'comments\\': \\'\\', \\'custemail\\': \\'sean@oconnell.ie\\', \\'custname\\': \"Sean O\\'Connell\", \\'custtel\\': \\'123-456-789\\', \\'delivery\\': \\'20:30\\', \\'size\\': \\'small\\', \\'topping\\': [\\'bacon\\', \\'onion\\']}, \\'headers\\': {\\'Accept\\': \\'*/*\\', \\'Accept-Encoding\\': \\'gzip, deflate\\', \\'Connection\\': \\'close\\', \\'Content-Length\\': \\'140\\', \\'Content-Type\\': \\'application/x-wwwform- urlencoded\\', \\'Host\\': \\'httpbin.org\\', \\'User-Agent\\': \\'pythonrequests/ 2.22.0\\'}, \\'json\\': None, \\'origin\\': \\'89.100.17.159\\', \\'url\\': \\'https://httpbin.org/post\\'}How it\\xa0works…Requests directly encodes and sends data in the configured format. By default, it sends POST data in the application/x-www-form-urlencoded format.The key aspect here is to respect the format of the form and the possible values that can return an error if incorrect, typically a 400 error, indicating a problem with the\\xa0client.[Related article: Building a Scraper Using Browser Automation]There’s more…Other than following the format of forms and inputting valid values, the main problem when working with forms is the multiple ways of preventing spam and abusive behavior. You will often have to ensure that you have downloaded a form before submitting it, to avoid submitting multiple forms or Cross-Site Request Forgery\\xa0(CSRF).To obtain the specific token, you need to first download the form, as shown in the recipe, obtain the value of the CSRF token, and resubmit it. Note that the token can have different names; this is just an\\xa0example:>>> form.find(attrs={\\'name\\': \\'token\\'}).get(\\'value\\') \\'ABCEDF12345\\'In this article, we learned how to obtain data from the forms of the web, parse it, and print the input fields using Python’s HTTP client. We also explored the role and application of requests, Beautiful Soup, and re–modules.About the\\xa0AuthorJaime Buelta is a full-time Python developer since 2010 and a regular speaker at PyCon Ireland. He has been a professional programmer for over two decades with a rich exposure to a lot of different technologies throughout his career. He has developed software for a variety of fields and industries, including aerospace, networking and communications, industrial SCADA systems, video game online services, and financial services.Editor’s note: Interested in learning more about coding beyond just retrieving webpages through Python? Check out some of these upcoming similar ODSC\\xa0talks:ODSC Europe: “Programming with Data: Python and Pandas”\\u200a—\\u200aIn this training, you will learn how to accelerate your data analyses using the Python language and Pandas, a library specifically designed for tabular data analysis.ODSC Europe: “Introduction to Linear Algebra for Data Science and Machine Learning With Python”\\u200a—\\u200aThe goal of this session is to show you that you can start learning the math needed for machine learning and data science using\\xa0code.',\n",
       " 'Bayes’ theorem is of fundamental importance to the field of data science, consisting of the disciplines: computer science, mathematical statistics, and probability. It is used to calculate the probability of an event occurring based on relevant existing information. Bayesian inference meanwhile leverages Bayes’ theorem to update the probability of a hypothesis as additional data is encountered. But how can deep learning models benefit from Bayesian inference? A recent research paper written by New York University Assistant Professor Andrew Gordon Wilson addresses this question “The Case for Bayesian Deep Learning.” As it turns out, supplementing deep learning with Bayesian thinking is a growth area of research.In this article, I will examine where we are with Bayesian Neural Networks (BBNs) and Bayesian Deep Learning (BDL) by looking at some definitions, a little history, key areas of focus, current research efforts, and a look toward the future. It is common for Bayesian deep learning to essentially refer to Bayesian neural networks.[Related article: Building Your First Bayesian Model in\\xa0R]BDL DefinitionsBDL is a discipline at the crossing between deep learning architectures and Bayesian probability theory. At the same time, Bayesian inference forms an important share of statistics and probabilistic machine learning (where probabilistic distributions are used to model the learning, uncertainty, and observable states).The primary attraction of BDL is that it offers principled uncertainty estimates from deep learning architectures. Uncertainties in a neural network is a measure of how certain the model is with its prediction. With Bayesian modeling, there are two primary types of uncertainty:Aleatoric uncertainty\\u200a—\\u200awhich measures the noise inherent in the observations, such as sensor noise which is uniform in the data set. This kind of uncertainty can’t be reduced even with more data collected.Epistemic uncertainty\\u200a—\\u200acaused by the model itself, so it is also known as model uncertainty. It captures our lack of knowledge about which model generated our collected data. This kind of uncertainty can be reduced by collecting more\\xa0data.https://odsc.com/BDL models typically derive estimations of uncertainty by either placing probability distributions over model weights (parameters), or by learning a direct mapping to probabilistic outputs. Epistemic uncertainty is modeled by placing a prior distribution over a model’s weights and then capturing how much these weights vary over the data. On the other hand, Aleatoric uncertainty is modeled by placing a distribution over the outputs of the\\xa0model.Many data scientists believe that combining probabilistic machine learning, Bayesian learning, and neural networks represents a potentially beneficial practice, however, it’s often difficult to train a Bayesian neural network. For training neural networks, the most popular approach is backpropagation, and for training BNNs, we typically use Bayes by Backprop. This method was introduced in the paper “Weight Uncertainty in Neural Networks,” by Blundell, et al. for learning a probability distribution on the weights of a neural network. The following excerpt from the paper summarizes the approach:“Instead of training a single network, the proposed method trains an ensemble of networks, where each network has its weights drawn from a shared, learnt probability distribution. Unlike other ensemble methods, our method typically only doubles the number of parameters yet trains an infinite ensemble using unbiased Monte Carlo estimates of the gradients.”In December 2019, there was a very compelling BDL workshop aligned with the NeurIPS 2019 conference. This site, with plenty of papers and slide presentations, represents a great learning resource for getting up to speed with BDL. The following is a published summary of the workshop that does a nice job of outlining the progress this field has been\\xa0making:“While deep learning has been revolutionary for machine learning, most modern deep learning models cannot represent their uncertainty nor take advantage of the well-studied tools of probability theory. This has started to change following recent developments of tools and techniques combining Bayesian approaches with deep learning. The intersection of the two fields has received great interest from the community, with the introduction of new deep learning models that take advantage of Bayesian techniques, and Bayesian models that incorporate deep learning elements. Many ideas from the 1990s are now being revisited in light of recent advances in the fields of approximate inference and deep learning, yielding many exciting new results.”The Historical Development of BNNs and\\xa0BDLResearch into the area of BNNs dates back to 1990s with the following short-list of seminal papers in this burgeoning field:“Keeping the neural networks simple by minimizing the description length of the weights,” by Geoffrey E. Hinton and Drew van\\xa0Camp.“Transforming Neural-Net Output Levels to Probability Distributions,” by John S. Denker and Yann\\xa0leCun.“Bayesian Learning for Neural Networks,” by Radford M.\\xa0Neal“A Practical Bayesian Framework for Backprop Networks,” by David J.C.\\xa0MacKay.Additionally, there is a growing bibliography available on research materials relating to\\xa0BDL.Areas of Focus for BNN\\xa0ResearchUnderstanding what a model does not know is a critical part of a machine learning application. Unfortunately, many deep learning algorithms in use today are typically unable to understand their uncertainty. The results of these models are often taken blindly and assumed to be accurate, which is not always the\\xa0case.It is clear to most data scientists that understanding uncertainty is important. So why isn’t it done universally? The main issue is that traditional machine learning approaches to understanding uncertainty do not scale well for high dimensionality data like images and videos. To effectively understand this data, deep learning is needed, but deep learning struggles with model uncertainty. This is one reason for the rise in the appeal for\\xa0BDL.Another key property of BNNs is their connection with deep ensembles. At a high level, both work to train a set of neural networks and yield predictions using some form of model averaging. One difference is that deep ensembles train these networks separately with different initializations while BNNs directly train a distribution of networks under Bayesian principles. Another difference is that deep ensembles directly average predictions from different networks while BNNs compute a weighted average using the posterior of each network as weights. The implication behind this idea is that BNNs actually incorporates deep ensembles in a certain sense, since the latter is an approximate Bayesian model average. Consequently, deep ensembles’ success essentially brings both inspiration and additional insights to\\xa0BNNs.An additional area of focus relates to the BNNs use of probability distributions of weights instead of having deterministic weights. Due to a softmax function at the output layer to achieve the probability score, it reduces one class output probability score and maximizes the other. This leads to an overconfident decision for one class. This is one of the major difficulties with a point-estimate neural\\xa0network.Finally, Dropout is a widely-used regularization method that assists in reducing overfitting by randomly setting activations to zeros in a given layer. Dropout also can be used to make neural networks “Bayesian” in a straightforward manner, and in order to use it during inference, you just have to keep the Dropout, and sample several models (a process called MC dropout).The Future Development of BNNs and\\xa0BDL[Related article: From Idea to Insight: Using Bayesian Hierarchical Models to Predict Game Outcomes Part\\xa01]The main hurdle for widespread adoption of BNNs and BDL in the past included computation efficiency and lack of publicly available packages. Recent encouraging development has taken a solid step over such hurdles. For example, there’s been considerable work in terms of hardware and software to accelerate computation and new packages such as Edward have been specifically designed for probabilistic modeling and inference.In the future, we can expect significant progress in BNNs for learning with small data, ensembles, along with model compression/pruning. In a broader sense, there also will be much more research based on the general nature of BDL, i.e. utilizing the reasoning ability of probabilistic graphical models for deep learning, in various problem domains such as computer vision, and natural language processing.Editor’s note: There are a number of upcoming ODSC talks on the topic of Bayesian models! Here are a few to check\\xa0out:ODSC West 2020: “The Bayesians are Coming! The Bayesians are Coming, to Time Series”\\u200a—\\u200aThis talk aims to allow people to update their own skill set in forecasting with these potentially Bayesian techniques.ODSC Europe 2020: “Bayesian Data Science: Probabilistic Programming”\\u200a—\\u200aThis tutorial will introduce the key concepts of probability distributions via hacker statistics, hands-on simulation, telling stories of the data-generation processes, Bayes’ rule, and Bayesian inference, all through hands-on coding and real-world examples.',\n",
       " 'One of the best ways to measure mobility data is through humans themselves. Turns out\\u200a—\\u200awe’re great sensors for this kind of thing. Once you’ve anonymized and segmented mobility data taken from our movements, the wealth of information is staggering.Dr. Arturo Amador of Capgemini works with this exact kind of data, gleaning insights into the way human mobility drives the economy and provides a breadth of data not found in traditional IoT sensors. In his talk for ODSC’s 2019 Accelerate AI, “Big Data and Mobility Analytics: What Can We Learn from the Way Things (and Humans) Move?” he gives us a glimpse into the power of big data and mobility.Cities as Economy-DriversCities are living organisms. Analyzing human movement throughout a city produces a graphic that (very poetically) mimics a heartbeat. This movement is tied to the economic drivers of the city\\u200a—\\u200athings like work, transportation, and residency.Aside from massive disruptions\\u200a—\\u200acelebrities or political figures arriving in town, for example\\u200a—\\u200athese movements are reasonably regular. Cities provide value and survive through these self-organizing means.Another source of movement is the transportation of goods. All of a country’s top industries rely in some part on transport, of which, cities are major drivers. Mobility is a fundamental part of life, and nowhere is this more apparent is in a\\xa0city.What this movement tells us is that value is exponential. As cities form and organize, they provide impacts that often survive from civilization to civilization. It’s no accident that cities like Rome or Mexico City have survived multiple iterations of civilization, and this mobility data could finally shed light on exactly\\xa0why.On a business scale, cities are using mobility data to create revenue for the city itself. In Boston, for example, driving a personal car into the city for work every day could cost as much as $20 in tolls while electric vehicles and public transportation pay no tolls. These policies create not only revenue but deep behavioral change over the long\\xa0term.Location Data: From Static to\\xa0DynamicOne of the most significant changes in mobility data is IoT itself. In countries like Norway, plentiful, rich data from many IoT sources allow us to see into the heart of the city. Norway, and the rest of the world, is monetizing this data through tourism, traffic monitoring, public transportation, and public\\xa0health.This surveillance is anonymous but provides vital insights. For tourism, this data could show patterns in travel habits based on nationality without paying for expensive personal monitoring and offers more data than simple static\\xa0methods.Artificial Intelligence is also driving this new era of dynamic analytics. It’s able to find patterns and interpret big data in ways that previous processing wasn’t able to. Now, AI is allowing us to look deep into the mobility of persons throughout cities and the world for real-time insight.For business, this technology also provides insights into the movements of consumers. The data is available anonymously, but it provides critical data for how people move through a store, for\\xa0example.The Privacy ChallengePrivacy in this data collection is challenging, but companies are finding ways to comply\\u200a—\\u200aprivacy by design. Each has benefits and downsides, but all can protect the privacy of individuals without explicit\\xa0consent.Encryption: data at rest and data in\\xa0transit.Masking: Masking techniques such as hashing could be suitable alternatives to encryption where possible.Extrapolation: Preserves privacy but could introduce some uncertaintyPath obfuscation: Pseudo-random noise can preserve privacy by preventing re-inference (but decreases accuracy)Aggregation algorithms: Strengthen privacy frameworks by avoiding exposure of individuals\\u200a—\\u200athink k-anonymity or t-closenessWithout the combination of these privacy measures, its still possible to recover data and reidentify individuals, putting companies in direct violation of compliance measures like\\xa0GDPR.Merging Data\\xa0SourcesWhen you begin merging these data sources, IoT plus local AirBNB data, for example, it creates an even more dynamic picture of the revenue and impact mobility brings into a city. For example, the data from cell phone movement combined with sentiment analysis of AirBNB reviews provides well-rounded insights about\\xa0tourism.Companies are also using wastewater analysis to plan anti-drug campaigns. AI can analyze anonymous data from wastewater based on mobility patterns and drug use. This data can provide realistic data about drug usage that could go against common assumptions. For example, AI revealed that drug consumption goes up per person in the summer. On the surface, it seems that drug consumption goes down, but in reality, with a lower population and higher numbers in wastewater, we find different results than what we\\xa0assume.Big Data Tech and Maritime NavigationFinally, this data is also possible to extrapolate for things. In maritime data, for example, you can predict port demand and saturation. You can also use this data to predict travel times based on factors such as weather, routes, and time of\\xa0year.With this data, route optimization is possible. Because most countries rely heavily on transportation for economic drive, this capability is a huge part win for AI. It makes the production pipeline more efficient and increases value for port cities and countries exporting goods.Making Better Use of\\xa0MobilityDr. Amador believes that this data is a massive boon to the impact and value of cities, allowing countries and businesses to monetize through logical and practical data interpretation. As the world moves towards real-time insights, Dr. Armador believes that we’ll be able to comply both with privacy regulations and build better, more robust data sets through other\\xa0means.Take advantage of ODSC’s learning resources by watching past talks on some of the hottest topics in data science and\\xa0AI.Original post\\xa0here.Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday.',\n",
       " 'The content in this post was presented as a talk at ODSC East 2020. The slides from that presentation are available here.Data science is a relatively new field and practicing data scientists come from a wide variety of backgrounds. This is a great thing for the field because the problems being solved with data science are diverse. However, a lot of data science education focuses on teaching algorithms and techniques for small projects that are developed over hours or weeks, and are much smaller in scope than projects developed in industry.Projects in the industry are highly collaborative and need to stand the test of time. There are a number of situations where work on a project gets difficult such\\xa0as:Your coworker owns a project but is on vacation for a few weeks and you need to run their code for the first\\xa0timeYou’re hired as a company’s first full-time data scientist and inherit a bunch of work from contractors that are no longer availableYou revisit a project that you developed a year ago and need to rediscover what you did and\\xa0whyMost technology workers are familiar with the concept of “but it works on my machine,” but it’s a phrase that we definitely want to avoid. By learning some best practices from software engineering, data scientists can write code that is reproducible and understandable. In this post, we’ll briefly describe the concepts of dependency management, version control, and coding standards and how they’ll make your job a lot\\xa0easier.Dependency management comes from the idea that it should be easy to install everything you need to run multiple projects on any computer. This is valuable\\xa0to:Work on multiple projects at that same time with different requirementsAvoid modifying your global Python environment that system processes may depend\\xa0onMake it easy for a new collaborator to start working on your\\xa0projectEnable you to fix your installation when you accidentally run a command that breaks\\xa0itA dependency is software that is published so that you can use it in your data project. Dependencies that you use may have their own dependencies, so it’s easy for your project to have hundreds of dependencies. For example, in the Python data science world, some common data science dependencies are the libraries NumPy and\\xa0SciPy.There are many tools that will help you manage your dependencies. Speaking generally, these work by providing a way for you to specify your dependencies and then installing the dependencies into an isolated execution environment for each of your projects. Explicitly specifying dependencies and keeping project dependencies isolated makes your code a lot more reproducible!In the Python ecosystem, some of the most popular dependency managers are virtualenv, venv, pyenv, and pipenv. In the R ecosystem, there’s Packrat and renv. There are even some language-agnostic tools such as conda. While every dependency manager has pros and cons, your setup can get complicated if you attempt to use more than one. The important thing is to choose a dependency manager that you like, learn how it works, and then use it consistently every time that you start a new\\xa0project.Version control is used to track changes to a set of files over time. The most popular version control system is Git, so it’s highly recommended that you get familiar with it. While Git itself is universal there are many Git services that you will have to choose between when you decide to host your code online. The most popular are GitHub, GitLab, and Bitbucket.Many people are introduced to version control for the first time when they start working with a team or using open-source software. This can lead to the misconception that version control is only for working with others. But I highly recommend that you always use version control, even if you know you’ll be working on a project alone. The reason for this is that version control can provide context to your code, making it more understandable.When you make a change to a set of files in Git it’s called a commit. Your commits include commit messages and these are your opportunities to provide context to the changes you’re making. Using commands such as `git log` and `git blame` you should be able to answer questions such as “When was a change made?”, “Why was a change made?”, and “Is there additional context I can look up about this change?” (usually, additional context comes via a link to an issue tracker). There are many great guides to writing good Git commit messages, such as this one by Chris\\xa0Beams.Coding standards refer to the practice of establishing rules with your team about how your code is structured and what processes you need to follow. Setting standards will make it easy for you to jump into code or projects that were created by other members of your team. They should also encourage best practices and catch bugs. At a minimum I would create standards around:– Structuring projects (I’m personally a big fan of DrivenData’s Cookiecutter Data Science template)– Using linters to automatically check your code– Writing Git\\xa0commitsI believe that three three software engineering best practices I’ve presented should be adopted by all data scientists. Use a dependency manager so that your environment is reproducible, version control so that your project’s history is traceable, and coding standards to make it easy to navigate your projects. Manage your data projects like a software engineer and start reaping the benefits!About the\\xa0author:Michael Jalkio is a data engineer at Amazon in San Diego. He works in the Buyer Risk Prevention team, whose mission is to keep Amazon stores safe and trustworthy by protecting customer accounts from takeover, fraud, and abuse. Before joining a “big tech” company he worked on an enterprise data warehouse migration project at Petco, and helped build the data science team at a startup called Classy. He’s most passionate about doing work that makes a positive impact and helps give everyone in the world equal opportunity to do what they\\xa0love.Original post\\xa0here.Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday.',\n",
       " 'When you start a learning bootcamp with the goal of learning a set of skills that will enable you to use AI in a particular way, it can be hard to identify the courses that best fit your needs. The mini bootcamp at ODSC West 2020, however, features six specialized learning paths to ensure that you can easily select the courses that will help you achieve your learning\\xa0goals.Below is a list of the Specialization Tracks, some of the topics that they’ll cover, and examples of the types of sessions you can expect for ODSC West in\\xa0October.[Related article: More Excellent Speakers Attending ODSC West\\xa02020]Machine Learning with Python: Statistics, Math, Programming with Pandas, Face Processing, Causal Inference, Explainable AI\\xa0ModelsSustainable Retail Through Open Source, Scraping, and\\xa0NLPSolving the Data Scientist’s Cold-Start Problem with Machine Learning\\xa0ExamplesMachine Learning with R: Building ML Workflows in R, Pipeline Optimization, Time Series Forecasting, Advanced ML AlgorithmsMachine Learning in R Part\\xa0I-IVBuilding Personalized Scores for Customers: How to Combine Different Data Types and Learn in the\\xa0ProcessDeep Learning: ML with scikit-learn, Variational Auto-Encoders, GANs, Neural Networks and TensorFlowTransformers for Computer\\xa0VisionImage Detection as a Service: How we Use APIs and Deep Learning to Support our\\xa0ProductsData Engineering/MLOps: Advanced SQL, CI/CD Pipelines, Kubeflow and Security, ONNX\\xa0RuntimeDeploying ML Pipelines with Open-dataSQL for Data\\xa0ScienceNLP: NLP Fundamentals, Text Processing, Deep Transfer Learning, BERT and Computer Vision TransformersForecasting the Economy with Fifty Shades of\\xa0EmotionsSpark NLP for Healthcare: Lesions Learning Building Real-World AI Systems Veysel Kocaman, PhD Sr. Data Scientist John Snow\\xa0LabsData Visualization: Data and Visual Storytelling, Animating Matplotlib into GIFs, Knowledge Graphs, Data Visualization Using\\xa0D3Animating Data: From matplotlib plots to GIFs Max Humber Lead Instructor General\\xa0AssemblyBringing Data to the Masses Through Visualization Alan Rutter Founder Fire Plus\\xa0AlgebraAs part of the Mini-Bootcamp, you’ll also have the opportunity to participate in several learning experiences throughout the summer, as well as get access to additional resources during the virtual conference. Find the full list of what is included in the Mini-Bootcamp ticket\\xa0belowPre-Bootcamp On-Demand TrainingPre-Bootcamp Live Training (4-day pass\\xa0only)Learning HackathonData Science Fundamentals2 to 3 days of workshops, talks, and training\\xa0sessionsVirtual AI Expo and Demo\\xa0TalksVirtual Career\\xa0Expo[Related article: Here’s What’s New with ODSC West\\xa02020]If you have a particular skill you would like to master, ODSC’s Mini-Virtual Bootcamp at ODSC West can help you do so. Plus, when you register by Friday, you’ll save 60% on your Bootcamp\\xa0pass.',\n",
       " 'Advances in natural language processing have allowed to quantify the intuitive yet elusive notion of sentiment expressed in text and to test its predictive power in relation to changes in social\\xa0systems.Studies in cognitive sciences as well as economics have found that unsettling narrative preceded events such as the Great Depression in the 1920s and the Global Financial Crisis in 2008, suggesting that news sentiment is a means of forecasting the economy (1,2). ECB President Mario Draghi’s “whatever it takes” speech from July 2012 is a great example of a narrative that has impacted markets and the economy. These three words marked the turnaround of the euro crisis, purely achieved by Draghi’s verbal intervention.Newspapers are a proven means for both individuals and institutions to share and distribute information. Most publications have an online presence and generate large amounts of data. This data includes information in the shape of sentiment and opinions about the economy, which is not yet reflected in macro-economic indicators.The Global Database of Events, Language, and Tone (GDELT) (3) is a research collaboration that monitors the world’s newspapers from a multitude of perspectives, extracting items such as themes, emotions, events, mentions of organizations, and persons and locations for every news article analyzed almost in real-time.News sentiment and the economic recovery following COVID-19As the coronavirus spread around the world, governments in many countries were forced to impose strict lockdowns and temporarily close businesses. As a result, a lot of companies are struggling to survive. Many had to lay off employees, leading to a spike in unemployment.The chart shows the average tone, financial uncertainty, and confidence indices based on emotions from GDELT, with news items filtered thematically for “economic growth.”Net sentiment (i.e. positive minus negative tone) from global newspapers has improved notably since bottoming in April this\\xa0year.Levels of financial uncertainty peaked at the height of the outbreak in early spring. Financial uncertainty has since been declining but remains at elevated\\xa0levels.Confidence increased until February, perhaps reflecting confidence in local governments’ ability to contain the virus. The index plunged in March as lockdowns were imposed in countries around the world. Confidence moderately recovered in April but moved mostly sideways in May and June as the longer-term economic repercussions from the COVID-19 outbreak became apparent.According to net sentiment, recovery is well on underway. However, the two other indices tell a somewhat different story. The coronavirus is likely to have a powerful impact on confidence as consumers are staying at home for a prolonged period, possibly feeling pessimistic about the future. With heightened levels of financial uncertainty and worsening financial conditions, household consumption typically falls as savings go up, weighing on economic\\xa0growth.Thoughts and conclusionsNet sentiment lacks the insights that more specific emotions can convey about the economic recovery, as it merely indicates whether positive outweighs negative sentiment. More specific emotions from news narratives can help form a clearer view of the present state of the economy. Both confidence and financial uncertainty suggest that the global economy is only at the beginning of a recovery from this year’s COVID-19 outbreak. The shape and speed of the recovery depend on factors like the economic impact of physical distancing, the effectiveness of government support packages, as well as the pace of the easing of lockdown restrictions and the occurrence of a second\\xa0wave.Traditional GDP forecasts are unreliable during normal market conditions and become even more challenging at present when the virus’s trajectory is unknown. It is difficult to capture the impact of COVID-19 on consumer and business behavior in a timely manner and thus to estimate a likely recovery\\xa0path.To find out more about forecasting the economy with news, narrative, and emotions, join me for my talk at ODSC Europe, “Forecasting the Economy with Fifty Shades of Emotions.”(1) Robert J. Shiller. NARRATIVE ECONOMICS. Jan. 2017(2) David Tuckett et al. “Bringing Social Psychological Variables into Economic Modeling: Uncertainty, Animal Spirits and the Recovery from the Great Recession”. In: IEA World Conference, Jordan (2014)(3) gdeltproject.orgMore on the author/speaker:Sonja Tilly is a PhD candidate at UCL. Her research focuses on forecasting macro-economic variables and stock market movements using narrative and emotions from global newspapers.Sonja has over a decade of experience working in asset management, most recently at Quoniam Asset Management, where she contributed to the development of trading strategies using media sentiment. Prior to that, she was a Quantitative Analyst at Hiscox. Sonja is a CFA Charterholder.',\n",
       " 'Traditionally, neural networks had all their inputs and outputs independent of each other, but in cases, for instance, where it is required to predict the next word of a sentence, information about previous words is essential. Thus, RNN came into existence. Recurrent Neural Network (RNN) is a type of neural network where the output of the preceding step is served as input to the current step. This solved various application issues with the help of a hidden layer. The most important feature of RNN is a hidden state, which retains information about the sequence.[Related article: Make Your Data Move: Using Schedulers with Data Storage to Generate Business\\xa0Value]The article is an excerpt from the book IoT and Edge Computing for Architects, Second Edition by Perry Lea. In this article, we will explore recurrent neural networks in the cloud and Edge. This book provides a complete package of executable insights and real-world scenarios that will help the reader to gain comprehensive knowledge of edge computing and become proficient in building efficient enterprise-scale IoT applications.RNNs, or recurrent neural networks, are a field of machine learning all to themselves and extremely important and relevant to IoT data. The big difference between an RNN and a CNN is that a CNN processes input on fixed size vectors of data. Think of them as two-dimensional images\\u200a—\\u200athat is, a known-size input. Instead of taking a fixed-size chunk of image data, an RNN has as input a vector and output another vector. At its heart, the output vector is influenced not by that single input we just fed it, but by the entire history of inputs it was fed. That implies an RNN understands the temporal nature of things or can be said to maintain state. There is information to be inferred from the data, but also from the order the data was\\xa0sent.RNNs are of particular value in the IoT space, especially in time-correlated series of data, such as describing a scene in an image, describing the sentiment of a series of text or values, and classifying video streams. Data may be fed to an RNN from an array of sensors that contain a (time:value) tuple. That would be the input data to send to the RNN. In particular, such RNN models can be used in predictive analytics to find faults in factory automation systems, evaluate sensor data for abnormalities, evaluate timestamped data from electric meters, and even to detect patterns in audio data. Signal data from industrial devices is another great example. An RNN could be used to find patterns in an electrical signal or wave. A CNN would struggle with this use case. An RNN will run ahead and predict what the next value in a sequence will be if the value falls out of the predicted range that could indicate a failure or significant event:Figure 1: The main diﬀerence between an RNN and a CNN is the reference to time or sequence\\xa0orderIf you were to examine a neuron in an RNN, it would look as if it were looping back on itself. Essentially, the RNN is a collection of states going back in time. This is clear if you think of unrolling the RNN at each\\xa0neuron:Figure 2: RNN neuron. This illustrates the input from the previous step xn-1 feeding the next step xn as the basis of the RNN algorithm.The challenge with RNN systems is that they are more difficult to train over a CNN or other models. Remember, CNN systems use backpropagation to train and reinforce the model. RNN systems don’t have the notion of backpropagation. Anytime we send input into the RNN, it carries a unique timestamp. This leads to the vanishing gradient problem discussed earlier, which reduces the learning rate of the network to be useless. A CNN is also exposed to a vanishing gradient, but the difference with an RNN is that the depth of the RNN can go back many iterations, whereas a CNN traditionally has only a few hidden layers. For example, an RNN resolving a sentence structure like: A quick brown fox jumped over the lazy dog will extend back nine levels. The vanishing gradient problem can be thought of intuitively: if weights in the network are small, then the gradient will shrink exponentially leading the vanishing gradient. If the components of the weights are large, then the gradient will grow exponentially and possibly explode NaN (not a number error). Exploding leads to an obvious crash, but the gradient is usually truncated or capped before that occurs. A vanishing gradient is harder for a computer to deal\\xa0with.www.odsc.comOne method to overcome this effect is to use the ReLU activation function mentioned in the CNN section. This activation function delivers a result of 0 or 1, so it isn’t prone to vanishing gradients. Another option is the concept of long short-term memory (LSTM), which was proposed by researchers Sepp Hochreiter and Juergen Schmidhuber. (Long Short-Term Memory, Neural Computation, 9(8):1735–1780, 1997.) LSTM solved the vanishing gradient issue and allowed an RNN to be trained. Here, the RNN neuron consisted of three or four gates. These gates allow the neurons to hold state information and are controlled by logistic functions with a value between 0 and\\xa01:Keep gate K: Controls how much a value will remain in\\xa0memoryWrite gate W: Controls how much a new value will affect\\xa0memoryRead gate R: Controls how much a value in memory is used to create the output activation functionYou can see that these gates are somewhat analog in nature. The gates vary how much information will be retained. The LSTM cell will trap errors in the memory of the cell. This is called the error carousel and allows the LSTM cell to backpropagate errors over long time periods. The LSTM cell resembles the following logical structure where the neuron is essentially the same for all outward appearances as a CNN, but internally it maintains state and memory. The LSTM cell of the RNN is illustrated as\\xa0follows:Figure 3: LSTM cell. Here is the RNN basic algorithm using an internal memory to process arbitrary sequences of\\xa0inputs.An RNN builds up memory in the training process. This is seen in the diagram as the state layer under a hidden layer. An RNN is not searching for the same patterns across an image or bitmap like a CNN; rather, it is searching for a pattern across multiple sequential steps (which could be time). The hidden layer and state layer complement are shown in the\\xa0diagram:Figure 4: Hidden layers are fed from previous steps as additional inputs to the next\\xa0stepOne can see the amount of computation in training with the LSTM logistical math, as well as how the regular backpropagation is heavier than a CNN. The process of training involves backpropagating gradients through the network all the way to time zero. However, the contribution of a gradient from far in the past (say, time zero) approaches zero and will not contribute to the learning.A good use case to illustrate an RNN is a signal analysis problem. In an industrial setting, you can collect historical signal data and attempt to infer from it whether a machine was faulty or there were runaway thermals in some component. A sensor device would be attached to a sampling tool, and a Fourier analysis performed on the data. The frequency components could then be inspected to see if a particular aberration were present. In the following graph, we have a simple sine wave that indicates normal behavior, perhaps of a machine using cast rollers and bearings. We also see two aberrations introduced (the anomaly). A fast Fourier transform (FFT) is typically used to find aberrations in a signal based on the harmonics. Here, the defect is a high-frequency spike similar to a Dirac delta or impulse function.Figure 5: RNN use case. Here, a waveform with an aberration from audio analysis could be used as input to an\\xa0RNN.We see the following FFT registers only the carrier frequency and doesn’t see the aberration:Figure 6: High-frequency spike via an\\xa0FFTAn RNN specifically trained to identify the time-series correlation of a particular tone or audio sequence is a straightforward application. In this case, an RNN could replace an FFT, especially when multiple sequences of frequencies or states are used to classify a system, making it ideal for sound or speech recognition.Industrial predictive maintenance tools rely on this type of signal analysis to find thermal and vibration-based failures of different machines. This traditional approach has limits, as we see. A machine learning model (especially an RNN) can be used to inspect the incoming stream of data for particular feature (frequency) components and could find point failures as shown in the preceding graph. Raw data, shown in the previous graph, is arguably never as clean as a sine wave. Usually, the data is quite noisy with periods of\\xa0loss.Another use case is around sensor fusion in healthcare. Healthcare products like glucose monitors, heart rate monitors, fall indicators, respiratory meters, and infusion pumps will send period or a stream of data. All these sensors are independent of each other, but together comprise a picture of patient health. They also are time-correlated. An RNN can bridge this unstructured data in aggregate and predict patient health, all dependent on the patient’s activity throughout the day. This can be useful for home health monitoring, sports training, rehabilitation, and geriatric care.[Related article: Image Detection as a\\xa0Service]You must be careful with RNNs. While they can make good inferences on time series data and predict oscillations and wave behaviors, they may behave chaotically and are very difficult to train. This article exposed RNN data analysis models as well as how RNN cases satisfy this context through proper training. For a holistic and expert review of implementing cloud and edge computing to build commercial IoT systems, check out the book IoT and Edge Computing for Architects, Second Edition by Perry\\xa0Lea.Recurrent Neural Networks in the Cloud and Edge was originally published in The Startup on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'This October 27–30, approximately two hundred renowned speakers will share their experience and expertise in cutting-edge topics and tools with thousands of data scientists at ODSC West 2020 Virtual. Below you’ll find just a few of the speakers and instructors you’ll have a chance to learn from at the conference.Anca Dragan, PhD: Assistant Professor in EECS, Head | UC Berkeley, InterACT\\xa0labMuch of Anca Dragan’s professional career has been focused on robots. After completing her PhD at the Robotics Institute at Carnegie Mellon University, she took a position as an Assistant Professor at UC Berkeley, where she also runs the InterACT lab. In these positions, she strives to enable robots to work with, around, and in support of people. Dragan also helped found the Berkeley AI Research Lab, is a co-PI for the Center for Human-Compatible AI.Andrew Gelman, PhD: Director of Applied Statistics Center │Columbia UniversityAndrew Gelman, professor of statistics and political science and director of the Applied Statistics Center at Columbia University, is an acclaimed researcher and author. Over the course of his career, he has received the Outstanding Statistical Application award from the American Statistical Association and the Council of Presidents of Statistical Societies award for outstanding contributions by a person under the age of 40. Gelman’s research interests include campaign polling, social network structure, toxicology, and medical\\xa0imaging.Megan Price, PhD: Executive Director │Human Rights Data Analysis\\xa0GroupMegan Price’s work as the Executive Director of the Human Rights Data Analysis Group has led her to work on projects in several locations including Guatemala, Columbia, and Syria. As part of her work, she creates strategies for analyzing human rights data focusing on the courts and policing, among other topics. She is also a Research Fellow at the Carnegie Mellon University Center for Human Rights Science, in addition to being a member of other boards and publications.Sriram Sankararaman, PhD: Professor of Computer Science │University of California Los\\xa0AngelesSriram Sankararaman leads the machine learning and genomic lab at UCLA where he is an assistant professor in the Departments of Computer Science, Human Genetics, and Computational Medicine. His research focuses on the intersection of statistics, biology, and computer science, where he strives to use machine learning algorithms to better understand the interaction between human genomes, evolution, and\\xa0traits.Hanie Sedghi, PhD: Senior Researcher │Google\\xa0BrainA research scientist at Google Brain, Hanie Sedghi’s current work focuses on large-scale machine learning. She strives to design algorithms that work in both theory and practice. Not only do they have theoretical guarantees, but they also work well in practice. Before Sedghi took her current position, she was a research scientist at Allen Institute for\\xa0AI.Biplav Srivastava, PhD: Data Scientist and Master Inventor\\xa0│IBMDr. Biplav Srivastava has over two decades of experience in Sustainability, Services Computing, and AI. His current focus is on encouraging collaboration between humans and machines through domain and user models. His areas of focus include transportation, health, governance, and water, particularly in developing countries. Currently, Srivastava is a Distinguished Data Scientist and Master Inventor at IBM’s Chief Analytics Office. He is also an ACM Distinguished Scientist, AAAI Senior Member and IEEE Senior\\xa0Member.David Talby, PhD: CTO│Pacific AI, John Snow\\xa0LabsDavid Talby is a core contributor to the Spark NLP library. He is also the chief technology officer at Pacific AI and John Snow Labs. Previously he has worked with Microsoft’s Bing group on Bing Shopping and at Amazon helping to scale their financial systems. Talby also has extensive experience in building effective Agile\\xa0teams.Jeannette M. Wing, PhD: Professor of Computer Science, Former Corporate Vice President | Columbia University, Microsoft Research.From 1985 to 2012 Jeannette M. Wing was a member of Carnegie Mellon University’s faculty, where she has also served as the Head of the Computer Science Department twice. During her career, she has also been a Corporate Vice President of Microsoft Research and the Assistant Director of the Computer and Information Science and Engineering Directorate at the National Science Foundation. She is currently the Avanessians Director of the Data Science Institute and Professor of Computer Science at Columbia University.These are just a few of the incredible speakers who will be leading workshops, training sessions, and talks at ODSC West this year. For more information about our speakers, be sure to visit our website, which we update frequently.And remember to get your tickets before Friday, July 10th to save 60% with our Early Bird discount.',\n",
       " 'I had the pleasure of presenting at eight ODSC events so far. Every time, there is something special on the trip: San Francisco, Boston, London, and so on. Eventually, I started understanding these cities bit by bit, conference by conference, restaurant by restaurant. But what is really special in an ODSC conference is the community: I’m still in touch with at least one attendee per event, and it does not happen in any other conference.I also saw the AI field growing in these conferences, as there are always presentations on cutting edge topics by great and knowledgeable industry experts from around the\\xa0world.This year, unfortunately, is different. But all crises bring new opportunities and this one is no different. ODSC is launching a new remote training platform, AI+ Training, that will allow anyone in the world to access world-class training in machine learning, deep learning, NLP, and other hot topics in the realm of\\xa0AI.I’m lucky enough to be part of this new offering with a brand new course on natural language processing this August 5th: NLP Fundamentals.I’ve decided to create a brand new course on NLP as in the past five years the field changed drastically. I’ve been in the AI space for ten years now and I vividly remember how NLP was once viewed in the conference sphere: defeat. Many researchers spend years and years on rule-based and statistical NLP, always chasing the magic formula that will solve NLP riddles, always reaching some sort of plateau. At that time, neural networks started getting traction and researchers started exploring new ways of using them in\\xa0NLP.Fast forward three years and there was a groundbreaking new algorithm: word2vec. We finally had a way to project words in a mathematical space and perform mathematical operations on them. The atmosphere changed completely, and there was a rediscovered enthusiasm and hope. It’s now possible to subtract, add, etc words to each other in a mathematical space and obtain another world that made\\xa0sense.Another breakthrough four years later (2017) was the introduction of transformers. Transformers are designed to handle an ordered sequence of data, for example, Recurrent Neural Networks. Transformers allow for a better parallelization than RNN as they don’t require the data to be processed in order. They can process the end of the sentence before the beginning.Now, state of the art models are built using transformers. Two particularly successful projects are GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) that we will see in my upcoming training.The workshop will follow this evolution of NLP, but only keeping the approaches that are still used in the industry. It’s divided into five parts, all with a practical coding\\xa0example.Here is what we are going to\\xa0see:Lesson 1: Text Representation (60m)Familiarize yourself with NLP fundamentals and text preprocessing to prepare the data for our models. We will go through the main steps like removing stopwords, stemming, one-hot encoding, and\\xa0more.Lesson 2: Topic Modeling\\xa0(45m)We will see what LDA is and how it can help to extract information from documents. We will also try different clustering techniques and implement a non-negative matrix factorization.Lesson 3: Text Classification (30m)We will learn how it’s possible to represent text and how a classifier can use this representation. We will use TF-Idf and experiment with a couple of supervised learning\\xa0models.Lesson 4: Introduction to Deep Learning in NLP\\xa0(45m)Understand word embedding, how it works, and how to use it. We will go through the main concepts behind word embedding and see some practical examples using the Gensim\\xa0library.Lesson 5: Overview of Advanced Deep NLP\\xa0(15m)We will introduce the most recent developments of deep learning in NLP, in particular, we will see how to leverage BERT and ELMo and their pre-trained models to solve NLP problems.More on Leonardo De\\xa0Marchi:Leonardo De Marchi holds a Master in Artificial intelligence and has worked as a Data Scientist in the sports world, with clients such as the New York Knicks and Manchester United, and with large social networks, like Justgiving.He now works as a Head of Data Scientist and Analytics in Badoo, the largest dating site with over 500 million users. He is also the lead instructor at ideai.io, a company specialized in Reinforcement Learning, Deep Learning and Machine Learning training. More details on the workshops can be found\\xa0here.He is also a contractor for several companies and for the European Commission, as an expert in AI and Machine Learning. As an author he wrote “Hands-On Deep Learning” and he authored an online training course for O’Reilly, Introduction to Reinforcement Learning.In the academic world he also helped set-up the PhD centre on Interactive Artificial Intelligence and will take part in the Inner Assessment Board to assign funding to Irish research in\\xa0AI.',\n",
       " 'Why and how we migrated our product usage data from PostgreSQL to SnowflakeContextAt Doctrine we provide access to legal information for professionals. This includes legislation, case law, a lawyer directory, company legal statistics. The main service consists of a web application (other services include a customized legal newsletter for instance).In our previous article about database architecture, we presented how we had previously improved our database infrastructure to solve two\\xa0issues:Our data analysis scripts heavily use the database server resources (CPU, I/O), slowing down the web application used by our customers.When our team wanted to query the product usage data, the queries took very\\xa0long.To better solve the first issue, we have recently improved our database architecture using logical replication as explained in this article. But here we will talk about what we have done to solve the second\\xa0issue.What is product usage\\xa0data?The product usage data we are talking about are events recorded when users interact with our platform Doctrine.fr. Events are actions like “perform a search”, “read a document”, “create a folder,”\\xa0etc.Recording them allow us to understand how our customers use our product, and to focus our development efforts on the right features. This data is then manually queried with SQL by product managers, data scientists or developers.The order of magnitude is around 5 million events recorded per day, and the total data is around 500\\xa0GB.The previous\\xa0solutionIn our previous architecture, we had converged to this solution for storing the real-time product usage data produced by our application:Our previous architectureThis architecture was already an evolution from directly writing to tables from our web application, which we had done at\\xa0first.When the database is heavily used, writing can be delayed, and we want to avoid delaying response time when users interact with our platform. The architecture shown above solves this issue by sending the SQL write queries (typically INSERTs but also a few UPDATEs) to a Redis queue. Adding requests to the queue is instantaneous. It is then the job of an independent daemon to pick the SQL queries and run them. Even if running the queries can be slow, it does not matter since it is independent from the application.However, we were not entirely satisfied with this solution. The main issue was that running queries on this data can be very slow. For a while, we worked around this issue by creating indexes which allow to query the data much faster. But the way data scientists might want to query the data can be very different depending on the use case, so very specific indexes need to be created. Creating an index on a big table is very long, so if you want to optimize a 3-hour query but you need to create first an index during 10 hours, your analysis will take as much\\xa0time.Setting up Snowflake and Amazon\\xa0AthenaIt all started when we were contacted by a sales representative from Snowflake. The company Snowflake Inc. was founded in California in 2012 and has raised $1.4 billion. Its main revenue comes from selling the Snowflake data warehouse as a Software as a Service. After a few exchanges with them, and a few reference checks, it appeared that their solution was a relevant candidate to solve our\\xa0issues.However, we wanted to test a competing solution. After collecting some information, we found that Amazon Athena was a solution worth trying, especially since we are entirely hosted on\\xa0AWS.To test the two products for real, we decided to build an entire data pipeline in a way that would allow us to benchmark the two products and find which one best fills our needs. Here is a diagram of what we set\\xa0up:The infrastructure during the Snowflake vs Athena benchmarkThe common part: Amazon\\xa0KinesisAmazon Kinesis Data Streams and Firehose were not services which we already used, so we had to set them up and learn how to use\\xa0them.A Kinesis data stream is basically a publish/subscribe system in which you can send arbitrary binary data. In our case, we chose to use newline-separated JSON encoded in UTF-8. The instances running the backend of our web application send their data to the Kinesis data stream, and any client which has subscribed to the stream then receives the data in almost real\\xa0time.Instead of using a program to subscribe to the data stream, we use an Amazon service called “Kinesis Firehose”. Kinesis Data Streams and Firehose are designed to work well together, so when you create a Firehose, you can tell it to use an already created Data Stream as input. Firehose performs two tasks\\xa0here:It converts the data format from JSON to Parquet, according to a schema declaration in AWS\\xa0Glue.It packs the data in batches which it writes to an Amazon S3\\xa0bucket.Here we decided to use Parquet because we heard that Amazon Athena is more efficient when querying Parquet data instead of raw JSON, but we could have stored it directly as raw JSON files, sparing the need to use AWS Glue. Athena and Snowflake both support JSON and Parquet, and in fact we then successfully used raw JSON in another data pipeline setup. In that case, the only job of Firehose would be to batch and write the data to S3, without performing any format conversion.Setting up\\xa0AthenaAmazon Athena is able to query the data from S3 directly. Athena provides the illusion that the data you are querying is in a regular database table, while it is in fact reading the files from S3 on the fly. To do that, you have to create a schema declaration in AWS Glue, which basically says which “columns” exist and what their data types are. We re-used the same schema declaration we provided to Firehose. Here is what a schema declaration looks\\xa0like:A few columns of our schema declaration in AWS\\xa0GlueSetting up SnowflakeIn Snowflake, we needed a bit of extra work, because we needed to copy the data to the Snowflake database.We first declared a “stage” in Snowflake, which is basically an external data source declaration. The stage we used is our S3 bucket, and we provided Snowflake an IAM user key to be able to read data from our S3\\xa0bucket.We could then a copy a stock of data using the COPY INTO instruction, which copies from a stage to a table. Here is a complete example with the table creation and the data loading (I have kept only 5 columns to make the example simpler):https://medium.com/media/cfc1e01a98f4da9d478cdb024572a325/hrefSpecial care is taken for the “data” column where the input is unstructured JSON data. We store it in a column of type VARIANT (basically the equivalent of JSONB in Postgres).Note that we call parse_json on the fly. A nice feature here is that we can transform data on the fly to adapt it to the destination table structure.The above procedure will load the data once, but we wanted the stream of new data to be automatically added to the table. To do that, we used a native feature in Snowflake called Snowpipe. Declaring a Snowpipe is almost like performing a COPY\\xa0INTO:https://medium.com/media/4ba78ff98e7cc85fc336cc3ea0b47960/hrefAn extra step was needed to notify Snowpipe when new files are written to S3. This was just a setting to add to the S3\\xa0bucket.From that point, any new file written to the S3 bucket was automatically picked up by Snowpipe and the data was added to the table. It makes the data available in the final table only a few minutes after it is generated by our web application backend.The benchmarkSimilarities between Athena and SnowflakeTo assess the performance of both products, we ran a bunch of queries corresponding to some of our data scientists use cases. Here is an extract of this benchmark:A few use cases and the time taken to run the\\xa0queriesThe overall result is that both Snowflake and Athena are at least 100x faster than Postgres when a large amount of data has to be\\xa0read.The only cases where Postgres performs better are when we run a query for which a specifically tailored index is present. For instance the “get last 100 events” query uses an index on creation time which can immediately find the correct 100 rows. This still makes Postgres an ideal database for the small read/writes queries performed in real time by our application where low latency is very important (Snowflake and Athena are not designed for this use\\xa0case).One of the common features on both products is a way to see the progression of your query. A big issue with Postgres was that when a query was running for more than a few minutes, you had no idea if it was going to take 10 minutes or 10 hours, so our engineers started their queries and cancelled them if it was too long, without knowing if they had made 5% or 95% of the\\xa0way.Progression of a query in SnowflakeAnother good aspect of both Athena and Snowflake is how costs are computed. In both products, you don’t pay for processing power when you are not using it. Athena bills only for run queries while Snowflake puts servers in sleep mode after a few minutes of inactivity and auto-scales the number of CPUs. This is a key advantage compared to Redshift or ELK where the servers are running and costing money around the\\xa0clock.Differences between Athena and SnowflakeThe main difference between the two products is their support of\\xa0writes.When a user deletes their account, we anonymize all their data. Basically, this means we perform an UPDATE to “blank” the fields which contain references to the user id. This is possible in Snowflake but not in Athena. However, we could write a custom script which anonymizes the data in S3 directly, so this advantage of Snowflake is not enough to make a significant difference.A more important use case is when data scientists want to perform a “CREATE TABLE AS SELECT”, for example to extract a sample of data on which to perform further analysis. In one of our tests, we created a sample of 10 M rows. Postgres had not finished after 3 hours (we canceled), Snowflake ran the query in 10 minutes and Athena crashed with “Query exhausted resources at this scale\\xa0factor”.The final benchmark resultOur different needs and the results with the different products are presented in this\\xa0table:Summary of our benchmarkSnowflake clearly corresponds best to our needs. It is more expensive than Amazon Athena because you have to pay for Snowpipe, while Athena directly reads from S3. But we thought this extra cost was worth it and decided to adopt Snowflake.One year after deploymentSince we chose Snowflake in Summer 2019, we have been adding more data in the pipeline. We have also taken advantage of the Kinesis setup to remove the previous Redis-based system. We have also used it to send some events to Mixpanel, a software we use to easily analyze simple event data. The final architecture looks like\\xa0that:The final product usage data architectureWe also added a Snowpipe to gather Amazon Application Load Balancer logs, which is very easy since it can write the logs in CSV format to S3, which Snowpipe can parse\\xa0easily.We also added some application JSON logs (table NODEJS_JSON_LOGS). These are our 3 biggest\\xa0tables:Our biggest Snowflake tables (size is the compressed size = around 1/10 of real\\xa0size)One year later, our monthly Snowflake cost is around 3% of our total server cost, compared to 28% for all Aurora Postgres databases, so the cost for us if very reasonable.ConclusionAfter a benchmark, we chose to deploy the Snowflake data warehouse. We have been fully satisfied with this product, which is now used by product managers, developers, data scientists and financial analysts. It allows us to run queries very fast and store several terabytes of data while keeping the cost\\xa0low.Making queries 100x faster with Snowflake was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Photo by Luke Chesser on\\xa0UnsplashBut why?At Doctrine, to build the best platform for legal professionals, we try to be as data-driven as possible. We use Mixpanel to understand our users and analyze quantitive data on their behavior. This tool allows us to aggregate information based on events we send when an action is performed on our\\xa0website.The theory is straightforward, but when you have hundreds of events to follow, it can become quite hard to know what actions trigger events and what data is sent. It happened several times that a change in our code made an event stop triggering, and we sometimes realize it weeks\\xa0later.Until now, we had several ways of seeing the\\xa0events:Developers can toggle an environment variable to see them in the console during development:Product managers and anyone else who want to see the events on other environments had to use the Network tab of their browser developer tools:The experience could be improved, so we decided to build a Chrome extension to have a better view of the events\\xa0sent.What does it\\xa0do?The end result is pretty simple: we display toast messages on the screen when an event is\\xa0sent.How does it\\xa0work?The extension contains multiple\\xa0parts.The manifestThis is a JSON file describing all the metadata of the extension: description, version, permissions needed and scripts to\\xa0use:https://medium.com/media/f99d4c0d72d034503616e700b961807f/hrefThe background scriptsThe code in the background contains most of the logic of the extension. It is the only place where we can have access to some of the Chrome APIs. In our case, we need to intercept the network\\xa0events:https://medium.com/media/8d9b027405c554a6f6865f494cf6ac70/hrefThe content\\xa0scriptsWhile the background scripts are useful for the logic, the content scripts are what’s injected in the client’s page. We use a JS library called Snackbar to display the events, and the code of the lib must be in the client page to\\xa0work.But where do we call the library with the data we want to display? We would want to do it in the listener function when we intercept the network events. But we can’t do that because the background scripts don’t have access to the content scripts. We have to use a messaging system to communicate between\\xa0them:https://medium.com/media/784ca1df1dd1139ce3a6be5d5200f5ef/hrefWe can then have code in the content scripts to listen to these messages, and use the Snackbar library to display the\\xa0data:https://medium.com/media/92709088a883500001b022c9cd84b9a6/hrefThe extension UIThis is all great but after installing the extension, we may not want to display the events every time we go on doctrine.fr. We need a simple way of enabling and disabling the extension.We can write some HTML to show a checkbox:https://medium.com/media/9922f313625fc12a21f81503cd579c9f/hrefThe result is simple but effective:Great, but we are not done yet! We need to store the state of the checkbox for this to be useful. The Chrome API has a built-in storage\\xa0system.When we open the extension UI, this script is executed:https://medium.com/media/b29b83c4b424fabad63195ee28540faf/hrefFinally, we need to enable the extension based on the state\\xa0changes:https://medium.com/media/848a6a7a527b7f7a898172a6b9f98d1d/hrefConclusionIn the software industry, we often talk about the “build vs buy” dilemma. When you need something to run your business, should you build it or should you pay to use a service that can do it for you? At Doctrine, we tend to be on the buy-side. Most of the time it’s not worth it to develop software that is not specific to your business because of the cost of maintaining it.That being said, we should not hesitate to get our hands dirty on small projects like this one, because it’s fun and we can learn\\xa0things.At Doctrine, every developer can take up to 2 days a month to try out new technologies, explore frameworks and build projects. Sometimes it’s just fun and not very useful but other times it can lead to very interesting things. Serendipity fosters innovation.Building an internal Chrome extension was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'How upgrading to PostgreSQL 10 and using its new logical replication feature has allowed us to improve our database architecture by making it simpler, less expensive and more\\xa0robust.The contextThis article follows our story about scaling our database architecture, which we already discussed earlier. However, I am going to summarize it it so you don’t need to read the previous\\xa0article.At Doctrine we provide access to legal information for professionals. This includes legislation, case law, a lawyer directory, etc. There is a visible side, which is the Doctrine service, accessible on our website after a subscription. This is the main service for which customers pay. But before landing on our website, the data has undergone massive analysis, to automatically extract metadata from raw documents, to link it together (lawyers and case law, companies and their lawyers, etc.) and many other complex processes using different AI methods, from regular expressions to deep learning.We can therefore divide this database usage it two categories:For the production website in real-time, when our customers use the service. This requires low latency and high availability (we have set ourselves a goal of 99.99% request success\\xa0rate).For the data pipelines performing the analyses before the refined data is available on the website. This uses huge computation and I/O resources but has no real-time or high availability needs (if a task failed it can just be retried several minutes later). Our pipeline is discussed in this other\\xa0article.The main challenge here is to prevent #2 from eating the resources powering the production service #1 which would slow it down and degrade user experience by inducing an unacceptable latency.The previous\\xa0solutionAfter experimenting with several solutions, we had previously converged to this complex and clever setup with 2 databases (called “Main” and “Webonly”), each one having a read-only replica:Our previous database architectureThe database we call “Main” is the one where the data is written by the non-real time scripts, while the “Webonly” database is the one where the web application both read and writes data (sessions, user folders, user history,\\xa0etc.).The replicas here are entire database replicas which are identical to their master database except that they are read-only. Since we are using Amazon Aurora, the data is not actually copied, but instead a smart system of shared storage is used, but this basically behaves the same way as a classic Postgres physical replication from a logical point of view. In both cases the replica contains exactly the same structure and data as the master database.Basically, the databases in the right-hand side of this diagram are the exclusive territory of the production application while those on the left-hand side can receive intensive queries from our data analysis pipeline, without slowing down the databases on the right. Since a replica can only be accessed in read-only, the data is split in two databases (Main and Webonly), and each side has read/write access to one of them while having access to a read-only replica of the data written by the other\\xa0side.However, this architecture has a few drawbacks:For complex technical reasons related to replication and locking, we still needed to connect the app to the “Main” master to handle the case when the replica is locked. This system we created, which we called Multiquery, is explained here and was later ported to Python. It works very well but introduces more complexity which makes the app more difficult to\\xa0debug.The data scientists need to be very cautious. If they want to change the structure of a table in Main, this table may or may not be used by the app so they basically have to grep the app code to know if the table is used in production.This setup is costly, because each database needs to have a replica, which makes the cost go up, especially for Main which needs a powerful\\xa0server.The new\\xa0solutionOne of the core values of Doctrine is to “challenge the status quo”, and Jérémie Thomassey, which had just joined us, did just that. He proposed to simplify our database architecture by using the new feature called logical replication added in the new PostgreSQL 10\\xa0release.The trick is that logical replication, unlike the classic physical replication, is able to copy only some tables, and to copy them to another database along the “regular” tables of the destination database. We would then be able to get rid of the replicas, and instead switch to an architecture in which only the “Webonly” database is a critical service with high availability constraints. The new architecture would look like\\xa0that:Our new database architectureTo be honest we actually need to keep a replica for Webonly (not shown in diagram above) because we need a standby that can take over the master in case it fails, in order to comply with our goal of 99.99% availability. The only cost reduction here comes from the Main replica which we don’t need any\\xa0more.How to get\\xa0there?It might seem quite simple to get there, just set up the replication and get rid of the replicas, right? But not so fast… All our databases are running PostgreSQL 9.6 which lacks the logical replication feature. Moreover, we use the Amazon Aurora variant, which at the time lacked the “Upgrade to Postgres 10” feature (it has been added\\xa0since).This means that the official way to upgrade was to perform a dump and then restore to a new DB. However, this would require stopping the service during several hours to upgrade the “Webonly” database, which would then jeopardize our goal of 99.99% availability. This availability goal means that we can stop the service only for a few minutes every month, not several\\xa0hours.But we used a special trick to work around this issue, and perform the major upgrade live with zero downtime, as we explain in the next\\xa0section.Upgrading to Postgres 10 with zero\\xa0downtimeUpgrading the “Main” database is relatively easy, because the app does not write anything in it. So we can make a read-only clone of it, connect the app to this clone, then stop the master database to perform the upgrade on the original server. The only constraint is that data scientists cannot use the database during this time, but it does not affect our customers.What is really tricky is the “Webonly” database, that is the database to which the application perform both read and writes, and needs to be able to read back the data it has written a few seconds\\xa0earlier.A typical way to perform such a major upgrade without downtime would be to use logical replication, however this is precisely the feature we don’t have in Postgres 9 and want to upgrade to Postgres 10 for. We can get some inspiration from the way a replication-based zero-downtime upgrade would be performed (this can be used for PostgreSQL 10 to 11 upgrade for instance):Create a replica of the database and wait for full\\xa0syncMake the app use the\\xa0replicaStop the replication and drop the old\\xa0databaseIf we could mimic this process without using logical replication, we would be able to achieve the same goal. We will now explain how we did\\xa0that.First, we create a new Postgres 10 database which will be our new database, and we copy all the structure of the old database, by dumping and restoring the structure. The new database thus contains the same tables but without any\\xa0content.Then, we deploy a version of our web application where the database connector is modified in such a way that every SQL query is performed both on the old and the new DBs, but the result returned is the one from the old DB. This is important because the result could be incorrect on the new DB since it contains no data (so SELECT, UPDATE, UPSERTs would possibly return different results). From that point, the new data is written both in the old database and the\\xa0new.1st deployThe next step is to copy the remaining data from the old DB to the new. In that step we need to be careful about conflicts or redundancy between the new data which is being written in real-time and the stock of data that we are loading. We basically use the primary keys of the tables to avoid duplicated data between the dump and the new data coming in real time from the\\xa0app.At that point, the two databases are fully synchronized in real-time. We then change the code of the DB connector in the app to use the query results sent from the new database as the result returned to the\\xa0caller.2nd deployDuring the deployment of this new version, there is a period during which both the old and new version serve some requests, which is why we need to continue to write in the old DB for backward compatibility with sister instances of the\\xa0app.When this deployment is over, we can deploy a new version which does not use any more the old\\xa0DB:3rd deployAnd it’s\\xa0over.In reality things are always more complex, there as edge cases and we have some other services which use our DB. Here is what the actual checklist for this migration really looked\\xa0like:Our actual migration checklistWe were quite confident with our strategy, so we performed this migration during regular working hours during a regular business day, while our customers were using the service at the same\\xa0time.Overall the migration went very well, with no service interruption at all, and no visible effect for users. Our users had no idea that while they were using the service, all their data had jumped from one server to\\xa0another.Setting up the actual replicationThe last step was actually to set up the logical replication in both directions to reach our final architecture:Our new database architectureA constraint for using logical replication is that “in practice” the tables you want to replicate need to have a primary key, so we had to fix a few tables which lacked one. In theory you could do without, but it would be really inefficient.Apart from this constraint, adding a table to the replication is easy. You create a table in the destination database with the same name and structure as the source table, and then you tell the source database that you want to add the table to the replication. The database will then automatically copy the original data and set up the replication stream for new\\xa0changes.LimitationsA key learning is that logical replication does not like big tables rewrites, as they create a huge replication lag. With physical replication, these issues don’t occur because if your master database can handle a certain amount of writes, then the replica can too. But logical replication has a more limited “bandwidth” because every write operation (even those that do not affect replicated tables) needs to be “translated” to a logical operation. This is performed by a single process (which can only use a single CPU) on the source database which can handle much less data that what can actually be written in the database. When big writes occur, the decoding will lag behind for some time untils it catches up slowly. This means that you cannot perform heavy writes on your database all the time because otherwise the replication would never catch up. The tortoise can only catch the hare if the hare does not run most of the\\xa0time.To avoid these issues, we optimized our data pipeline to avoid massive table rewrites which create a lot of I/O, and we set up some monitoring on big writes using the pg_stat_user_tables system view. We report a warning if a table undergoes more than 20 M row writes every\\xa0day.To monitor the replication itself, we used the out-of-the-box CloudWatch metric OldestReplicationSlotLag:Native replication monitoring provided in AWS CloudWatchSince a replication lag measured in minutes would also be useful, we have set up some custom monitoring by regularly reading the special system views pg_stat_replication and pg_current_wal_history. The resulting monitoring looks like\\xa0that:Our custom replication monitoring in\\xa0MetabaseConclusionAlthough it is much slower than physical replication, logical replication has the key advantage of allowing to copy a few tables from one database to another. We were able to migrate from PostgreSQL 9 to 10 and then from physical to logical replication without stopping the service for our users. Thanks to logical replication, our new architecture is now simpler, less expensive and more\\xa0robust.Leveraging logical replication to simplify our database architecture was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'As a legal platform, Doctrine aggregates a lot of legal data with the intent of making them accessible, understandable and usable. The Machine Learning Engineers’ day-to-day material is mostly text: court decisions, legislation, legal commentaries, user queries, etc. All of our content is natural language, which we process in a number of ways: bag-of-words, embeddings or with language\\xa0models.In an ideal world though, our product would be built on top of scalable, flexible and reusable modules, ones that would be generic enough to accommodate a wide variety of legal contents and feed the whole spectrum of our product features. It is exactly with that vision in mind that we started working on a unified language model a few months ago, whose associated challenges, findings and results we’ll do our best to summarize in this\\xa0article.I. One language model to rule them\\xa0allDepending on the project, we were representing our legal contents\\xa0with:different techniques:TF-IDF vectorsBM25 (e.g., with ElasticSearch)A variant of Word2Vec, called Wang2Vec, embeddings fine-tuned on legal data\\u200a—\\u200anote that even if those embeddings work pretty well for a lot of tasks, they are not the state-of-the-art anymore. There’s not enough modeling power in simple word embeddings and we definitely see their limits now on some\\xa0tasks.2. different data:vocabulary of the content\\xa0itself,vocabulary of the linked contents from our legal\\xa0graphvocabulary from some metadata provided by the\\xa0courts…Yet eventually, we want to be able to represent all of our legal content using a unified framework for any text-understanding based feature, because\\xa0of:Reusability: all teams can rely on this unique language model for their projects.2. Scalability:a modeling power sufficient to be applied to any new legal content (e.g., legal documents from the lower house and the upper\\xa0house),robust enough to unlock use cases we’re not yet considering, like legal bots, legal trend detection, argument mining,\\xa0etc,generic enough to be applied to a new language (with a retraining on the new language of\\xa0course).3. Agnostic usage: one of the problems with our current representations is that the text follows some guidelines in the way they phrase statements, and a textual similarity is thus strongly biased towards documents that have the same overall phrasing (of the same court for example), despite the fact they’re not invoking the same laws about the same thing. For example, it is now difficult for us to match decisions from the High Court/Court of Appeal to those from the Supreme Court simply because of their different writing styles (the former tends to focus primarily and precisely on the facts, while the latter favors usually only relies on the legal matter, which has an adverse effect on our current representations).When we initially started thinking about this, there were some properties that we thought our language model should ideally\\xa0cover:Taking advantage of the semantic proximity:In French:préjudice corporel should be equivalent to dommage\\xa0corporelIn English: death should be equivalent to loss of\\xa0life2. Being able to represent our content on different granularities:Token-level for Named Entity Recognition: anonymization, entity detection,\\xa0…Paragraph-level: structure detection, argument similarities,\\xa0…Document-level: legal domain classification, document recommendation,\\xa0…It’s with all those things in mind that we started to work on a unique, all-encompassing language model serving all our use cases and features.II. Our legal language\\xa0modelThe first step of this project was to design the architecture and implementation of our language model. This step was crucial since it would serve as the foundation to all of our future work and help us move towards our initial vision. We first thought about our technical constraints:use an existing and robust implementation, in order to take advantage of the support and the community,use a state-of-the-art technique to achieve very good performances,ideally use a PyTorch implementation, because our previous Deep Learning algorithms were made with PyTorch. Moreover, PyTorch (along with a few others) remains the dominant deep learning library at the time of writing this\\xa0article,if possible, find an implementation with a French pre-trained model before fine-tuning, because transfer learning has shown its efficiency in\\xa0NLP.It should also be noted that compared to other use-cases, especially in academic research, the framework should be efficient at representing very long texts. Here is an interesting blog post about different document embeddings techniques. We’ll come to that\\xa0later.Under these constraints, the Hugging Face Transformers library appeared to be a very good\\xa0choice:they offer all the recent state-of-the-art architectures (BERT, RoBERTa, ELMo, XLNet,\\xa0…) complete with their associated PyTorch and TensorFlow implementations,some of them have a French pre-trained model,their implementation has quickly become an international reference, to the point where the famous NLP framework Spacy provides a Transformer implementation based on the Hugging Face\\xa0one.Among the models providing a French pre-trained model, we had the choice\\xa0between:BERT-Base, multilingualDistilmBERT, multilingualcamemBERT, French RoBERTa\\xa0modelWe decided to go for camemBERT, since it already provided good results for the French language on several tasks according to this paper. Of course, multilingual models will probably be very useful for internationalization later, but we initially wanted to check that a transformer model could be relevant. Moreover, camemBERT has fewer parameters than multilingual models, which makes it a little easier to\\xa0use.Note that camemBERT is case-sensitive, which will be useful for Named Entity Recognition and especially for anonymization.The legal CamemBERTNow that we had settled on the underlying technology, we decided to check how well it would perform on actual, real-life legal\\xa0data.Knowing that camemBERT was initially trained on the French subcorpus of OSCAR, which features gigabytes of data crawled from the web, we knew that it would fare well at general French language tasks, but we suspected that the task of speaking the more specific French legalese would prove to be a tougher nut to crack, which our initial tests confirmed.For example, when asked to predict the next word of the sentence Par ces\\xa0...\\xa0, camemBERT suggested the word mots, which is not exactly legal-oriented. We would expect something like moyens or\\xa0motifs.It was obvious at this point that the trove of millions of legal documents we have at our disposal at Doctrine would prove to be great material for the subsequent fine-tuning needed to harness the full power of our model. At this point, we were confident that the model could be trained, however, we needed it to be potentially used universally across features. Yet, one issue remained: how to handle long texts, a strong prerequisite for legal documents, but something that doesn’t pair naturally with transformers’ inherent limitations.BERT models, for example, have a hard limit of 512 to 514 tokens (as enforced by the max_position_embeddings parameter), which would surely be a challenge when dealing with court decisions: texts that can be infamously verbose, with an average token count hovering around 2000 (and some even more extreme cases like this decision).To circumvent this issue, we envisioned two different approaches:Embedding each paragraphHaving sliding windows, as explained hereTo avoid ending up with redundancy in the embeddings, we decided to go with paragraph embeddings first, with exceedingly long paragraphs getting snipped past the limit during training. What was left for us to determine at that point was an aggregation strategy over the different paragraphs, so that we could harvest the final document embeddings, something that we would come back to\\xa0later.We then proceeded with the implementation, which was done by splitting our legal documents on paragraphs and fine-tuning camemBERT on the masked language model task (using dedicated AWS GPU instances). It converged after a few days and we tested its relevance by using a few qualitative checks:Comparison between the standard pre-trained French camemBERT model and our legal camemBERT on a masked LM\\xa0taskWe assessed the differences in prediction for semantically similar sentences, which seemed to be consistent. The qualitative check seemed to provide very good results. It was now time to validate the language model on a real\\xa0task.III. Our first legal camemBERT use-case: classification of legal\\xa0domainWe wanted to try our legal camemBERT on a simple task for a first validation: text classification of legal domains on court decisions.This is indeed a simple and well delimited task, and easy to compare to other basic models. Moreover, this classification has a huge product impact, on the search filters, recommender systems and analytics.We have two hierarchies on the legal domains at Doctrine:the main legal\\xa0domain:Droit civil,Droit commercial,Droit social,Droit public,…2. the subdomain: for example in Droit civil, there\\xa0areDivorce et séparation de\\xa0corpsDroit locatifDroit des successionsDroit de la responsabilité…Today, we support 9 different domains and 40 different subdomains, where some are more complex than others to determine. These categories have a hierarchical structure, but we addressed the problem by reducing it to a 40-class classification problem.The HuggingFace repository suggests a classification head module integrated with CamemBERT. However, as discussed earlier, the main problem is that court decisions can be very verbose (have a look at this very long decision for example), and BERT does not work well on long texts. A very good review of document embeddings showed that there are no clear embedding technique that works better than others for very long documents. It really depends on your objective.Working at a paragraph level seemed more relevant, all the more so as the language model has been trained at a paragraph scale. BERT will then provide an embedding for each paragraph. We then had to think about a way to aggregate the paragraphs in order to get a decision embedding.ModelingParagraph embeddings methodIt is known that BERT architectures provide not only word-level contextual embeddings but also the special CLS-token whose output embedding is used for classification tasks. However it turns out to be a poor embedding of the input sequence of other tasks if not fine-tuned on the specific\\xa0task:The paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks from Reimers et al, 2019, shows that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity.According to BERT creator Jacob Devlin: “I’m not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations.” sourceStill, the most classic ways to embed a document (in our case, a paragraph) with BERT\\xa0are:to use the [CLS]-tokento use an aggregation of the last X hidden states of the word embeddings ( we usually saw\\xa0X=4)What is interesting in our case is that one paragraph does not represent the whole court decision. We had to plug something on top of it. We decided to go with the [CLS]-token as paragraph embeddings for a first shot, because our task is a classification task.2. Document embedding with an aggregation over paragraphsGiven embeddings for all our paragraphs, we then had to think of a way to get document embeddings.Here again, different approaches can be considered, since this is another sequence-to-one vector modeling:A simple average of all paragraph embeddings (the [CLS]-token of each BERT-output paragraphs),A weighted average of the paragraph embeddings, with weights built with a self-attention mechanism explained in the paper A Structured Self-attentive Sentence Embedding,A bi-LSTM to exploit the sequential information contained in the paragraphs,A Convolutional Neural\\xa0Network,Another BERT that would learn the language at the paragraph scale,…Given that our task is a mere classification problem, the solution with a self-attention mechanism seemed to be pretty relevant for our case\\xa0because:It’s a bit smarter than a simple average-pooling, and it will automatically get rid of the useless paragraphs that contain no information for the legal domain. Indeed, the final paragraphs of French decisions are often related to the operative part of the judgment, and about who pays the costs. This is usually not relevant to our current\\xa0problem.It also provides some precious insights on how to best interpret the model. We can indeed have access to the attention weights and check on which paragraphs the model focused on the most for its prediction.With all that mind, here’s the final architecture for the classification task:Final architecture of our legal document classification on documents, using the legal camemBERTWe first tried to train the whole pipeline, including the fine-tuning of the legal camemBERT on this task, but we got memory errors. We quickly froze the BERT model and trained only the rest of the pipeline (attention layer + classification layer). It provided good results so we didn’t go with further experiments on an end-to-end training. This is something that we made a note of though, since unsupervised BERT outputs are known to be poor if not fine-tuned, as discussed earlier in this\\xa0article.ResultsThe goal here was not only to improve our legal domains classification, but also to show that we could achieve at least the same results as a simple TF-IDF\\xa0model.Dataset creationDeep learning in general often requires a consequent training set size. That’s why we used a semi-automatically labelled training dataset, labelled:by humans, using\\xa0Prodi.gywith business rules, using the associated court as a reference. If a decision is linked to another one from Labor court, it’s very likely that the decision is about Droit du travail(labor laws).with the most reliable predictions of our former algorithm, based on TF-IDF for the domain, and a legal taxonomy for the subdomain.Comparison between models and discussionWe achieved the same performance with our legal camemBERT and with a simple TF-IDF, which is actually good news! We indeed didn’t spend a lot of time on the modeling part of camemBERT, and this classification task is in the end a rather simple NLP\\xa0task.Moreover and perhaps just as interestingly, we noticed after a qualitative analysis of model’s prediction errors that the errors of the simple model were more often out of context. It means that when the TF-IDF gets it wrong, it’s really way off the mark. For example, this decision is predicted as Droit du transport with a probability of 0.96, instead of Droit des assurances because the decision is about a vehicle insurance claim and contains a lot of vocabulary related to transportation, and not that much about insurance.On the other hand, the legal camemBERT can of course be wrong, but it never steers too much out of context and will mostly predict subdomains that are very close, like Droit immobilier et de la construction and Droit de la copropriété et de la propriété immobilière, when we look at the confusion matrix.Moreover, CamemBERT managed to predict some subdomains that were not obvious at all, even for humans. For example, this decision has been predicted as Divorce et séparations de corpswithout any explicit mention of the word divorce in the decision! The subdomain here is very implicit and implied by a mention to a father that has to pay alimony to the mother of his\\xa0child.Let’s now have a look at the attention weights of our modeling. Here are some examples\\xa0below:Paragraph with the highest attention score (0.34) for the prediction of https://www.doctrine.fr/d/CA/Reims/2008/SK60FC7292250FC0B001E6 as Divorce et Séparation de\\xa0corpsParagraph with the highest attention score (0.26) for the prediction of https://www.doctrine.fr/d/CA/Rouen/2016/1F43DFAE32435B18DC90 as Droit des étrangers et de la nationalitéThese attention scores totally make sense, and confirmed the approach.We also confirmed that paragraphs related to generic procedures had a very low attention weight, like this\\xa0one:Paragraph with a very low attention weight of 0.01 for the prediction of https://www.doctrine.fr/d/CA/Rouen/2016/1F43DFAE32435B18DC90 as Droit des étrangers et de la nationalitéFinally, when we had a look at the errors of the models (both models), we quickly noticed that some classes were very well predicted and some others were not. Our intuition about the observed discrepancy boils down to the fact that language models are only ever as good as their training dataset. In our case, the issue seems to stem from volume and errors in the training set. This is definitely the next priority for this task to focus on, before trying to play with the different architectures. Indeed, the current one seems to work pretty well on subdomains when the training dataset is satisfactory.ConclusionWe built a legal language model with a state-of-the-art technique, that proved to be very efficient at capturing highly relevant information on a simple classification task. This is a huge step for Doctrine, as we have a lot of very complex tasks in Natural Language Processing to tackle! The granularity of this new language model, which can seamlessly provide token, paragraph and document embeddings will be key for us to find new applications for the technique on a wide array of complex Natural Language Processing tasks at Doctrine.In fact, the legal camemBERT has already found a second problem to tackle with the issue of semantic similarity between users and legal content in the context of a recommendation system and seems to already have yielded promising results, which we’ll be sharing in an upcoming blog post very soon. Stay\\xa0tuned!A single legal text representation at Doctrine: the legal camemBERT was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " \"Data Science has been at the core of Doctrine.fr since the beginning. As such, a lot of effort has been put into developing data science models and industrializing them using pipelines tailored to provide quality answers to business questions. This article focuses on the industrialization part, as we detail how we are doing offline inferences using our models. For insights about our online inference system, you can refer to our article about a dedicated API for machine learning.More precisely, this article describes the generic logic we have developed in order to make the inference step straightforward such that we only have to focus on the specific processing for a given\\xa0task.Technical stackBefore going into the details of our generic pipeline, here’s some context with a small summary of the technical stack used in our data science projects.These projects are coded in\\xa0Python.Data is mainly queried from a PostgreSQL database (we are also using AWS S3 but less frequently).Different versions of a model are stored on AWS\\xa0S3.Offline tasks are scheduled and run using\\xa0Airflow.Global overview of the generic\\xa0pipelineLet’s say we have a production-ready model which is trained to predict the structure of a decision (it predicts a label for each paragraph of a decision) and we want to apply it to our corpus of decisions. If you want to know how we trained this model, you can read our article about structuring legal document through deep learning.We are using the example of predicting classes on decisions, but it could be inference of any type of model on any type of content, which emphasizes the need for a truly generic data pipeline. This is how the generic logic works for this specific use case, steps 3 to 5 are parallelizable:Query decisions to be processed through the unique ID we have for each decision at\\xa0DoctrineSeparate those IDs in batches of N decisions for multiprocessing purposesFetch the data needed for those ids (e.g. contents of the decision)Process each decision (its contents) and apply the\\xa0modelInsert inferred results in the\\xa0databaseEvery single one of the steps laid out above is implemented in a generic python class we have called\\xa0Project.Generic pipeline in Project\\xa0classThis class also includes other things like connectors to database, connectors to Elasticsearch, loaders for models on AWS S3 or management of asynchronous operations\\xa0. Those technical components are of tremendous value and help us avoid fragmentation and duplication, but fall outside the scope of this article, so we will not go into further details about\\xa0them.This is a simplified implementation of thisProject class:https://medium.com/media/d7a491a98a3a96431962f1ddc88880d3/hrefThe different steps of the pipeline are launched in the run method of Project. All new data science projects thus inherit from theProject class and overload its get_rows, process_element and insert_results methods. In the next section, we will also go into more detail for get_ids method, which is used to get the identifiers of documents to process, in this use case decisions. So, if we apply it here to our task of structure prediction on decisions, our data script looks like\\xa0this:https://medium.com/media/af8810ead74c16c99588508ad7dce52c/hrefLet’s now dig a bit more into what each step of the pipeline is\\xa0doing.Identify elements to\\xa0processThe first step of our pipeline is to query identifiers of documents we want to process, which is done with get_ids method. In our example we're dealing with court decisions, so let's suppose we have the identifiers stored in a PostgreSQL table called decisions. This table also contains data like the content of the decision on which we want to apply our model of classification.The table decisions is defined\\xa0with:https://medium.com/media/8f5d00a5a1823072ce57ebf6be7cc060/hrefThis first step highlights a first natural need: we have to store information about which decisions have been processed and when they have been processed.Storing this information has several purposes:It is used for debugging, knowing when a model has given a label for a decision can be very helpful if we detect errors in predictionsIt is used to only process new decisions and not those already processedIf the prediction script fails for any reason, we can rerun the script from the decisions where it\\xa0stoppedHow do we store this information?We simply use a table in our PostgreSQL database to do it. We have a schema named operation_states to store tables of this type in our database. And this is how the structure of the table is\\xa0defined:https://medium.com/media/aad1a6a7c472c8887b792a033b5fe503/hrefWe store the ID of the decision with the date of the first and the last time the model has made a prediction on\\xa0it.How do we select the IDs for reprocessing?We need the first and last time we have made a prediction for a given decision because we can make predictions several times in the lifetime of a decision. There are two main reasons why we would make several structure predictions for a decision:Since a given decision is fetched from multiple external sources at Doctrine, it sometimes happens that the metadata or actual contents for a given decision get updated several times in its lifetime. When that happens, the model has to make a new prediction.A new, better model has been trained and we want to apply it again on decisions.The first and naive solution we have found to automatically reprocess decisions is to only process a fraction of them every day. But with this solution, in most of the cases, we are processing decisions which did not change since the last computation. It was a waste of time and resources.Hence we have thought of a more efficient solution. It is clear that we need to focus on modified content only. We need to store information about when a decision has been modified. For this purpose, we are still using a PostgreSQL table, which stores the last modification date about the decision (date of the content change, date of the metadata change etc…). That’s why we are storing those kind of tables in a schema called modification_states. The structure of the table looks like\\xa0this:https://medium.com/media/1876796260677d5ec492e9a0ee0fcefd/hrefHow is the modification information updated?The modification information is obtained from the decision loading scripts. In those scripts, the content or metadata of a decision are compared to the existing ones using a hash function. If there should be a difference, we have rules to determine which content to keep, and if the kept content is new then the script updates the updated_at field of modification_states.decisions_modified_at table for this decision.Finally, the method get_ids takes as input decisions, operation_states.decisions_classified_atand modification_states.decisions_modified_attables in order to select decisions to be processed.Selection of IDs to be processedFor example, this is what the SQL query in get_ids looks like as we want to get the new decisions and the decisions which have been modified:https://medium.com/media/f21aa80e205ca8ab876ab56e55dc443a/hrefActually, in our Project class, we have implemented a generic get_ids function which takes input table names as arguments among other arguments. An interesting argument is stock, when True it selects the entire list of decisions, in the case we want to apply a new model on the whole corpus of decisions.In order to use these IDs, we split them into batches of N elements to be processed, because the subsequent steps are fully parallelizable. The parallelization is done using python’s multiprocessing package inside the Project.run method.Query data to be processedHaving a batch of decisions to process, through their ids, we now have to get useful information about the decisions. We want to predict a class for each paragraph of the decision, therefore we need to have the contents of the decision.The get_rows method is simply about querying this information for the chosen decisions from the previous step. It actually translates into simple SQL\\xa0query:https://medium.com/media/1c261465729f18a91e25449fc67a050f/hrefWhere science\\xa0happensWe have queried information required for the batch of decisions, we now need to apply the logic specific to the task of predicting a class for each paragraph of a decision. The logic of the task is implemented in the process_element method. This method takes one decision as input and includes preprocessing on the raw contents of the decision (lowercase, stemming, etc.), separation of the content into paragraphs and use of the model to make a prediction for each of them. Finally, the method returns predictions as\\xa0outputs.In addition to the fact that we can parallelize several batches of decisions, we can also do it inside a single batch by doing asynchronous operations when we are requesting different services which can be done simultaneously (Database, ElasticSearch index, etc.). This is not presented in our simplified implementation of the Project class, but in practice it is based on the asyncio package. In our example, we could take advantage of asynchronous processings because the limiting resources could be the database or the available CPUs, however in practice it processes in reasonable time, such that we did not need to leverage the asynchronous part.Insert resultsFinally, having our predictions, the last step of our pipeline is to insert data in our database using the insert_results method.At this step, we are inserting several pieces of\\xa0data:Results of the predictions in a defined\\xa0tableProcessed IDs in the operation_states.decisions_classified_at table to store the information about which decisions have been processed and\\xa0whenThe insert_results can also deal with data comparison before insertion. Typically, if we make a prediction for a decision because the content has changed for example, we compare the prediction against the previous one, and we only update data if the prediction has changed. It optimizes resource consumption, as aSELECT is more efficient than a DELETE followed by INSERT in PostgreSQL. The comparison can be done using some hash functions (PostgreSQL has an implementation of MD5 algorithm).ConclusionUsing this generic pipeline, we have removed a lot of the redundant code in the productionization of a model such that we only have to focus on some limited aspects. We took the example of inferring classes for decisions using a model, but this pipeline can deal with any kind of content and any kind of processing (inference using models, text processing etc.).Thanks to Pauline Chavallard, Bertrand Chardon and Nicolas Fiorini for their valuable feedbacks.A generic pipeline to make offline inferences was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\n",
       " 'Last year, SIGIR\\u200a—\\u200aa top international conference on Information Retrieval (IR)\\u200a—\\u200atook place in Paris. It was thus a tremendous opportunity for Doctrine to stay up-to-date in the fast moving field of search, the main feature of our platform. In this article, we would like to describe how research impacts Doctrine (and vice versa), as well as progressively dive into the technical aspects of the conference we appreciated.SIGIR’19’s opening session\\u200a—\\u200acredit: ACM SIGIR\\xa02019Doctrine’s activity in\\xa0researchBesides attending many very interesting talks, of which we shortlisted a selection further down in this article, we also got the opportunity to give an invited talk for the industry track (the video of the talk can be downloaded on this page). At Doctrine, we frequently gather in the data science team to discuss new or state-of-the-art approaches. This allows us to always propose cutting edge features to our customers, because we always apply the current best algorithms in the field and we adapt them for the specific domain that is legal research.Still, beyond just reviewing papers together, we also try to attend international conferences, like SIGIR, because we believe that attending is a lot more proactive and motivating than just reading research outputs. It’s also a fantastic opportunity to share our methods with others and better understand authors of a paper with their presentation and/or follow up discussions. Because SIGIR has a pretty dense program (with up to 4 parallel sessions), we decided to have 3 of our Data Scientists attend it: Nicolas Fiorini, Geoffroy Nicolas and Nathan\\xa0Tedgui.Geoffroy Nicolas (left) and Nicolas Fiorini (right), Data ScientistsThe next step for us is then to publish a peer-reviewed article on one of our numerous innovative approaches, but this will be the topic of another post. In the meantime, we also continue to participate in various international events, for example we gave a talk a Search Solution 2019 in November. This year, we will present some of the search challenges we face during the Industry Day at ECIR 2020\\u200a—\\u200afeel free to join and meet us\\xa0there!IR trends and lessons\\xa0learnedBefore going to our paper selection, we think it’s relevant to provide our understanding of where the field stands and where it’s going\\u200a—\\u200anote, however, that we focus on AI only and this article won’t cover user studies despite their value. The field is currently split in two major subfields:Traditional IR: this includes more or less traditional ranking models such as BM25, their tweaks and improvements, or machine learning (ML) approaches like learning-to-rank (LTR), classification, etc.Neural IR: this one is quite self-descriptive and covers all attempts of applying deep neural networks for search, its scalability, its performances (especially comparatively with more traditional approaches).Surprisingly, while we would expect the domain to shift towards neural IR, we attended many talks on more traditional, ML-based methods. To us, it meant that gradient boosted tree approaches were still very well represented in production systems. It’s understandable, as they are fast, scalable approaches with nice performances which makes them good production candidates. They are, in fact, what we also use for the search at Doctrine. Particularly, LambdaMART seems to always be driving a lot of the research although it was released more than 10 years ago. The best paper award (see further down) was given to a gradient exploration optimization that could lead to training LambdaMART faster in an online learning\\xa0setup.Online learning. That’s a phrase we also heard quite a bit this year. The main reason for having online LTR is the fact implicit user feedback (clicks, success events, etc) is automatically generated and exploitable. The assumption here is: if we can train a ranking model as our users use the system, we will be more flexible (if trends change, for example) and it will be quicker to converge towards a good model compared to running the system live for months and derive a training set from query logs\\xa0only.Finally, a pretty large body of research was dedicated to neural IR. However, this felt a lot more exploratory, where speed is not always mentioned. For many papers, in fact, the objective was not to perform better than LambdaMART-like approaches, but rather to do better than other neural IR methods and see how they\\xa0behave:what works now that didn’t use\\xa0to?why don’t we see similar or better performances than with LambdaMARTwhat does LambdaMART fail to do that neural IR does\\xa0well?It feels like these questions are still unanswered at this point, but also that the field is making consistent progress. Neural IR will definitely replace more traditional IR in the future, especially if the number of discoveries in neural IR continues to grow\\xa0rapidly.Paper selectionWe share here a selection of 3 papers we think proposed interesting approaches, concepts or innovations. This is certainly biased towards our applications, but we thought it could be of interest to some of our\\xa0readers!Variance Reduction in Gradient Exploration for Online Learning to\\xa0RankThis is the best paper award of this year. And it truly deserves it. Online learning-to-rank is (in)famous in industry because it takes time to converge to a satisfying model. And, in the meantime, users are impacted with poor search result quality. This is mainly because of variance generated from exploring random gradients\\u200a—\\u200awhereas during batch training, gradients are defined according to the objective function to be optimized. The authors here propose a smart way to reduce the space of features being updated to make sure they evolve in the right direction, and faster than previous online learning-to-rank approaches.Domain Adaptation for Enterprise Email\\xa0SearchThe motivation of this work is that a single ranking model for enterprise search (say, the Gmail suite of the company you work at) is not sufficient to address the much wider set of user needs. While the internet is the same for everyone\\u200a—\\u200abut of course, there’s personalization\\u200a—\\u200aemail accounts contain personal data that would benefit from a better tailored search engine. Since training a new model from scratch for each company is not feasible (they don’t necessarily generate tons of data), the authors propose a domain adaptation of the general model that will be fine tuned to the specific enterprise needs. To us, this is a very scalable and smart way to address the\\xa0issue.Revisiting Approximate Metric Optimization in the Age of Deep Neural\\xa0NetworksNDCG is often optimized indirectly in learning-to-rank algorithms, by integrating it in the loss function. This allows to optimize the overall ranking despite the fact we may evaluate pairs or even single documents only. This is usually fine because traditional LTR such as LambdaMART use indirect boosting: gradient are weighted with NDCG, but they are defined from a differentiable cost function. In this paper, the authors propose an approximation of NDCG that is differentiable and that can be directly added in the cost function. All algorithms can thus optimize this approximation directly.If these topics are of interest to you and if you want to revolutionize legal research, apply and come share the\\xa0fun!Doctrine @ SIGIR 2019 was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Doctrine.fr is a legal search engine which allows to search in the French legal data, read court decisions, legislation, commentaries and receive legal news. For 3 years, all our data was served to our users through a Node.js backend, based on two sources: data calculated offline, and various algorithms devised by the data scientists.Doctrine data scientists develop in Python, which is the language of choice for data science. Notably, all the offline calculations, which are run in batches, are developed in Python (among other things: Deep learning models developed with Pytorch). On the other hand, online calculations, triggered by a user action, are based on algorithms devised by the data scientists, but implemented in Node.js by our backend engineers.This approach does not scale, and prevents data scientists from quickly iterating over their algorithms, and bars them from using deep learning models for online calculations. To overcome this limitation, we decided to provide data scientists with a dedicated service, called the ML API (Machine Learning API), that exposes features directly developed by them, to be consumed online by the Node.js\\xa0backend.The ML API includes an infrastructure layer, also written in Python, that abstracts all the low-level infrastructure details, such as database accesses with credentials, accessing ElasticSearch, and so on. This layer also deals with error handling, logging, monitoring and other infrastructure features.In this article, we will discuss how a specific feature, named multiquery, has been ported from asynchronous Javascript to multi-threaded synchronous Python, but before going further, we will describe some of our architecture choices for the ML API, relevant for this article, and give more details about the multiquery feature.ML API architectureThe ML API is, at its core, an HTTP server exposing APIs designed by the data scientists. There are multiple options to build an HTTP server in Python; we decided to use Flask, for various\\xa0reasons:Flask is easy to deploy, maintain and monitor, in a production environment as well as in a development environment.Flask is not tied to a specific Web server, and can be deployed with production-scale Web servers (we are using gunicorn, but other choices are possible)Flask scales well: Adding a Flask instance to a server is one configuration change away, and it is easy to deploy multiple servers running their own Flask instances behind a load-balancer.Flask has been time-tested in various data science environments, and is familiar to Doctrine data scientists.Main architectural differences between Python/Flask and Javascript/Node.jsThere is much to say about the differences between Python and Javascript, but we are especially interested in the most salient feature that is handled differently between Node.js and Flask running on\\xa0Python.Node.js is well known for its asynchronous programming paradigm. In particular, each I/O call is asynchronous. Until recently, this meant using callbacks. Nowadays, most of the asynchronous programming is performed using promises, either directly through the methods of the Promise class, or using the async /await\\xa0pattern.Javascript, and therefore Node.js, are mono-threaded: A single thread is running the various execution tasks, switching from one to the other when an async call is performed. This approach is often called cooperative multitasking.Python also supports asynchronous programming through cooperative multitasking, which has been introduced in Python 3.4 and has been rapidly evolving since then. The concept is similar to the Javascript approach: A single thread is running, and the various tasks are built using async /await; when a task is waiting, another task can\\xa0run.Another interesting approach in Python is the use of multi-threading. At first glance, multi-threading appears limited, since CPU-bound tasks do not run concurrently: There is a global interpreter lock that is taken each time a thread is running, blocking other threads. Note that thread executions can be interlaced: we could have two methods actually executing concurrently, but there is only a single thread running at a time, even on a multi-core processor.In Python’s multi-threaded model, when I/O is performed, the calling thread yields the interpreter, letting another thread run. Basically, an I/O call in multi-threaded Python has the same behavior as awaiting an I/O call in\\xa0Node.js.It looks easy to port asynchronous Javascript code to Python, since both support the same cooperative multitasking approach with asynchronous programming. Nevertheless, we decided not to use asynchronous programming in Python for two\\xa0reasons:First, Flask does not support Python’s asynchronous model. We have debated whether to use another, asynchronous-friendly, HTTP framework, but the second reason made us decide otherwise.Second, we already had a large body of data science code that we wanted to reuse in the ML API, and that was not asynchronous. We would need to port all this code to the asynchronous model, which would in turn impact all the offline operations; this would have been far too expensive to port, and would introduce numerous reliability risks.It remains that some of the features implemented in our Node.js backend have been designed as asynchronous algorithms. Among them, one had to be ported to the ML API backend. We describe this feature hereafter, before discussing how to port\\xa0it.The multiquery featureThe multiquery feature has been described in another Doctrine blog post. We will summarize its most prominent characteristics here.Our main database contains the product data, whereas user data such as login, folders, favorites, etc. are stored in another database. The main database usage is\\xa0twofold:Data is written, often massively, by offline scripts that perform calculations and inject their results in the database. Response time is not critical.Data is read by our Node.js backend to serve them to our users. Response time is critical.To handle these two usage patterns, the database is replicated, and writes are sent to the master, whereas the replica is dedicated to read\\xa0queries.The replica may be slow to respond, either because it is performing a maintenance task (such as vacuuming the database), or it is handling costly read queries, or it may even be down and restarting. On the other hand, write tasks do not monopolize the master all the\\xa0time.In our Node.js backend, all read queries are sent to both the master and the replica, and we keep the first response received. In about 15% of the cases, the master is the fastest responder.With an asynchronous approach, it is easy to implement the multiquery system: Send the request to both databases, with the same callback function, and return the first response received to the caller. The only difficulty is in dealing with\\xa0errors:If both call succeed, we return the results of the fastest one, and forget the results of the second\\xa0one.If one call succeeds and the other fails, we return the results of the successful one, and log the error of the failing one as a warning (because we still want to know if one of the replicas is failing).If both calls fail, we log both errors as warnings, and raise the last error received to the\\xa0caller.The following code is a simplified implementation of this algorithm:https://medium.com/media/f0789be1b0b6c0dae2da0972a698eb39/hrefPorting the multiquery system to\\xa0PythonThe Javascript code above has been designed as asynchronous from the start, and its architecture had to be completely redesigned to port it to\\xa0Python.We will use Python threading in order to implement this feature. Basically, we have one thread serving calls to the master, and the other to the replica. The architecture would be like\\xa0this:The main thread sends a message on both of these queues (actually in a random order), and waits for the response.Since we have two threads handling the queries, we will receive two responses, one for each\\xa0thread.The usual approach would be to use a response queue to return the responses, but this does not fit our approach. Let us consider the following scenarios:As we can see, a response queue would not work when the first response that we receive is a success, because we would not wait for the second response, which would lead to the following situation:A message will be left in the response queue indefinitely.If the second response is an error, it will never be notified to monitoring.Actually, the correct approach is to pass a stateful object as the message. This object has, on one hand, a payload, namely the query to execute and its parameters, and, on the other hand, a synchronization mechanism between the calling thread and the two executing threads (one for each database). The gist of the multi-threaded Python multiquery lies in this\\xa0object.First, we create a class named ThreadingMessage and define its constructor:https://medium.com/media/e483974f4de8ed28d8d8e92fafe7850e/hrefThis class constructor receives the payload and the number of consumers as parameters. We’ll get back to this second parameter when we discuss the _is_finished_unsafe method\\xa0below.The class has two members, whose purpose is to expose the response object and the list of errors received; by construction, we can only have one response, the one from the first successful call, but we can have several errors, if both calls\\xa0fail.Last, the class has two protected members, self._finished_condition and self._responder_count, whose purposes will be discussed shortly. For the moment, just note that self._finished_condition is a condition object, that serves as the synchronization mechanism.In various methods of the class, we need to know whether we have finished or not, that is if we have a response or an error to return to the caller. We add the following method to the ThreadingMessage class:https://medium.com/media/694c78d5359a1bd1925ded2be9b3241c/hrefWe declare that the main handling is finished when we have a successful response, or when all the consumers have responded (with an error response). This is why we need to know the number of consumers of the message (the consumer_number of the constructor presented above), and we see the point of the self._responder_count protected member, which maintains a count of the responses received.Note that this method is marked as unsafe for threading. This is because it accesses instance members outside of a lock context, and must therefore be called from a protected context.On a side note, we also could have used a reentrant lock (an RLock in Python parlance) to be on the safe side, but in a self-contained class such as the one described here, we believe it is a more efficient approach to mark a private method as not thread safe rather than putting locks everywhere.Back to our class, we can then add the method that is called by the sender of the message to wait for a response:https://medium.com/media/0f07d978af2e7a279ab32cf25ca5658b/hrefThis method simply uses our condition object, defined in the constructor, to wait for completion, using the _is_finished_unsafe method described above. Calling _is_finished_unsafe is possible here, since wait_for on a condition object acquires the underlying lock before proceeding.Then, we either return the response or raise the error. Note that this method raises an error if all calls failed, or if there is a\\xa0timeout.To complete the class, we need to define a method that lets us set a success response:https://medium.com/media/e5040151da1396b4b2f7e7f7548ebfa2/hrefAnd another method that lets us set an error response:https://medium.com/media/61f84acdc7bfbf215602fa12b6a97194/hrefThese two methods are basically organized in the same\\xa0way:They acquire the condition object, thereby taking the\\xa0lock.They set whatever member they need to set (response or\\xa0error).They increment the response\\xa0number.They notify the condition object.The last step will wake the wait_for call in wait_for_completion, that will check if we are in a finished state, and either wait again or continue the remainder of the execution.The only notable difference is how the members are set: If we have an error, we only add it to the list of errors (and report it), whereas, if we have a successful response, we first check if we already have another response or not; if not, we have a winner, if yes, this new response is forgotten.A quick glance on a seemingly strange line: One may argue that it is useless to increment responder_count in set_success_response; as a matter of fact, since we have a successful response, we do not need to check the number of responders, because we will immediately return to the\\xa0caller.But, it is never a good practice to leave an object in an inconsistent state, especially in multi-threaded programming: When maintaining this class in two years from now, we do not want to wonder why we have a response and an error, and only one responder.Other modifications needed for the multiquery systemWe described above the principles at the heart of the multiquery system. Other parts of the application also need to be modified, most\\xa0notably:The database accessor, that is the entry point for all database calls in our application, must set up the multiquery system, starting the consumer threads, and routing the messages through the\\xa0queues.The consumer thread life cycle must be maintained. Most notably, we need to be able to stop the thread when the application is stopping. To do so, we added an application-wide termination service, which registers function that have to be called when the application is stopping (when the Flask application is exiting, and also when a unit test terminates). In the case of multiquery, the termination function has been implemented by pushing a None value to the queue, which by convention is identified as a halting instruction by the consumer\\xa0thread.Handling multiple queries simultaneouslyWe only have one pair of workers to handle queries (one worker for each database). This means that we can only handle one query at a time. This usually is not a problem, except if we spawn multiple threads that are all trying to run database\\xa0queries.Overcoming this limitation is quite simple: We only need to add additional workers to each queue. This way, if we dimension the number of workers correctly, there will always be a pair of workers available to handle a query submitted by a\\xa0thread.On a side note, you must tailor the size of your database connection pool to the number of pairs of workers, else you may run out of connections.PerformanceWe need to be sure that our multiquery system, with its rather sophisticated multi-threaded algorithm, does not hinder performance, compared to a standard approach (that we will dub monoquery).To perform the following measurements, we have set up a route that executes a rather costly SQL query, and deployed the ML API on a server plugged to our production database\\xa0system.To measure monoquery, we simply unplug the multiquery system: All database calls are then performed straightforwardly, without going through the multi-threaded and queued system described above. It is worth noting that all monoquery calls are executed on the same database\\xa0replica.We want to verify two\\xa0things:How does the multiquery system behave compared to monoquery?Is the multiquery system stable? That is, do we observe a performance degradation depending on the number of queries\\xa0handled?The first question is straightforward: Does our increased code complexity have an impact on performance? The second question is necessary to check if we do not have a stateful condition that would degrade performance over time (such as a queue clogging up, for instance).We ran a set of tests, calling our route sequentially from 100 to 500 times. Warm-up runs are also performed before, to be sure that all the processes and all the threads are started, that all the connections in the pool have been established, and that the database has a hot cache. All the tests are executed several times, and we show the averages\\xa0here.The results are in the following table, showing the times for 100 queries, 200 queries,\\xa0etc.:In terms of performance comparison, the following figure shows each run side-by-side:There is no performance degradation at all. Actually, the “Time difference (%)” in the table above shows a percentage of the performance degradation between monoquery and multiquery. It appears that the multiquery is systematically slightly faster, but the percentage is way below the error margin, so we cannot derive any conclusion from\\xa0it.The last line from the above table normalize the multiquery execution time on a 100 base (i.e., for 200, we divide by 2, for 300 by 3, etc.) We can see that the execution time is almost perfectly linear, so there is no risk of a performance degradation in a long\\xa0run.ConclusionWe have successfully ported a purely asynchronous JavaScript code to Python, using a multi-threaded approach. Compared to a solution without multiquery, there is no impact on performance.The multi-threaded Python is notably more complicated than the Node.js asynchronous solution:There is about four times more\\xa0code.The logic is split among several methods, splattered among several classes, which complicates maintenance.This code has been running in production, without any problem, for several months now, and has allowed us to isolate a substantial part of data-science related code in the ML\\xa0API.Porting code from fully asynchronous Node.js to multi-threaded Python was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'By Raphaël Champeimont and Xavier\\xa0BlondelDoctrine.fr is a legal search engine which allows to search in the French legal data, read court decisions, legislation, commentaries and receive legal news. Our backend has been entirely implemented in Node.js for 3\\xa0years.Most of the search engine load relies on external components, that are called asynchronously by our backend code, which perfectly suits the Node.js architecture. Nevertheless, for historical reasons, some of this code still contains long synchronous operations. It was therefore mandatory for us to detect these long synchronous hotspots, in order to eliminate them.We will introduce this presentation by describing the problem with long synchronous operations in Node.js. After that we will talk about our analysis method and its results, which allowed us to make hypotheses that drove our success fixing the most important hotspots.On synchronous operations in\\xa0Node.jsMost Web framework use the same approach to serve an HTTP request, that can be modeled as a different thread being used for each request. The following figure illustrates this\\xa0model:We call this approach the traditional framework model.An asynchronous operation happens when the work is delegated to another resource, for instance an I/O call, and the processing is suspended waiting for the response. In a traditional framework model, the behavior can be illustrated as\\xa0follows:This figure shows that most threads are suspended waiting for a response.A synchronous operation is an operation where the thread is not suspended. In the figure above, the call and respond operations are synchronous. Having synchronous operations is inevitable and normal. Most synchronous operations are\\xa0short.In the present article, we define a long synchronous operation, in the context of a Web server, as a long-running task, that runs for tens or hundreds of milliseconds before returning a result. When a long synchronous operation occurs in the traditional framework model, the thread handling the request is busy executing the synchronous operation, but the other threads keep on running, so that they continue serving the other requests:Node.js is based on the hypothesis that synchronous operations are always short, and most of the operations are asynchronous. Requests are served by a single thread, that sends queries and receives asynchronous responses, thereby interlacing the execution of several requests at the same\\xa0time:This architecture is extremely efficient, optimizing the resource usage by avoiding the costs of managing multiple threads. On the other hand, this model is defeated when a long synchronous operation occurs:In this figure, we see that the thread is monopolized by the first request, and the Node.js engine is not able to serve the second request until the long synchronous operation finishes.The extent of the\\xa0problemIn a previous article, we described how we instrumented Async Hooks to analyze errors. It was only a small step to instrument it to notify us of long-running synchronous operations:The warning threshold was set to 1 second. It appeared that we had more than 300 synchronous operations per day with a duration above 1\\xa0second.We had removed the most flagrant long synchronous operations long ago, hence we knew that the remaining ones would probably be looking like harmless pieces of code, for instance a simple loop, whose actual performance is in fact dramatically impacted by the size of the user\\xa0input.Our codebase is large and complex, so we needed a way to detect these synchronous hotspots.Classifying synchronous operationsBefore going further, we first defined classes of synchronous operations, according to their duration, and depending on their overall impact on the application:When a monster operation, with a duration above 1 second, occurs, our monitoring system may consider Node.js as unresponsive, in which case it would kill and restart the server, by that losing all the currently processed requests. For the user, this would amount to an application crash.The very slow operations have a visible impact on the user experience, by slowing down the whole server, and give a feeling of sluggishness.Priority tasks are simple tasks, that cannot afford to be slow; the most prominent example is the autocomplete feature, which proposes completion while the user is typing. This feature is only useful if the completions come as fast as the user types, which needs a backend time of less than 40 ms, an impossible goal if a slow synchronous operation (10 to 100 ms) is monopolizing the execution thread.We deem acceptable operations between 1 ms and 10 ms, if they are not too frequent, and we consider fast a duration below 1 ms (actually, this class is itself divided in multiple subclasses that we will not describe further).As discussed above, we knew that we had more than 300 monster operations per day, which may seem a lot, but is in fact an anecdotical percentage of the whole set of queries, and these monsters can be traced back to various infrastructure glitches and edge cases. On the other hand, we did not know anything about the other classes. In particular, we had no information about the very slow, slow and medium\\xa0classes.Recording duration of synchronous operationsIn order to analyze synchronous operation, we created a module that encapsulates async_hooks.This module first adds a hook to handle measurements, by calling createHook:AsyncHook.createHook({    init: init,    destroy: destroy,    before: before,    after: after,}).enable()The module instantiates a simple map, in order to store information about each async call\\xa0started.As described in their documentation, and callbacks are called when a class is constructed (respectively, destroyed), that has the possibility of emitting an asynchronous event. The init function simply creates a tracking object in the map, the key being the asyncId of the call. Note that it is mandatory to remove this object from the map in destroy, to avoid memory\\xa0leaks.Once this data structure is in place, it is fairly easy to record the duration of an asynchronous call. The aptly-named callback is called before each async call, and is implemented as\\xa0follows:function before(asyncId) {    const async_data = map.get(asyncId)    if (async_data == null) return    async_data.before_time = process.hrtime()}The object associated to this asyncId is first fetched, then the start date is stored, in the form of a high-resolution timer, which is far more accurate than using a Date object, and whose precision gets down to the nanosecond.The callback is executed at the end of each\\xa0call:function after(asyncId) {    const async_data = map.get(asyncId)    if (async_data == null) return    const delta_time_struct = process.hrtime(async_data.before_time)    if (reportSyncTimeCallback != null) {        reportSyncTimeCallback({            /* ... */            hrtime: delta_time_struct,        })    }}The start time is retrieved from the map, based on the asyncId, and the elapsed time is calculated. A reportSyncTimeCallback module variable (which is declared elsewhere in the module, with a setter) contains a callback to report various information on the call (including other data than those related to duration calculation).The function we assign to the reportSyncTimeCallback does not store the durations themselves, but a histogram, where each bucket is one of the duration classes discussed above. Each of these buckets contains the number of sync calls corresponding to this duration class. Every ten seconds, we log the histogram line, and reset all the buckets to 0. We then have data looking like the following:SYNC_TIMES_HISTOGRAM 0 0 0 139 1250 446 663 2 0 0 SYNC_TIMES_HISTOGRAM 0 0 0 162 1268 512 704 6 0 0 SYNC_TIMES_HISTOGRAM 0 0 0 139 1236 444 632 16 0 0Storing only the counts, rather than the durations themselves, ensures a constant storage\\xa0size.Analyzing duration\\xa0recordsDuring a peak period in production, we recorded durations for one hour where about 238000 requests were served. Its was therefore trivial to calculate a distribution per 100 requests for each duration\\xa0class:From this distribution, we were able to estimate the time spent in each class and, based on the number of requests, we constructed the following table, which exposes, for each class, the probability of a request to hit a synchronous operation:If we look at the class 100 ms\\u200a—\\u200a1 s, we notice that there is a 5% percent chance that a request hits a long synchronous operation of this\\xa0class.Most notably, the autocomplete feature is a priority task whose performance is tightly monitored. Prior measurements exhibited a 95th percentile of autocomplete at about 100ms. Considering the above 5% probability for a request to hit a 100 ms\\u200a—\\u200a1 sec long synchronous operation, we hypothesized that the 5% of autocomplete that were above 100 ms were all caused by synchronous operations. Solving this priority task problem was therefore a strong motivator to find and resolve these long synchronous operations.Before proceeding further, it was mandatory to set up a monitoring, in order to be able to decide whether a solution had the effect we wanted, and also to quickly detect new hotspots if they appeared. Since the histogram data is stored in our logs, we could directly visualize its evolution other time. The following graph exhibits the three most critical duration classes (> 1s, 100 ms\\u200a—\\u200a1000 ms and 10 ms\\u200a—\\u200a100\\xa0ms):The most prominent line is the 10 ms\\u200a—\\u200a100 ms class. Following our above hypothesis, its is worth zooming on the 100 ms\\u200a—\\u200a1000 ms\\xa0class:Both of these graphs span two weeks, between Monday, February 25th, 2019 and Friday, March 8th, 2019. The flat part in the middle of the graph is a weekend, where the traffic on Doctrine.fr reaches a low\\xa0point.Identifying synchronous operationsAt this point, we knew that there were an important number of long synchronous operations, that we were able to classify by their durations. Nevertheless, we needed to find the exact hotspots in the code that exhibited these synchronous calls.Searching for these hotspots required to be far more intrusive in the code, which excluded performing this search on the production platform. Hence, before going further, we first assessed whether the development platform exhibited the same distribution as the production platform.This was easily done by running the whole backend test suite, and comparing the relative sizes of the different buckets for the two environments. The following figure summarizes the\\xa0results:As you can see, the distribution was similar enough between the two platforms for the remaining work on the development platform to be meaningful.First, we recorded a profiling trace of an execution of our whole backend test suite. The flamegraph is a\\xa0follows:We then parsed the profiler logs to spot the synchronous operations. To do this, we wrote the analyzeLongSyncOpsInProfilingData tool, that we open-sourced in the Doctrine open-source github repository.This tool constructs an ordered list of the hotspots in the source code, with the source file, the line number and the function name (if any). For each line, the following three measures are provided:The worst occurrence (in\\xa0ms)The number of occurrences that lasted more than 10\\xa0msThe number of distinct call stacks, which is a quick discriminator between functions being part of a hot path (low distinct call stack count\\u200a—\\u200aoptimizing the function would only provide gain for a small number of paths in the code) and functions being hotspots (high distinct call stack count\\u200a—\\u200aoptimizing the function will have an impact on several different code\\xa0paths).The following figure shows the top results of this analysis:With these information, it was straightforward to know what the hotspots were, and what was their\\xa0impact.Fixing long synchronous operationsWe proceeded to fix some of the detected long synchronous operations. Each codebase is different, but some common lessons may be derived from our investigations.We decided to focus only on a sample of the top-ranking functions, namely the ones linked to our search engine query analysis, because we felt that optimizing them would bring quick rewards. All the functions linked to HTML parsing are based on more complex algorithms, that would be more complicated to optimize.All of these query analysis functions look for specific patterns in the user input (the search query), for instance to identify jurisdictions or law articles. Therefore, they all have similar structures: They are loops on large sets of regular expressions. Of course, the longer the user input, the higher the regular expression search duration, and therefore the longer the overall loop duration.These functions perfectly fit the traditional framework model, since executing them in a dedicated thread would not disturb the other queries. In a Node.js framework, they are anti-patterns, executing long synchronous code that monopolize the main\\xa0thread.The simplest approach to tackle these synchronous hotspots was to target optimizations at the loop level (rather than, for instance, trying to use Node.js worker threads).We decided to use async.eachSeries to replace the loops. Our initial code looked like\\xa0this:constants.JURIDICTIONS_RULES.forEach(function(rule) {    /* CPU-intensive processing */})We replaced it with asynchronous calls looking like\\xa0this:async.eachSeries(    constants.JURIDICTIONS_RULES,    (rule, iterationCallback) => {        /* CPU-intensive code */        return setImmediate(iterationCallback)    },    (error) => {        return callback(error, juridictions_in_query)    })Note the use of setImmediate, to release the main thread at each loop iteration. There are several options to release the queue in Node.js (setTimeout and process.nextTick), but setImmediate is executed when all the other callbacks have been executed (to be precise, when the poll phase becomes idle), which is exactly the best way to let other queries be handled by the main thread in\\xa0Node.js.Unfortunately, after implementing this approach, we noticed that the performance penalty was huge: The async loop was 15% slower than the synchronous version, which was unacceptable. Worse, this performance penalty was measured without other queries running, which means it was only linked to the cost of scheduling the next loop iteration.We took a forthright route to try overcoming this problem, by adding a mechanism to call setImmediate only on each N loop iterations. Depending on the functions, we concluded that calling setImmediate every two or three iterations would drop the asynchronous performance tax to\\xa05%.Even though not entirely satisfied with this approach, we nevertheless pushed this version to production, to assess its impact on long synchronous operations. The results were clearly visible on the 100 ms -1000 ms\\xa0class:The left hand-side of the graph is the number of synchronous operations of this class during the day of March 13th. The modification has been pushed to production in the late afternoon of this same day. We clearly see that, on March 14th, the number of synchronous operations has been divided by\\xa02.Another interesting fact is that the number of 10 ms -100 ms synchronous operations did not\\xa0grow:This indicates that the synchronous operations we modified has moved below the 10 ms threshold, thereby slipping in our medium and fast\\xa0classes.The effect on priority queries was also quite noticeable:We see that the autocomplete response time, at the P95 level (the orange line) has dramatically dropped, and, furthermore, is now only marginally impacted by the number of requests (the green background), which was not the case\\xa0before.This validates our hypothesis that the 5% of slow autocomplete queries were entirely linked to long synchronous operations.Conclusion and future\\xa0worksWe showed how we analyzed the synchronous hotspots in our code with async hooks, and how we fixed them. The results on the application performance, and on the behavior of the priority tasks, proves our approach is sound and meaningful.We still have synchronous hotspots to tackle, and we first favored an ad-hoc fix that we were able to release early. Since then, we have been iterating on this solution, by setting up a framework that will automatically break long synchronous operations when they reach a certain threshold.Finding long synchronous operations in Node.js with Async Hooks was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'At Doctrine, we started with a single PostgreSQL database 3 years ago. With more data, we faced the usual challenges of scale and high availability with a relational database. This article shows how you can scale your favorite relational database to several terabytes of\\xa0data.A single\\xa0databaseFollowing a “release early” logic, we first started with a simple architecture: a single production PostgreSQL database (on AWS RDS), used both for the application (our Node.js backend) in real-time and for all non-real-time usage such as data loading and data analysis.Here I want to talk about issues related to the production environment only, but everything is a mirror of this in development, so we also had a development database following the same\\xa0logic.We used this simple architecture when there was less than 10 engineers in the company. It worked because we had little data so any manipulation would use few resources.However, some problems started to appear. It often happened that data scientists, who wanted to load new data or perform analyses, slowed down significantly the production database, making the product almost unusable. When this happened, we had an emergency procedure which basically consisted in listing all running SQL queries and killing as fast as possible those which we thought were the cause of the heavy resource\\xa0usage.As a first workaround, we fine-tuned the loading script to add things like “wait 1 second” in order to slow down data loading/analysis and leave free resources for the application.However, this “solution” was just a quick fix and could not scale as more engineers joined the company, as we loaded more data and performed more intensive analyses.Analysis of the\\xa0problemTo analyze the problem, we thought about the different use cases of our database and identified 2 main use case categories:Real-timeThese are all requests made by the application backend in real-time when users use the product. For this use case, the important properties are:Low latency (we want the autocomplete to run in less than 100 ms for instance)High availability (we want more than 99.99% of user requests to\\xa0succeed)Non-real-timeNon-real-time usage includes different use\\xa0cases:Scripts written by data scientists which load and analyze the new legal data every\\xa0dayNew scripts written by data scientists which need to run once on a complete data set (heavy one-time computation)Experiments of new algorithms on data by data scientistsAnalysis of product usage by product\\xa0managersNon-real time computation of custom recommendations for users, based on their product usage (for instance to send them the newsletter)Sales operations (to know which customers to call for instance)The needs here are different:Heavy I/O usage (around 10 x the application)High CPU usage (around 15 x the application)The resourcesWhat resources are we fighting for? There are 3 main scarce resources:For each of these resources, we had experienced a situation in which a non-real-time query (data loading script for instance) had depleted the resource, which had made the application too slow compared to the quality of service we want to provide for the user, or even completely unusable (for example in the case of an “out of memory”\\xa0error).Why not use priorities?After analyzing the problem, we thought that we could introduce priorities. If all real-time queries were given priority access to CPU and I/O, it could solve the problem without adding much complexity to the infrastructure. However, I/O priorities are simply not available in Postgres, and CPU priorities are available through an extension that cannot be installed in the PostgreSQL provided by AWS\\xa0RDS.Introducing a read-only replicaOur first attempt to implement a solution was to introduce a read-only replica for our database (using PostgreSQL physical replication proposed by AWS\\xa0RDS).The idea was that the replica machine would be used only for the application, which means it would not be affected by heavy loads by data scientists for instance.This approach only partially solved the problem, because the application was still dependant on the master for a lot of\\xa0queries:Writes have to be performed on the master, which involves a significant number of cases in the application (update the users’ search history, session,\\xa0etc.).Some reads also need to be performed on the master because of replication delay: Replication is asynchronous, so when you INSERT some data in the Master DB and the DB tells you that the query has succeeded, you may not see the new data if you query the replica. This is annoying because it means you have to perform a lot of read queries on the master when you want the modification to be visible immediately. In practice this is a pain because developers need to think a lot about the DB infra to know in each case if they have to use the master or replica when performing read\\xa0queries.The split in 4\\xa0serversFor the reasons explained above, we were not happy with the previous solution. To solve the problem once and for all, I thought that we should actually use specific servers for the real-time usage. But since both the application and data scientists need to write data in some tables, and since we cannot have more than one master, the only solution was to introduce another database. So we basically divided the tables in 2 categories:Tables written by data scripts and read by the application (basically the legal content our users can read on Doctrine.fr)Tables written by the application (like sessions, user history, user account settings, events, performance measures, etc.)The idea is then to move all tables in category 2 to a new database (which we call “Webonly”), to which only the application has access. Since data scientists also need to access this data, we created a replica of this database for them. So in the end, each “side” has its own database, and the “other side” has a replica of\\xa0it:To facilitate usage by data scientists, we added a last trick to the architecture shown in the above schema: We added foreign tables using PostgreSQL foreign data wrapper in the Main master, linked to tables in Webonly replica, so that data scientists can continue to query the same tables in Main transparently, and the requests are automatically forwarded to the Webonly\\xa0replica.To switch to this architecture, we moved all tables for which the application needs write access from the Main to the Webonly DB (see appendix for how we did\\xa0that).This time, this new architecture really did improve our reliability massively. It eliminated conflict when using the following resources:However, one problem remained: heavy writes on Main DB. When writing data to the Main master, all the bytes written to disk are transferred through the network and then written by the replica to its own disk. Therefore, a massive write to the master hard disk results in an identical massive write in the replica hard disk. In practice, when data scientists loaded a lot of data and saturated the I/O bandwidth of the master with writes, it resulted in the replica I/O bandwidth to be saturated too, which slowed down reads to “Main replica”\\xa0too.The switch to\\xa0AuroraTo solve this remaining problem, we decided to switch to the AWS Aurora PostgreSQL-compatible edition. This is basically a version of PostgreSQL modified by AWS where they replaced the data access layer to use a distributed storage infrastructure they created. Instead of each server using its own disk, it uses a distributed file system that is shared between the master and replicas. This does the magic trick, because it means that writes to the master do not need to be replayed by the replicas. The replicas will simply use the updated data from the shared file system. In practice there still needs to be some propagation of information to guarantee transactional consistency and to “invalidate” the disk cache (the replica needs to know that the disk content has changed). More information about how this technology works can be found in the videos here and\\xa0there.This new architecture solved the heavy writes problem. It then looked like we had finally reached a perfect infrastructure and problem was solved. However, we discovered a new issue: When a table is being vacuumed on the master, there is a lock in the replica which prevents it from being read. The lock is “stronger” than on the master, in which it does not prevent reads. Vacuums are rare, but we want 99.99% availability, so we cannot tolerate even a few ~ 30 seconds lock every\\xa0week.The Multiquery systemWe were able to solve this final problem with a system we created: Multiquery. The idea is very simple: When we want to read data from the Main DB, we send the query to both the master and replica, and we use the first response we receive. This is great\\xa0because:If data scientists are heavily querying the master, the replica will still respond rapidly. The master will respond long after, but the user will get the fast response from the\\xa0replica.If the replica is locked because of a vacuum, the master will give an answer even if the replica is still locked, and we will use the response from the\\xa0master.If any of the two is down, we will be able to have no downtime even before the AWS systems detects the failure and reroutes the query to the working\\xa0DB.Here is a drawing showing this updated architecture:We have also set up some monitoring to see which database “wins the race” between the master and\\xa0replica:This shows the proportion of queries for which each DB was faster, during the last 24 hours. The bottom part (blue) is when the master is faster and the top part (green) is when the replica is faster. As can be seen, the replica is often faster, which is expected since the replica is “reserved” for the application, while the master is used by everyone else. But it is not 100% green which means that our system results in faster response times than just always using the\\xa0replica.This Multiquery system was the last nail in the coffin of our problems for a long time and we did not make any significant DB infrastructure changes between March 2018 (deployment of Multiquery) and May\\xa02019.The write-only data optimizationHowever, with scaling our user base, we had a new problem. It started to be painful to query the data in the “Webonly replica” database. For instance, interactions with the product (called events) are recorded to make statistical analysis of user behavior. This is the kind of data that you typically sent to product analytics software (like Mixpanel or Amplitude). This is a lot of data and there are several pains associated to\\xa0it:1. When accessing the data indirectly, ie. by requesting Main DB which forwards queries to Webonly replica though Foreign Data Wrapper (FDW), it is very slow\\xa0because:The data needs to be transferred through the\\xa0network.The query planner is less smart when using FDW, and sometimes it does not use an available index (which an access without FDW would use), making the query several orders of magnitude slower.2. An alternative is to read directly from the Webonly replica,\\xa0however:It makes it impossible to store the results in another table (since it is a read-only DB).It makes it impossible to use joins with data tables in\\xa0Main.3. It is impossible to change the structure of the data because the tables are very big, so rewriting entire columns would cause heavy I/O on Webonly Master, recreating one of the problems we wanted to solve in the first place (I/O bandwidth saturation).So we thought again, and we realized that we could refine our classification as 2 kinds of data by distinguishing 2 sub-cases for data accessed in write mode by the application:Read/Write data: data which is written by the application for its own needs, like for example sessions, user history, user folders, account settings. The data is read back, updated and sometimes deleted by the application itself.Write-only data: This data is written once and for all by the application, which does not care about it afterwards. This data can for instance be user behavior data and performance measures like loading speed. This is rather big compared to the first category (like at least 10x\\xa0bigger).Our problems are with the second type of data. So we decided to move it back to the Main DB. But to avoid going back to our initial problems of resource conflict, we decided to perform all writes through a queue system and execute the queries from an independent daemon:We did not go back to our initial problems because, if we heavily use the Main master and slow it down massively, it will just make the daemon process requests more slowly, but it won’t slow down the app which will continue to push the queries to the queue at the same\\xa0speed.Because the data is never read back by the application, we do not care that it is written asynchronously. This is the key point which makes such a solution possible.So to summarize, here is what we do with each kind of\\xa0data:Data only read by the application (and created by data scripts) => Stored in Main, read from the application from both master and replica with Multiquery.Read/Write working data of the application => Stored in Webonly\\xa0DB.Data written but not read any more by the application => Sent to a Redis queue, then written asynchronously to the Main master\\xa0DB.ConclusionWe started with a simple and typical architecture and we gradually scaled it to integrate more data. It was an interesting discovery for us that we don’t necessarily need to change of database engine as we scale. By splitting the data on specific servers and developing the right tools for our needs (Multiquery, background query runner deamon), we were able to build an architecture fitted to our current\\xa0scale.See alsoBreaking PostgreSQL at Scale (FOSDEM 2019\\xa0video)Introducing Amazon Aurora with PostgreSQL Compatibility\\u200a—\\u200aAWS Online Tech\\xa0TalksAWS re:Invent 2018: Deep Dive on Amazon Aurora with PostgreSQL CompatibilityAppendicesHow we migrated tables from one DB to another without\\xa0downtimeTo move a table from one database to another without any downtime, even though the application needs read/write access to it in real-time, we used the following procedure:Create a table with the same structure in the new\\xa0databaseDeploy a new version of the application which continues to read the table from the old database, but writes in both the old and the new database (ie. we change UPDATE/INSERT/DELETE but not SELECTs).Dump the content of the old\\xa0table.Load the dumped content in the new table (along the content added since step 2 by the application).If the app UPDATEs (or DELETEs) in the table, it is a bit trickier because some data inserted before step 2 might have been updated after 3 in the old table (but not in the new one, since there was no row to update). In that case, we need to compare them and propagate differences (we used foreign data wrappers to be able to JOIN the tables and compare\\xa0them).Deploy a version of the application which now reads from the new table (but continues to write to\\xa0both).Deploy a version of the application which read and writes only from the new table.* This step could be combined with the previous one if we deploy the new version of the app “all at once”. But in fact we use rolling deployments, ie. we deploy our app to some production servers by batches, so at some point both version n and n+1 can be served to users, and we don’t want inconsistencies caused by a new version of the app which does not write any more in the old table, and an old version which still reads from\\xa0it.Drop the old\\xa0table.Scaling with PostgreSQL was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'by Arnaud Miribel and Pauline Chavallard🎯 IntroductionNearly 4 million decisions are delivered each year by French courts. The case law they drive is critical to lawyers who use it in court to defend their clients. Legal research is tedious and Doctrine’s mission is to help them get straight to the\\xa0point.Court decisions are traditionally long and complex documents. To make things worse, for example, a lawyer may be interested in the operative part of the judgement only, that is, the trial’s outcome. In fact, in general, it is pretty common to be looking for a specific legal aspect in such a lengthy text and it can quickly feel like looking for a needle in a haystack. Our goal here is to detect the structure of decisions on Doctrine (i.e. the table of contents) to help users navigate through them more\\xa0easily.📄 How are French decisions structured?A decision is generally structured as\\xa0follows:Metadata («\\xa0En-tête\\xa0» in French): court, number, date, etc., of the\\xa0trial.Parties («\\xa0Parties\\xa0» in French): information about the claimants and defendantsComposition of the court («\\xa0Composition de la cour\\xa0» in French): name of the President of the court, the Clerk,\\xa0etc.Facts («\\xa0Faits\\xa0» in French): what happened?Pleas in law and main arguments («\\xa0Moyens\\xa0» in French): arguments presented by the claimant and defendant.Grounds («\\xa0Motifs\\xa0» in French): reasons and arguments used by the court for its final judgment.Operative part of the judgment («\\xa0Dispositif\\xa0» in French): final decision of the\\xa0court.These are the usual sections, however, there isn’t a systematic structure in decisions. Courts may use different styles, both in terms of writing and organising the documents. For instance, some have titles that highlight a specific part, some have not. And some have all the sections described above, some only have a\\xa0few.Example of a table of contents of a decision on Doctrine. This decision does not include metadata, nor pleas in\\xa0law.The French Court of Appeal usually has a very unified way of writing. Around 55% of their decisions (among those hosted on Doctrine) have explicit titles for each category:Extract of a French Court of Appeal decision with an explicit title for the Facts section. Extracted from https://www.doctrine.fr/d/CA/Orleans/2007/SKDD824CCFE8D8D9D93128. See an English translation.For the remaining 45%, we don’t have an explicit title for all sections.Example of a non structured text provided by the French Court of Appeal. Extracted from https://www.doctrine.fr/d/CA/Metz/2015/RAC1261A1563690C06B77. See an English translation.The decisions can be seen as small stories and while humans can understand them because they understand the context and have some expectations, how would an algorithm do?So, how can we automatically generate tables of contents for court decisions?💡 Technical approachFor every input paragraph from a decision, we want to predict which section it should be labelled with (the one it belongs\\xa0to).Let’s think a little bit about how a human being can understand the transition between the two sections in the figure above (from «\\xa0Facts\\xa0» to «\\xa0Grounds\\xa0»).The order of the paragraphs is a big clue: the court will always remind us of the facts before giving its\\xa0grounds.Then, the vocabulary used looks also different within these sections. In the non-structured example above, the first paragraph is summarising events, whereas the second paragraph is quoting some precise legislation points and seems to be an authority argument. But it’s not that easy to see the difference by only looking at the words used. Indeed, in this case both paragraphs quote a legislation aspect, and so both paragraphs contain semantics on health care and health environments.Our first approach was to use a bag-of-words (BoW) approach to encode the vocabulary information of a paragraph, along with conditional random fields (CRF) over paragraphs to encode the sequential information. Unfortunately, this attempt quickly proved insufficient, certainly because of its simplicity given the tough problem. As highlighted before, the vocabularies among paragraphs are not different enough to allow BoW to perform\\xa0well.In order to address this challenging issue, we trained a more complex model: a neural network (bi-LSTM with attention) using PyTorch to help us predict a table of contents given a free text decision.🛫 ModellingIn this section, we first detail the data we used and how we pre-processed it. Then, we explain which models we tried, and the reasons why. Finally, we provide a look back with results and interpretations 👀.💣 Spoiler: our models work\\xa0well!📂 Our dataset & pre-processingAs detailed in the introduction, our task is to classify every paragraph (let’s call it X) of a decision into one of the 7 most common categories (let’s call it\\xa0y).These categories are the following: Metadata, Parties, Composition, Facts, Pleas in law, Grounds and Operative part.Defining XBelow, a paragraph (noted ¶) is a distinct section of text, indicated by a new line \\\\n character. See below how we retrieve them in our decisions.Splitting into paragraphs ¶1, ¶2 and\\xa0¶3.Setting and finding\\xa0yWe want to have a supervised classification setting (with a true, or expected y), we thus need labelled data for our paragraphs.In fact, it’s pretty straightforward. For many decisions of the French Court of Appeal there’s an explicit, clear table of contents. These decisions use titles to structure the documents, so we can look for these titles to annotate our dataset (y), and remove them afterwards. Our data set is now labelled.Pre-processingIn order to avoid having a too large vocabulary, we:Lowercase textStem wordsReplace all numbers by\\xa0zeroReplace singletons with word <UNK> with a probability p=0.5. In our case, it is quite important to keep rare words, even under one single representation, because it will often correspond to last names. And we hypothesised we can learn things from last names (which eventually proved to be\\xa0true).✏ Model\\xa0designIn this section, we briefly describe what models we chose and the intuition behind these\\xa0choices.Wait… is this Named Entity Recognition?One thing we quickly noticed is the similarity of our task (paragraph classification) with Named Entity Recognition (NER). NER is a traditional task in Natural Language Processing where an algorithm is trained to detect entities in a sentence.Named Entity Recognition: finding lawyers, dates & other entities in a decision.In our case, we are not working at the word scale but at the paragraph scale. Still, the idea remains the same, we can infer the label of a word / paragraph with:its inherent propertiesits context (the neighborhood gives insights on the\\xa0label).So, studying the literature in NER provided us with some great model architecture ideas (namely bi-LSTM neural networks with attention\\u200a—\\u200awe’ll see that soon). Our chosen architecture is strongly inspired from this\\xa0paper.Paragraph embeddingsJust like NER, models use word embeddings as an initial representation for their inputs, we need to represent our paragraphs with paragraph embeddings.Aggregating word embeddingsAt Doctrine, we trained our own word embeddings on 2 million legal documents using wang2vec. We can use these word embeddings to compute paragraph embeddings, for example by aggregating the paragraph’s words embeddings. We tried two aggregations:Average of the word embeddings of the paragraph, initialised by our pre-trained vectors.Sum of the word embeddings of the paragraph, initialised by our pre-trained vectors.Training with\\xa0bi-LSTMsOne problem of the previous method (average & sum) is that it doesn’t capture sequential information from word order in the paragraph embeddings and we may hurt the training. One solution is to use bi-LSTM that keep information from left to right and from right to left. We trained these using our pre-trained word embeddings.bi-LSTM over the paragraph and reading the words, keeping the last hidden state from left to right and from right to\\xa0left.bi-LSTM with self-attention mechanism, strongly inspired by this paper from Bengio et al, 2017. Will prove very useful for the interpretation of which words impact the final classification.Architecture of our paragraph-embedding (PE)\\xa0model.ClassifierNow that we have a good design for paragraph embeddings, we can define the operations on those embeddings to predict which class they belong\\xa0to.Encoder choiceWe want to classify one paragraph based on surrounding paragraphs information too, so as to capture the sequence of paragraphs. Here again, we chose an\\xa0bi-LSTM.Decoder choiceWe chose CRF because they enable to model transition probabilities between classes. And in our case, our classes have sequential information (i.e. class «\\xa0Motifs\\xa0» should come after class «\\xa0Facts\\xa0» and before class «\\xa0Dispositif\\xa0»), so looking at the transition matrix helps to check whether / where the model learns well or\\xa0not.Architecture of the paragraph classification network.Note that we also tried a simple softmax for the decoder part, and compared results (we’ll see it\\xa0later).👓 LearningWe split into train & test sets and learnt using Adam optimiser with learning rate = 0.0025 and beta = 0.85. Our loss is the negative log-likelihood.📊 ResultsFind below the performance of our best\\xa0models:Results of our best models on paragraph classification.Note that the training of models with mean aggregation for paragraph embeddings takes about 30% less time than with\\xa0bi-RNNs!Note also that the f1-score is for all tags. However, we went deeper in the results and noticed that the model had more difficulties to predict the specific tag «\\xa0Moyens\\xa0». They are indeed not always present in the decisions, and sometimes mixed with the «\\xa0Facts\\xa0»\\xa0part.Performance of the best\\xa0modelOur best model reaches 0.98 f1-score!Thanks to the CRF, we can have a look at normalised transition scores between classes, and see where the model performs well or\\xa0not.Transition matrix representing the transition scores between different sections. Red means a low transition score, green means a high transition score.Some interpretations:Each class is likely to be followed by itself (because each class nearly always has multiple paragraphs).Section «\\xa0Metadata\\xa0» is likely to be followed by «\\xa0Parties\\xa0» or «\\xa0Composition\\xa0».Generally, the lower triangle is green and the higher triangle is red, which is a good sanity check ensuring that the sections generally avoid going from a label to the previous\\xa0one!There is indeed a poor transition probability between classes «\\xa0Grounds\\xa0» & «\\xa0Parties\\xa0» because they are far apart in a decision.Visualisations with attentionWe used the attention weights to analyse which words help the model in predicting the correct classes. In the following examples, it is very interesting to note that the words with high attention make total\\xa0sense.Words very specific to the parties section are highlighted: avocat (lawyer) and barreau\\xa0(bar).Words very specific to the composition of the court section are highlighted: président (president), conseiller (counsellor), délibéré (deliberated) and Greffier\\xa0(clerk).Words very specific to the operative part are highlighted: Confirme (confirm), déféré (referred), surplus (remainder).Words very specific to the operative part are highlighted: DÉCLARE (declare), RENVOIE (refer), RAPPELLE (remind) and CONDAMNE (condamne).🚀 ConclusionThis project was a great opportunity to test paragraph embeddings and attention mechanisms at Doctrine. We obtained fantastic results and our model is deployed for our users today. It enables us to complete the table of contents as a fallback when regular expressions fail to catch an explicit section title. Over the 45% incomplete table of contents of Court of Appeal decisions, we now manage to get 90% complete ones thanks to the model!\\xa0👏👏⚖ Structuring legal documents with Deep Learning was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'MotivationLast year Joelle Pineau launched the Reproducibility checklist to facilitate reproducible research presented at major ML conferences (NeurIPS, ICML,\\xa0…). Most of the items on the checklist focus on components of the\\xa0paper.One item on that checklist is “provide a link to source code”, but little guidance has been given beyond this. We at Papers with Code host the largest collection of paper implementations in one place, so we collated the best practices we’ve seen used by the most popular research repositories.We summarized these best practices into ML Code Completeness Checklist that is now part of the official NeurIPS 2020 code submission process, and will be available to reviewers to use at their discretion.ML Code Completeness ChecklistWith the goal of enhancing reproducibility and enabling others to more easily build upon published work, we introduce the ML Code Completeness Checklist.The ML Code Completeness Checklist assesses a code repository based on the scripts and artefacts that have been provided within it. It checks a code repository for:Dependencies\\u200a—\\u200adoes a repository have information on dependencies or instructions on how to set up the environment?Training scripts\\u200a—\\u200adoes a repository contain a way to train/fit the model(s) described in the\\xa0paper?Evaluation scripts\\u200a—\\u200adoes a repository contain a script to calculate the performance of the trained model(s) or run experiments on\\xa0models?Pretrained models\\u200a—\\u200adoes a repository provide free access to pretrained model\\xa0weights?Results\\u200a—\\u200adoes a repository contain a table/plot of main results and a script to reproduce those\\xa0results?Each repository can get between 0 (has none) and 5 (has all) ticks. More details on criteria for each item can be found in our github repository.What’s the evidence that checklist items encourage more useful repositories?The community commonly uses GitHub stars as a proxy for repository usefulness. Therefore, our expectation is that repositories scoring higher on the ML Code Completeness Checklist should also tend to have more GitHub\\xa0stars.To verify this hypothesis we selected the 884 GitHub repositories submitted as official implementations to NeurIPS 2019 papers. We randomly selected a 25% subset of these 884 repositories and manually scored them on the ML Code Completeness Checklist.We grouped this sample of NeurIPS 2019 GitHub repositories by how many ticks they have on the ML Code Completeness Checklist and plotted the median GitHub stars in each group. The result is\\xa0below:NeurIPS 2019 repositories with 0 ticks had a median of 1.5 GitHub stars. In contrast, repositories with 5 ticks had a median of 196.5 GitHub stars. Only 9% of repositories had 5 ticks, and most repositories (70%) had 3 ticks or\\xa0less.We also ran the Wilcoxon rank sum test, and found that the number of stars in the 5-tick class is significantly (p.value < 1e-4) higher to all other classes except 5 vs 4 (where the p.value is borderline at 0.015). You can see the data and code behind this figure in our github repository.To examine if this relationship holds more broadly, we created a script to automate the checklist calculation from the repository README and its associated code. We then repeated the analysis on the whole set of 884 NeurIPS 2019 repositories, and also on a wider set of 8926 code repositories for all ML papers published in 2019. In both cases, we got qualitatively the same result, with median stars monotonically increasing with ticks in a statistically significant way (p.value < 1e-4). Finally, using robust linear regression, we found that pretrained models and results have the largest positive impact on GitHub\\xa0stars.We feel this is useful evidence that encouraging researchers to include all the components stipulated by the ML Code Completeness Checklist will lead to more useful repositories, and that the checklist score is indicative of higher quality submissions.At this time, we are not claiming that the suggested 5 checklist items are the only or even the biggest contributors to repository’s popularity. Other factors are likely to influence popularity, such as: size of scientific contribution, marketing (e.g. blog posts and Twitter), documentation (comprehensive READMEs, tutorials and API documentation), code quality and previous\\xa0work.Some example NeurIPS 2019 repositories with 5 ticks\\xa0are:https://github.com/kakaobrain/fast-autoaugmenthttps://github.com/bknyaz/graph_attention_poolhttps://github.com/eth-sri/eranhttps://github.com/NVlabs/selfsupervised-denoisinghttps://github.com/facebookresearch/FixResWe acknowledge that while we aimed to make the checklist as general as possible, it might not be fully applicable to all types of papers, e.g. theoretical or dataset papers. However, even if the primary goal of a paper is to introduce a dataset, it can still benefit from releasing baseline models, with training scripts, evaluation scripts and\\xa0results.Start using\\xa0itTo make it easier for reviewers and users to understand what is included in the repository and for us to correctly score it, we provide a collection of best practices around writing README.md files, specifying dependencies, releasing pretrained models, datasets and\\xa0results.Our recommendation is to clearly lay out these 5 elements in your repository, and to link to any external resources, such as papers and leaderboards to provide more context and clarity to your\\xa0users.These are the official code submission recommendations at NeurIPS\\xa02020.Click here to see the README.md template and get\\xa0startedLet’s work together to improve reproducibility in our field and help advance\\xa0science!ML Code Completeness Checklist was originally published in PapersWithCode on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'We launched Papers with Code in July 2018 with the simple idea of helping the community track newly published machine learning papers with source code and quickly understand the current state of the art. Since that time, it has grown into a general resource for machine learning, with over 18,000 papers with code, over 1,000 tasks, and over 1,500 leaderboards.Anyone can contribute to Papers with Code and all data is released under a permissive open license (CC-BY-SA). We are thankful to the thousands of contributors who have added results, papers and code to the resource.Today we are excited to announce we are joining Facebook AI to further accelerate our\\xa0growth.Facebook AI has been a strong advocate for reproducibility and open access, leading the way in publishing code and models alongside their research. There is a strong strategic alignment and we are looking forward to the coming year and what we can achieve together.Papers with Code will remain a neutral, open and free resource. There will be no changes to how the service runs or how the community can engage with the site. We’re committed to the independence of this platform and are confident that joining Facebook AI will enable us to grow this resource for the entire community, supporting all frameworks and research\\xa0areas.We are really excited for the future of Papers with Code and for what’s to come in\\xa02020!Robert Stojnic & Ross\\xa0TaylorPapers with Code is joining Facebook AI was originally published in PapersWithCode on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead! Thread will stay alive until next one so keep posting after the date in the title. Thanks to everyone for answering questions in the previous thread!    submitted by    /u/AutoModerator   [link] [comments]',\n",
       " '        submitted by    /u/ThisVineGuy   [link] [comments] ',\n",
       " \"Hi r/MachineLearning! My team and I have recently launched a website that helps provide a seamless way to search datasets for your machine learning projects. We’ve all experienced the pain of searching for that perfect dataset. The world's datasets are scattered across academic websites and Github repos. That’s why we came up with Bifrost Data Search. Bifrost Data Search is an initiative to aggregate, analyse and deliver the world's image datasets straight into the hands of AI developers. You can search from over 1000 listings paired with rich information and in-depth analyses. It’s 100% free and we’re always adding more datasets and features. This is just a beta release, and we’d love to hear your feedback so we can make this a valuable resource for the community! We're currently live on https://www.producthunt.com/posts/bifrost-data-search. We really hope you like it!    submitted by    /u/Xcrinklecut   [link] [comments]\",\n",
       " 'https://youtu.be/nv6oFDp6rNQ Hopfield Networks are one of the classic models of biological memory networks. This paper generalizes modern Hopfield Networks to continuous states and shows that the corresponding update rule is equal to the attention mechanism used in modern Transformers. It further analyzes a pre-trained BERT model through the lens of Hopfield Networks and uses a Hopfield Attention Layer to perform Immune Repertoire Classification. \\u200b OUTLINE: 0:00 - Intro & Overview 1:35 - Binary Hopfield Networks 5:55 - Continuous Hopfield Networks 8:15 - Update Rules & Energy Functions 13:30 - Connection to Transformers 14:35 - Hopfield Attention Layers 26:45 - Theoretical Analysis 48:10 - Investigating BERT 1:02:30 - Immune Repertoire Classification \\u200b Paper: https://arxiv.org/abs/2008.02217 Code: https://github.com/ml-jku/hopfield-layers Immune Repertoire Classification Paper: https://arxiv.org/abs/2007.13505 \\u200b My Video on Attention: https://youtu.be/iDulhoQ2pro My Video on BERT: https://youtu.be/-9evrZnBorM    submitted by    /u/ykilcher   [link] [comments]',\n",
       " 'https://jmlr.csail.mit.edu/papers/volume8/sonnenburg07a/sonnenburg07a.pdf Given that ML has reproducibility problem, should we rate paper which promise to open source their code higher (and reject automatically if promise are made but not fulfilled)?    submitted by    /u/lolisakirisame   [link] [comments]',\n",
       " '        submitted by    /u/voidupdate   [link] [comments] ',\n",
       " 'Millions of people live without an arm or a leg and require prosthetic devices. These are often extremely expensive, far from imitating the capabilities of the human limb and are not intuitive to control. Machine Learning is revolutionizing many industries. What chances do you see in applying ML to prosthetics?    submitted by    /u/ml_keychain   [link] [comments]',\n",
       " 'Given tech companies provide increasingly-popular inexpensive / free online training in programming and ML, supply of ML engineers will increase faster than demand for them. How soon until the average ML engineer salary drop from $145K to $100K?    submitted by    /u/gma617   [link] [comments]',\n",
       " \"Hello everyone, for my PhD, I am researching about privacy and security in machine learning. For me, it seems like an amazingly interesting topic. However, I'm not sure to what extend it already made its way out of academia into real life. Therefore, I started a project to research what role security and privacy play for real-world machine learning practitioners. If you have 10-15 minutes of your time to share, it would greatly help, if you could fill out the anonymous online survey I set up for the purpose of finding out what machine learners think about it: https://websites.fraunhofer.de/ML_security/index.php/149369?Start1=A5. You are also welcome to share the link with people who you think might be interested. Of course feel also free to share your thoughts here. Where do you see security and privacy risks in your models? What attacks are you aware of? Which libraries do you use to support secure and private ML development? (Lists of attacks, possible security measures and interesting libraries can be found within the survey questions ;)) I'm excited to hear what you think!    submitted by    /u/fraboeni   [link] [comments]\",\n",
       " 'I am looking to scale up our ML team with distributed training, job scheduling and hyperparameter search. I found several solutions online but Determined seems to be the one that has everything in one package and seems easy to use. 1- Does anyone have experience with it? Has it worked well so far? 2- Is it possible to run multiple experiments on a given GPU? For what I’ve read it seems to suggest that each GPU can only allocate RAM for a single experiment at a time, is that true? 3- I am afraid Determined is a jack of all trades, master of none. Perhaps a combination of tools would be better suited for these 3 tasks? I would also appreciate suggestions of other scalable ML tools. I have looked at Horovod but it seems to not have a job scheduling and experiment tracking tool which would be crucial for me. Thanks all!!    submitted by    /u/eigenlaplace   [link] [comments]',\n",
       " 'Hello! I go over a paper on spectral norm regularization for deep learning which is a promising regularization technique which works by trying to lower the spectral norm of the weight matrices. video: https://www.youtube.com/watch?v=F-cckDPsWWU paper: https://arxiv.org/abs/1705.10941 abstract: We investigate the generalizability of deep learning based on the sensitivity to input perturbation. We hypothesize that the high sensitivity to the perturbation of data degrades the performance on it. To reduce the sensitivity to perturbation, we propose a simple and effective regularization method, referred to as spectral norm regularization, which penalizes the high spectral norm of weight matrices in neural networks. We provide supportive evidence for the abovementioned hypothesis by experimentally confirming that the models trained using spectral norm regularization exhibit better generalizability than other baseline methods. Hope you enjoy! :)Federico    submitted by    /u/Fedzbar   [link] [comments]',\n",
       " \"I'm looking for some Python 3.X implementations of prototype selection/generation methods for use with a kNN classifier. Preferably, the implementation would integrate well with something like scikit-learn but it's okay either way. The only repos I seem to be able to find are student projects for courses (e.g. https://github.com/ambareeshsrja16/Prototype-selection-for-nearest-neighbor) and a no longer maintained project for Python 2.X (https://github.com/dvro/scikit-protopy). Does anyone know of any nice packages or implementations?    submitted by    /u/ilia10000   [link] [comments]\",\n",
       " \"Reading papers is such a time consuming task but still super important to stay up to date and know what's happening in the field. I've seen a bunch of various tools that help you organize papers like arxiv sanity etc but not many that actually help you read papers more efficiently. how do you all deal with that? do you use particular tools? or just better processes?    submitted by    /u/tdls_to   [link] [comments]\",\n",
       " \"Let's say we have a dataset that we want to use for an ML model, maybe SVM. If there are 20 features, how do we know if it's linearly separable? Since we have 20 features, it would be difficult to visualize. Is this a case when something like PCA is used, so we can visualize the data? I suppose I'm asking if there's a way to determine how separable the data is without visualization?    submitted by    /u/Tyron_Slothrop   [link] [comments]\",\n",
       " '        submitted by    /u/justinkterry   [link] [comments] ',\n",
       " 'Do you have fast streaming time series data that you need to analyze?  Now, you can compute the super informative matrix profile and incrementally update it in real-time using STUMPY!  In Part 6, we cover a step-by-step guide on “Matrix Profiles for Streaming Time Series Data”  https://link.medium.com/TML4ntYuO8    submitted by    /u/slaw07   [link] [comments]',\n",
       " 'My goals are twofold, first to understand more about how one can work with image data, and second and most important to show a concrete use of machine learning to my daughter, who sees me use python and R and has taken some interest but still hasn’t seen concretely how or why it works. So this would be a good father/daughter project, and I would take pictures of her clothes.  Having said that, what I still can’t do is to transform the pictures I take with the cell phone into the numeric matrices that feed into the predict function. Does anyone know how to do it? Any pointers? Thanks!    submitted by    /u/DouglasK-music   [link] [comments]',\n",
       " 'Hi, I am new to machine learning, and I am currently working on GANs with a non-image dataset. However, I found that with GANs, the output layer needs a activation function which produce scaled values. For images, there are libraries that can already recognized images with scaled values ([0 ,1] or [-1,1]), but for non-image data, how do I de-normalize the output from GANs? Many thanks!!    submitted by    /u/tenMin4Name   [link] [comments]',\n",
       " '\\u200b https://reddit.com/link/i6hhuc/video/76xamzozjyf51/player    submitted by    /u/hnipun   [link] [comments]',\n",
       " \"I have two somewhat related questions:  Suppose I have some model where performance is extremely sensitive to small changes in the values of certain hyperparameters. Are there techniques that reliably (or even just sometimes) reduce this dependency without significantly harming best-case performance or inference speed? Suppose my model's performance is smooth enough as a function of its hyperparameters that gradient-descent methods reliably find a local optimum. Many of these performance optima are below my goal, but I know that some are not. Is there any way to smooth out the performance landscape in a way that has a much larger effect below some threshold than above it?     submitted by    /u/reasonablyeffective   [link] [comments]\",\n",
       " 'I need to cut out the face in the photo, make transformations with the face, and then insert the transformed face back into the original photo, how do I do it better? I think to use blending gan    submitted by    /u/wb-08   [link] [comments]',\n",
       " '\\u200b Efficiently store/load and manage models and training data needed for deep learning with matorage Matorage is Tensor(multidimensional matrix) Object Storage Manager with high availability distributed systems for Deep Learning framework(Pytorch, Tensorflow V2, Keras). - Github : https://github.com/graykode/matorage - Document : https://matorage.readthedocs.io/en/stable/ Set backend storage with MinIO $ mkdir ~/shared # create nas storage folder $ docker run -it -p 9000:9000 \\\\ --restart always -e \\\\ \"MINIO_ACCESS_KEY=minio\" -e \\\\ \"MINIO_SECRET_KEY=miniosecretkey\" \\\\ -v ~/shared:/container/vol \\\\ minio/minio gateway nas /container/vol  Set data configuration from matorage import DataConfig traindata_config = DataConfig( endpoint=\\'127.0.0.1:9000\\', access_key=\\'minio\\', secret_key=\\'miniosecretkey\\', dataset_name=\\'mnist\\', additional={ \"mode\": \"train\", \"framework\" : \"pytorch\", ... \"blah\" : \"blah\" }, attributes=[ (\\'image\\', \\'float32\\', (1, 28, 28)), (\\'target\\', \\'int64\\', (1)) ] )  Save pre-processed dataset from matorage import DataSaver traindata_saver = DataSaver(config=traindata_config) train_loader = DataLoader(dataset, batch_size=60, num_workers=8) for (image, target) in tqdm(train_loader): # image shape : torch.Size([64, 1, 28, 28]) # target shape : torch.Size([64]) traindata_saver({ \\'image\\': image, \\'target\\': target }) traindata_saver.disconnect()  Load dataset from matorage from matorage.torch import Dataset train_dataset = Dataset(config=traindata_config, clear=True) train_loader = DataLoader( train_dataset, batch_size=64, num_workers=8, shuffle=True ) for batch_idx, (image, target) in enumerate(tqdm(train_loader)): image, target = image.to(device), target.to(device)  Save & Load Model when training from matorage import ModelConfig from matorage.torch import ModelManager model_config = ModelConfig( endpoint=\\'127.0.0.1:9000\\', access_key=\\'minio\\', secret_key=\\'miniosecretkey\\', model_name=\\'mnist_simple_training\\', additional={ \"version\" : \"1.0.1\", ... \"blah\" : \"blah\" } ) model_manager = ModelManager(config=model_config) model_manager.save(model, epoch=1) model_manager.load(model, epoch=1)  Save & Load Optimizer when training from matorage import OptimizerConfig from matorage.torch import OptimizerManager optimizer_config = OptimizerConfig( endpoint=\\'127.0.0.1:9000\\', access_key=\\'minio\\', secret_key=\\'miniosecretkey\\', optimizer_name=\\'adam\\', additional={ \"model\" : \"1.0.1\", ... \"blah\" : \"blah\" } ) optimizer_manager = OptimizerManager(config=optimizer_config) # The optimizer contains information about the step. optimizer_manager.save(optimizer) optimizer_manager.load(optimizer, step=938)     submitted by    /u/nlkey2022   [link] [comments]',\n",
       " 'Hello, I just wanted to share my model, free for anyone to use. I used transfer learning with MobileNetV2 using imageNet weights to create this model. Its use case is on a mobile iOS device. Feel free to use and enjoy it. I will be updating the model to use MobileNetV3 soonTM . Github: https://github.com/JBall1/FlowerNet_CoreML If there is anything in particular you want to know about this model please let me know. I am willing to share anything. The dataset is around ~80,000 images I collected from google and other sources via python script.    submitted by    /u/ShortSPY   [link] [comments]',\n",
       " '        submitted by    /u/cloud_weather   [link] [comments] ',\n",
       " '[R] [P] I came through this recent interesting paper , which promises to provide a robust and stable Black Box Explanations of state-of-the-art models. I wished to recreate the results at my end, but not able to do so. It would be great if someone has already implemented this framework. https://proceedings.icml.cc/static/paper_files/icml/2020/5945-Paper.pdf    submitted by    /u/Pale-Pattern6334   [link] [comments]',\n",
       " \"With all the GPT3 hype going on, I have an itch to do some language modelling myself, but this is more of a side project, so I would be working with little compute, likely just Colab GPUs. A few years ago I did some character level modelling with a simple two-layer LSTM architecture which yielded reasonable results. But given how much the field has evolved, that can probably be massively improved upon.  Some kind of attention mechanism seems to be the way to go, but most SOTA models are huge and compute hungry, so if anyone can recommend a more bootstrap kind of approach (papers/tutorials), that would be much appreciated. (Whether character level or token level, and whether training from scratch or fine-tuning doesn't matter.)    submitted by    /u/AuspiciousApple   [link] [comments]\",\n",
       " '  submitted by    /u/Fedzbar   [link] [comments]',\n",
       " 'https://analyticsindiamag.com/tutorial-on-keras-callbacks-modelcheckpoint-and-earlystopping-in-deep-learning/    submitted by    /u/analyticsindiam   [link] [comments]',\n",
       " \"Hello, I am a first year of PHD student in machine learning. And I am currently facing a dilemma of choosing the PHD research topic. My professor's research focuses on three areas (reinforcement learning, computer vision-supervised learning and NLP ) that I can choose from. Which area should I go into? In terms of job prospect, which area will have more opportunities after PHD? In terms of interest, I am equally interested in all. So I am more concerned about the job prospect. It seems that reinforcement learning doesn't have as many application as CV and NLP and a lot more tech company hires people with CV or NLP background, is it correct? Only very few specialized company (deepmind, openAI, maybe best scenario google brain) hire people with reinforcement learning background, right? Can anyone shed some light on this.  Thanks.    submitted by    /u/charlink123   [link] [comments]\",\n",
       " \"One of the most common pre-processing techniques used in traditional computer vision is called image thresholding. It simplifies the image for easy analysis. For example, you may use it in medical image processing to reveal tumor in a mammogram or localize a natural disaster in satellite images.  https://preview.redd.it/nqkrz7lzv0g51.png?width=576&format=png&auto=webp&s=5e3f7ba52b229ee447c8e60fb2b322f08c45aeae A problem with simple thresholding is that you have to manually specify the threshold value. That requires a lot of trial and error. A threshold that works with one image may not work with another image. So, we need a way to automatically determine the threshold.  This is where Nobuyuki Otsu's creation aptly named as Otsu's technique helps us with auto thresholding. Let's look at the post for more details.  https://www.learnopencv.com/otsu-thresholding-with-opencv/  and the code is at the link below. https://github.com/spmallick/learnopencv/tree/master/otsu-method.    submitted by    /u/spmallick   [link] [comments]\",\n",
       " 'Are linear problems necessarily important nowadays? Because it seems most problems are being solved in a nonlinear fashion, specifically for deep learning.    submitted by    /u/Baconsarnie1   [link] [comments]',\n",
       " 'They asked us to review a paper and critic it. Wanted to know what I could have done differently. Paper: Understanding deep learning requires rethinking generalisation    submitted by    /u/banenvy   [link] [comments]',\n",
       " \"I'm new in deep learning and I've to do college project on this dataset(https://raw.githubusercontent.com/hungry-hands/Time-forecasting/master/Timeseries.csv). I've seen many blogs and videos and honestly I got errors everytime when I code myself. I've gone through TensorFlow tutorial on time series(https://www.tensorflow.org/tutorials/structured_data/time_series). Still my model is not great. If anyone can help me with this, you're a lifesaver. Thank you    submitted by    /u/hungry_hands   [link] [comments]\",\n",
       " '  submitted by    /u/pavansait26   [link] [comments]',\n",
       " '  submitted by    /u/ayvin_tech   [link] [comments]',\n",
       " 'https://analyticsindiamag.com/tutorial-on-keras-callbacks-modelcheckpoint-and-earlystopping-in-deep-learning/    submitted by    /u/analyticsindiam   [link] [comments]',\n",
       " '  submitted by    /u/OnlyProggingForFun   [link] [comments]',\n",
       " '  submitted by    /u/justinkterry   [link] [comments]',\n",
       " '  submitted by    /u/realvenky   [link] [comments]',\n",
       " '  submitted by    /u/mr-minion   [link] [comments]',\n",
       " '  submitted by    /u/MLtinkerer   [link] [comments]',\n",
       " '  submitted by    /u/shani_786   [link] [comments]',\n",
       " 'Hello, I was reading this paper by Google on DIrect End to End Speech Translation. Is there any official/unofficial or any whatsoever implementation of paper available. If it is please share the link.    submitted by    /u/jiraiya--an   [link] [comments]',\n",
       " 'How can I find the most important or influential Deep Learning papers in say 2020, 2019 etc. Is there some kind of collection?    submitted by    /u/antdra   [link] [comments]',\n",
       " '  submitted by    /u/vision_noob   [link] [comments]',\n",
       " '  submitted by    /u/leon__scott   [link] [comments]',\n",
       " \"I'm writing a classifier network, but rather than only having one correct category for the network to pick, each training example has a number of correct possible categories. However I only care that the network pick one of the correct categories. Could anyone suggest what loss function would be suited to my problem? I have been looking at multi-class cross entropy, but I'm not certain it does what I want. I have read that it requires a one-hot vector, whereas my system is an N-hot vector. Does this render multi-class cross entropy useless? How does it behave in such a situation? Anyway, I'm grateful for any pointers or tips you might have. Cheers Matt    submitted by    /u/OnceAHermit   [link] [comments]\",\n",
       " '  submitted by    /u/MLtinkerer   [link] [comments]',\n",
       " '  submitted by    /u/OnlyProggingForFun   [link] [comments]',\n",
       " 'What are some current target benchmark problems that is focused by the community of researchers? The kinda problems that haven’t been solved effectively or there’s considerable research to better the solution in that direction.  Examples in the past could be achieving best results in ImageNet in 2012, TIMIT speech recognition in 2005, AlphaGo etc.  Could someone help me link to these massive problems that are currently relevant? (I am less than 6 months old in my exposure to neural networks)    submitted by    /u/__stats__   [link] [comments]',\n",
       " 'Hi everyone, I have a intel realsense depth camera and a jetson tx2. What deep learning and computer vision project can I do with these parts? Thank you :)    submitted by    /u/sam_is_me123   [link] [comments]',\n",
       " \"Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:  Learning resources (e.g. books, tutorials, videos) Traditional education (e.g. schools, degrees, electives) Alternative education (e.g. online courses, bootcamps) Job search questions (e.g. resumes, applying, career prospects) Elementary questions (e.g. where to start, what next)  While you wait for answers from the community, check out the FAQ and [Resources](Resources) pages on our wiki. You can also search for answers in past weekly threads.    submitted by    /u/datascience-bot   [link] [comments]\",\n",
       " 'The primary languages for analysts and data science are R and Python, but there are a number of \"no code\" tools such as RapidMiner, BigML and some other (primarily ETL) tools which expand into the \"data science\" feature set. As an engineer with a good background in computer science, I\\'ve always seen these tools as a bad influencer in the industry. I have also spent countless hours arguing against them.  Primarily because they do not scale properly, are not maintainable, limit your hiring pool and eventually you will still need to write some code for the truly custom approaches. Also unfortunately, there is a small sector of data scientists who only operate within that tool set. These data scientists tend not to have a deep understanding of what they are building and maintaining. However it feels like these tools are getting stronger and stronger as time passes. And I am recently considering \"if you can\\'t beat them, join them\", avoiding hours of fighting off management, and instead focusing on how to seek the best possible implementation. So my questions are:  Do you use no code DS tools in your job? Do you like them? What is the benefit over R/Python? Do you think the proliferation of these tools is good or bad? If you solidly fall into the no-code data science camp, how do you view other engineers and scientists who strongly push code-based data science?  I think the data science sector should be continuously pushing back on these companies, please change my mind.    submitted by    /u/exact-approximate   [link] [comments]',\n",
       " '        submitted by    /u/TheInsaneApp   [link] [comments] ',\n",
       " 'Thank you to all the guys who sent suggestions and messages. What an amazing community.  https://www.reddit.com/r/datascience/comments/gei64h/off_my_chest_after_almost_9_facetoface_interviews    submitted by    /u/keyboard1ish   [link] [comments]',\n",
       " '        submitted by    /u/Dogsindahouse1   [link] [comments] ',\n",
       " \"Hi guys, I'm very very new to data science. I have learned SQL and a bit of R. But I haven't had GitHub and don't know where to start putting these skills to use. How and where can I find little projects to add to my portfolio?    submitted by    /u/nickybalboa   [link] [comments]\",\n",
       " 'Where to look for some good projects. Your help and suggestions will be highly grateful.    submitted by    /u/sanjeev_narain   [link] [comments]',\n",
       " 'I am about to get an MSc in Developmental Psychology but I discovered that I actually like working with data a lot. I have some stats knowledge (I’d say it’s decent enough but most likely not at the same level as someone with a STEM degree) and I also understand and can apply various research methods. I’ve also used R during both my BSc and MSc, and now I’ve also been learning Python and doing some personal projects with that. I also have working knowledge of SQL. However, I am worried that people may dismiss me because my academic background is not math/computer science etc. How adequate is that assumption? Do I actually have a shot at an entry level position? It’s really hard for me to get a real sense of how I compare to other candidates. EDIT: forgot to say that I also completed the data analyst path on Dataquest, and even though I plan to include that in my CV, I’m not sure if it’ll actually be of any advantage.    submitted by    /u/DSwipe   [link] [comments]',\n",
       " 'Heading into my senior year and finishing up a degree in data science. Every single \"new grad\" job I\\'ve seen for data science or data analytics requires a master\\'s or PhD. I would like to work for at least a few years before going back to school, and preferably would do something where my degree is relevant so.... what do.    submitted by    /u/childishnemo   [link] [comments]',\n",
       " 'As the title suggests I am curious what would be the future of your typical BI who writes SQL, creates dashboards/reports, talks with business etc. Are these tasks going to be taken as part of a data analyst in the organization, if so, where would that leave BI? Keen to know thoughts of professionals who are in this space.    submitted by    /u/user19911506   [link] [comments]',\n",
       " \"I've been trying to figure this out for a while now, and I'm wondering if I just haven't thought of an easy solution. Basically, I'd like to get a large dataset (ideally 10k+) blog articles pertaining to a query. There are a number of news APIs I've looked at, but they typically don't have the body of the article, only a hyperlink. I've considered writing a Google search scraper that finds blogs matching a query, and then scraping each page, but that would be difficult since the format of blogs varies a lot, even if I limited it to Wordpress. Any thoughts?    submitted by    /u/maxiedaniels   [link] [comments]\",\n",
       " 'A very warm Hi to all the r/datascience community members. I am a junior at my university and am starting my first data-driver project. Apart from web scraping, I am looking for survey related data to back my comprehensive data set. Would really appreciate it if you could take out some time to fill the form provided in the link below. Thanx a lot in advance! https://docs.google.com/forms/d/1943MniqmF8SLMbxz5haubxDDePtE9EWRFqUojI73sVI/edit#response=ACYDBNjILoRIJ-kdLeGCPjqaLCr7P-9i3rYjGP4zQLSh_7mb_BvTNYS4MtJeDa3CF9-DtcE    submitted by    /u/Shorzinator   [link] [comments]',\n",
       " 'Is the masters necessary?  Trying to see if I can find a job as an entry level DS with just a bachelors in CS. Open to getting a masters but would rather save the money.  From reading this sub, there seems to be a lot of misconceptions between a bachelors level DS vs the PHD version. Would appreciate any advice on that too...    submitted by    /u/Past_Sir   [link] [comments]',\n",
       " \"Hi everyone!  I'm currently a data analyst and am having a very difficult time in providing insights of any value. I currently analyze the effectiveness of a company x's advertising on Facebook. Normally, I would use Excel or SQL to analyze the data. However, I've been realizing that a lot of my insights are basic generalizations, like creating regression lines, finding which ad performed the best, which ad had the worst and best ROI, and the likes. Now, I wish to provide something related to statistics so that I'm able to bring better insights. While studying statistics, my understanding is that a lot of tools like standard deviations, t-scores, z-scores, and so on require a dataset to be normally distributed. However, let's say I was analyzing the amount of clicks one advertisement received over the course of the year. The data has a very heavy right-skew due to the very high number of days the advertisement received ZERO clicks.  What would be the best course to statistically analyze this data set? If I wanted to understand standard deviation, I know to transform it, since the SD of a skewed data doesn't say much. So, skewed, the SD is 3000. However, if I do a log +0.5, the SD is 3. If I apply the exp to it, then it's 25. Is this the right way to find SD on a right-skewed graph? What else could I do so that I can start discovering cool things? I guess I lack the experience and guidance at my current company.  OH! Looking at the data points, out of 365 data points, 320 of them have 0 clicks.    submitted by    /u/dtla99   [link] [comments]\",\n",
       " 'So I will graduate in a year, and very fortunately have an offer. But the catch is, I’m from a Tech background. And the Role is “Decision Analytics Associate” in a consultancy firm(ZS Associates) famous for long working hours. The JD mentions “Use tools like R, Tableau, SAS, Visual Basic and Excel to investigate and inform client needs”. I applied for them a couple months back because I thought it would be similar to Data Analytics. But now I’m scared if I’m stuck in a non-tech job working on Excel.    submitted by    /u/V4G4X   [link] [comments]',\n",
       " 'I’m a master’s level DS that works for a healthcare company. For the first time the principal investigator (an MD, also my direct boss) and I worked on a publication to put something in a scientific journal. I did 100% of the data analysis.  Since this project was sponsored and led by my boss, his name goes as the last author. And I gave up the first author spot to a junior employee that is more clinical driven and focused on data collection. She benefits more from this publication anyway. Second author is another junior employee that helped with the data collection. The data collection process is very rigid, and they very much deserve the credit.  I voluntarily placed myself second to last. All the middle authors are people who reviewed the paper or added a punctuation or so. However there is someone (one of the middle authors) who is coveting for my spot because he has a PhD in a healthcare discipline. He didn’t write any of the paper.  Where does your name get placed if you do 100% of the analysis? Also do you think publications matter a lot in your DS career? Curious to know if the name placement even matters.    submitted by    /u/wasabiboba   [link] [comments]',\n",
       " \"I've been trying to analyze a data set that has a large number of zeros. In order to run some statistical tests, I thought it would be best to run a log transformation on the data so that I would achieve a normal distribution of data. However, I'm seeing that maybe that isn't the best and that there may be better tests more appropriate for a data set that has a high count of zeros.    submitted by    /u/dtla99   [link] [comments]\",\n",
       " \"If you're trying to use semi-supervised models to predict an output to unlabelled data, why wouldn't you use that as your main model? What value does it add strictly as an intermediate step before using supervised learning? Unless I’m mistaken that it IS NOT an intermediate step and actually is used as a final model?    submitted by    /u/anthologyxxviii   [link] [comments]\",\n",
       " '        submitted by    /u/MLtinkerer   [link] [comments] ',\n",
       " 'I’m an undergrad majoring in data science, and I’ve been considering taking an elective course in low level programming - is it worth it? Where do you use this info as a data scientist? Btw, here’s the course description: This course is concept-oriented, not specialized to a particular operating system, and not trying to teach how to code the kernel of an operating system. After reviewing a number of system programming issues, it examines the basic components of modern operating systems in terms of their function, domain, design, principles and implementation techniques, use and impact on systems programming. It describes and uses in programming homework two modern operating systems (UNIX and Windows NT). Design and implementation of a number of concurrent programs is examined. Hardware support for operating system functions is discussed. Performance issues are considered through the course.    submitted by    /u/sk81k   [link] [comments]',\n",
       " 'I am trying to find a podcast that is about statistics/data science. Most of the podcasts I’ve found use statistics to analyze things like COVID or sports, but I am looking for a podcast that talks about the field of statistics. I want to learn more about P-Hacking, the replication crisis, multiple testing and actual issues in the world of Statistics/Data Science. If y’all have any recommendations I would really appreciate them. Thanks!    submitted by    /u/em-lead-2021   [link] [comments]',\n",
       " 'In general, does anyone know which one would be faster? Numpy array CPU vectorization or PyTorch tensor GPU vectorization?    submitted by    /u/leockl   [link] [comments]',\n",
       " \"Hello, I'm new to the Data Science and Analytics worlds and would like to get more involved, meet new friends, network and learn something (I have a lot to learn). There aren't any local meetups and I doubt there will be for a while due to COVID. I was curious if there are any virtual ones? Thanks!    submitted by    /u/beauconstrictor   [link] [comments]\",\n",
       " 'Hey folks,  I am wondering what kind of JNotebook extension or addon you would like to see/use in the future. Or at least one you think others might find handy or interesting. It could be just a tweaked or augmented version of an existing extension. Here are some extensions people use today as an example: https://towardsdatascience.com/jupyter-notebook-extensions-517fa69d2231 \\u200b Any feedback or ideas would be appreciated. Cheers.    submitted by    /u/trngoon   [link] [comments]',\n",
       " 'I’m working on binary classification of a cancer dataset wherein I have made two models. One- I selected some attributes with the class and in the other model I added some further attributes for same events and then the class. I used different classifiers and checked for AUC as the metric to determine my best model. For both the cases, Logistic Regression and XGBoost did equally well. So I compared the prediction probabilities from predict_proba of both the models for Logistic Regression and found them to be different. What does this change imply and if I need to quantify it, how shall I?    submitted by    /u/takemeto95   [link] [comments]',\n",
       " \"Asking for this because I am relatively new, I have no idea where the experienced people go to get their data generally. I've been trying so hard to find a reliable data set of 'average price of properties in neighborhoods of Berlin'. For the last 2 days, and i have no luck yet, my online classes are starting in a week, I wanted to finish my capstone project before that :(  Any help would be useful!!    submitted by    /u/sicksikh2   [link] [comments]\",\n",
       " \"Which comes first - language / thought ? I don't know.  https://medium.com/illumination/you-are-not-free-and-will-never-be-38a9b5404567    submitted by    /u/Caspar_Medium   [link] [comments]\",\n",
       " 'Lakoff publicised in talks (like a talk on the conceptuals underpinnings of mathematics) a book he was going to publish with Narayanan about concepts in the brain and stuff. He used to say it was almost finished. So is it known what happened to it?    submitted by    /u/Niqolacito   [link] [comments]',\n",
       " \"Anyone know of any good sources where I can learn more about these approaches such as the comparison between 'neo-gricean' theories that emphasise pragmatics such as relevance theory by Sperber & Wilson vs Cognitive based approaches such as that by Lakoff and others + their criticisms?  My title may be incorrect, I believe relevance theory is also considered a cognitive approach to metaphor. Thank you!    submitted by    /u/overtoad2o2   [link] [comments]\",\n",
       " \"How did people came up with word, concept invention; how did they choose which sounds to use to represent certain objects,especially verbs and Gramm conjunctions etc? Moreover, how did they invent the 'tenses', besides basic future/past and present ?  It would be helpful if you could refer me to some literature for laymen :)    submitted by    /u/gertrude420   [link] [comments]\",\n",
       " 'If we take into account that language as a complex cognitive function reflects the thought processes, then is it plausible to infer that the more complex the grammar of the certain language (i.e. Finnish language where nouns have 15 different cases, and English that only has 3) , the more developed the IQ of people who speak it ( because as children, when their brain development was on fire, the Finnish kids had to use more cognitive resources than English kids)?    submitted by    /u/gertrude420   [link] [comments]',\n",
       " 'https://southeastasiaglobe.com/southeast-asian-languages-internet/    submitted by    /u/almac26   [link] [comments]',\n",
       " 'Hello all, I am interested in learning about how we construct concepts. I believe that we do this through embodied cognition and briefly reading Lakoff believe we construct concept through metaphor. But what about abstract concepts? I became blind a few years ago and wonder how people who have been blind since birth construct visual concepts... Any papers/books/debates that anyone can point me to will be greatly appreciated :)    submitted by    /u/Furnessian90   [link] [comments]',\n",
       " '  submitted by    /u/Lilziggy098   [link] [comments]',\n",
       " 'Hello! Would be grateful if you could share the criteria you use/have encountered to evaluate the adequacy of rendering semantic frames.   [link] [comments]',\n",
       " 'Hi! Does anyone know good-quality papers on semantic frames actualized by intertextual allusions? Or maybe resources where I can find them.   [link] [comments]',\n",
       " '  submitted by    /u/Self-C   [link] [comments]',\n",
       " 'Hello. I want to collect as many as papers as I can that will fall into this category. The main problem is that the \"tagging\" is not consistent for linguistic papers. Hence I\\'m looking for an exhausitve list of tags which are directly related to this field, in order to make better queries and find more relevant data.  Thanks!    submitted by    /u/quit_daedalus   [link] [comments]',\n",
       " '  submitted by    /u/Robosidd   [link] [comments]',\n",
       " 'I read an article saying that best age to learn a language is below 5 years. Why is it so difficult to learn a language when you are older. And this is from personal experience.... Even when we learn a new language at an older age, we cannot feel it.... in the sense we cannot deduce the correct emotion of a phrase in that language. Is this a correct observation? drop your ideas    submitted by    /u/Robosidd   [link] [comments]',\n",
       " 'After having many conversations in Spanish with an English speaker who is learning the language; It left me thinking why after so many times telling them that \"another\" is simply \"otro/a\" and not \"un/a otro/a\" and kind of explaining the \"logic\" behind, they still say the latter. Maybe some people fall more frequently into habits ingrained by their native language, like always mismatching words\\' gender with the ones from their native tongue. can\\'t it be as easy as just switching them to follow suit the grammar of their target language?    submitted by    /u/deiong   [link] [comments]',\n",
       " 'Many languages describe musical pitch (a perception mapped to acoustic frequency) as \"high\" or \"low\" with shorter wavelengths called \"high\" and longer ones \"low\" or \"deep.\" From previous searching, I seem to remember a language (or language family) using \"slender\" for high pitches. If memory serves, \"broad\" was the corresponding concept for low sounds. Asking English speakers about it in face-to-face conversation, I mostly get speculation. Most recently someone linked it to a piano keyboard... that goes from left to right. Some cast it in terms of singing, stretching the head and neck up for high notes and tucking in for low notes. I\\'ve heard a voice teacher say the opposite applies to trained singers, who extend the vocal tract for low notes. Speculate away if you like, but I\\'m really interested in scholarly takes on how this conceptual metaphor came about, if that\\'s what it is.    submitted by    /u/bazzage   [link] [comments]',\n",
       " 'https://medium.com/illegal-human-experimentation/illegal-immigrant-human-test-subject-experimentation-in-the-usa-2961895e4b2b    submitted by    /u/sdhksasm   [link] [comments]',\n",
       " ' [link] [comments]',\n",
       " 'Frederick Pollock. Principles Of Contract. (1902) p. 170. p. 220/400 here.  \\xa0 \\xa0 \\xa0 \\xa0 The name of Consideration appears only about the beginning of the sixteenth century, and we do not know by what steps it became a settled term of art. The word seems to have gone through the following significations : [1.] first, contemplation in general; [2.] then deliberate decision on a disputed question (hence the old form of judgments in the Common Law Courts, “It is considered”) (e); [3.] then the grounds as well as the act of deliberation; and [4.] lastly, in particular, that which induces a grant or promise.   I too don\\'t \"know by what steps\" signification 2 shifted to 3 and 4! Can you kindly expatiate the steps?  I understand signification 1, as it\\'s just the ordinary meaning of \"consideration\"! But 2 feels unintelligibly alien, and 3 and 4 have semantically shifted too much for me to fill in the steps. Kindly find the quote alongside the red line in this scan. If we can rely on Blank\\'s 1999 typology, what type of semantic shift is this? Specialization of meaning?     submitted by    /u/etym0n   [link] [comments]',\n",
       " \"I'm doing a masters next semester and doing as much reading in cognitive linguistics in the meantime, particularly Langackers' Essentials of Cogntive Grammar. I'm struggling to fully clarify in my mind what is meant by construal and schemetisation and how it relates to the cognitive grammar framework. Any help on this would be highly appreciated. Thank you!    submitted by    /u/overtoad2o2   [link] [comments]\",\n",
       " 'Just wondering if anybody here is going to attend ICLC in Nishinomiya, Japan this summer. If so, are you presenting and what are you presenting on?    submitted by    /u/Simazhi   [link] [comments]',\n",
       " 'My Hong Kong friend\\'s native languages are Cantonese and Mandarin. She rates herself at B1. Peer-reviewed research substantiates that etymology can assist in learning vocabulary. See Google Scholar. English SE has an obvious example.  What books teach English using linguistics like Etymology, Semantics, and Derivational Morphology? She has been learning English for 20 years. Yet, until I showed her today, she didn\\'t know the semantic shift of \"even\" as an adjective to \"even\" as an adverb, or how Latin putare is the root behind \"amputate; compute; count ; depute; deputy; dispute; impute; pave; pavement; putative; reputation; repute.\" To filter for quality, she prefers authors who are linguists aware of research on L2 Acquisition, with PhD in linguistics.     submitted by    /u/phrassein   [link] [comments]',\n",
       " 'What a story!  Evans versus the University of Leiden. Split decision. 3 years in the making. University guilty of mishandling his application for a position but Professor Evans claims of damage to reputation are not found to have merit.  The link to the original court document, in Dutch, below.  To summarize from the court document, in March 2016 Evans applies for a professor position at Leiden, invited for interview in April. Meanwhile, before the interview and any position is offered Evans takes voluntary redundancy from his UK University on April 15th, 2016. A member of Leiden committee hears rumours about Evan’s not working well with others, he illegally reaches out, breaking Dutch HR regulations, and solicits letter stating such from Evan’s former UK colleague now at the Uni of Graz. Letter read to committe. Meanwhile it is revealed that Evan’s wife’s PhD promotor is a member of the appointment committee. Conflict of interest issues raised. Private derogatory letter and conflicted panel member to much – panel dissolved, candidate rankings discarded, candidates informed of irregularities on May 26th, 2016 and that new panel to be formed. Evans informed on June 21st by Leiden HR that he simply has to affirmatively reply to email to be reconsidered – no new application required. Evans consults lawyer. Leiden HR gives Evans until July 4th, 2016 to affirmatively respond. Evans again declines, his application is not considered, position closed. Evans starts 3 years of litigation that concludes on June 19th, 2019. In the court documents Evans claims that his career has been ruined by Leiden and that he has been unable to find work since. If you read his version of events at https://www.vyvevans.net/court-case-evans-vs-leiden many of the critical details are not included. Details such as , again, he took voluntary redundancy with respect to his UK professorship before being offered the Leiden position, so unemployment is really his fault, and that the court found any reputational damage suffered was his essentially own fault. The court writes, in Section 4.15 ‘with respect to reputation [Evans] has chosen to publicize his application experience at Leiden University with the widely distributed press release in June 2016 and his cooperation in various interviews. Leiden University et al., On the other hand, consciously chose to give as little publicity to the course of events as possible.’ Talk about an own goal.  So his application was mishandled and his rights under Dutch employment law were violated so he deserved to win that part of his case, but he was not successful on the most important claim, damage to his self? claimed reputation as a ‘World-leading authority on English Linguistics’.  As far as I can tell that description, World-leading authority, is not found anywhere else but Evan’s own website. In fact, all of the text he has cited appears to have been taken from unattributed Dutch sources. I could be wrong but still not great form,especially for one who still calls himself a professor. But he has not held that position, or title, in his own words, since December 2016. That seems odd to me. It doesn’t appear that he currently has any academic affiliation from his website. It was my understanding that the title comes with the job, no position then no title, unless one has emeritus status and I don’t believe he has retired. Yet he still self refers to himself as Professor Vyv Evans. He is an accomplished author and a well known cognitive linguist but still claiming the academic title feels somewhat deceptive.  https://www.rechtspraak.nl/Organisatie-en-contact/Organisatie/Rechtbanken/Rechtbank-Den-Haag/Nieuws/Paginas/Universiteit-Leiden-heeft-in-sollicitatieprocedure-onrechtmatig-gehandeld.aspx    submitted by    /u/K-Canuck   [link] [comments]',\n",
       " \"I'd really like to find something like this and thought linguists might be good people to ask, as you might use this for your research?    submitted by    /u/ErinFlight   [link] [comments]\",\n",
       " 'Up/down is referenced with gravity, front/back with where your eyes are on your body for example.    submitted by    /u/squiryl   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " '  submitted by    /u/ShareScienceBot   [link] [comments]',\n",
       " 'https://www.youtube.com/watch?v=62HBi0B3se0&feature=youtu.be&fbclid=IwAR2hhMZAQgMBilNnb3Cv20nyHN786rprTwliaw19I1QYFCQxqFI_FNZbtiU    submitted by    /u/analyticsindiam   [link] [comments]',\n",
       " \"Hello guys,  My first notebook on Kaggle, I'm a newbie data science enthusiast. I've built a Python spell-checker. Handling about 3-4 different types of spelling mistakes and different approaches to correct them.  Do give me some feedback and thanks for your time! :) Do leave an upvote on the notebook if you found it interesting! https://www.kaggle.com/amarananth/spellcheck-python    submitted by    /u/theamar961   [link] [comments]\",\n",
       " '  submitted by    /u/durgeshsamariya   [link] [comments]',\n",
       " 'https://analyticsindiamag.com/arxiv-makes-all-its-research-papers-available-on-kaggle-to-boost-machine-learning-developments/    submitted by    /u/analyticsindiam   [link] [comments]',\n",
       " \"Hello. There is a public notebook (https://www.kaggle.com/stalkermustang/converting-lyft-dataset-to-kitty-format) that generates an output folder and I wish to download that output folder. However, once I generate the output folder, I cannot see the option to download it. I save it using 'Save & Run All (Commit)' but I am still unable to download the output folder. Please tell me how to do this.    submitted by    /u/zimmer550king   [link] [comments]\",\n",
       " 'https://analyticsindiamag.com/chief-data-scientist-interview-sandip-bhattacharjee/    submitted by    /u/analyticsindiam   [link] [comments]',\n",
       " \"I commited my kernel and waited for the output to be made, I can see previews of the images I generated in the kernel viewer. I can't find the button to download everything (I made thousands of images, can't save them one by one) and all the how tos seem to be outdated for the current interface. I'd be very grateful if someone told me how to download my output from the kernel.    submitted by    /u/KasRazak   [link] [comments]\",\n",
       " 'Hi all, Since today afternoon I am unable to publish discussion on kaggle. I am able to comment on discussion but can’t publish new discussion topic. Really appreciate your help.    submitted by    /u/durgeshsamariya   [link] [comments]',\n",
       " '  submitted by    /u/analyticsindiam   [link] [comments]',\n",
       " 'Hi community, We at Q Blocks are offering 100 hours of GPU computing credits to 5 Kaggle Pros Send across your Kaggle profile to win these computing credits. Cheers    submitted by    /u/svij137   [link] [comments]',\n",
       " 'Hi guys, I\\'m just wondering if you could use kaggle for crypto mining with their gpus. I looked around their terms of use and didn\\'t see the words \"crypto\" or \"mining\" once. You can tell I\\'m a bit low on cash :)    submitted by    /u/subzer0d   [link] [comments]',\n",
       " 'https://analyticsindiamag.com/khoi-nguyen-kaggle-master-interview-data-science/    submitted by    /u/analyticsindiam   [link] [comments]',\n",
       " 'I want to use output file in another kernal i created how should i do it    submitted by    /u/shaelander1990   [link] [comments]',\n",
       " \"Hello. I am using Kaggle for the first time. Before I used Google Colab but, after you use a GPU session in Colab for 12 hours, you get a cooldown of about a day which is annoying. I am now using Kaggle but I cannot figure out how to connect my Google Drive account to my Kaggle notebook. I try typing the following code in a cell: # Set your own project id here PROJECT_ID = 'MYNAME' from google.cloud import storage storage_client = storage.Client(project=PROJECT_ID)  And then I type ls but I don't see my Google Drive over there (I only see __notebook_source__.ipynb). In Google Colab, your Google Drive is mounted and you can access your files by typing in shell commands in the Colab cell but I don't see anything like that here. Please tell me how I can connect my Google Drive to my Kaggle session.    submitted by    /u/zimmer550king   [link] [comments]\",\n",
       " \"I wrote a paper summary about the state-of-the-art neural machine translation model developed by Facebook. This is an unsupervised model and it doesn't require parallel corpora to train. If you are interested in neural machine translation, you definitely would like to check this post out.  https://medium.com/@outiscjh/unsupervised-machine-translation-using-monolingual-corpora-paper-summary-c387de4ed6e3  Feel free to leave comments and give me a clap and follow me on medium if you like to see more paper summary. Thank you.    submitted by    /u/eistint   [link] [comments]\",\n",
       " 'Do you know if kaggle courses are ok to stream to public in periscope or twitch where I and the viewers go through them together?    submitted by    /u/fetishless   [link] [comments]',\n",
       " \"Hey Guys, \\u200b Intro: I was a particle physicist with work experience at CERN and few other physics labs in the world. I stumbled upon the concept of distributed supercomputing there and that fascinated me and now we have invented a peer to peer computing network as a first step towards the big goal. Imagine this as the Airbnb of computing. As an introduction of our service, we are inviting few superusers to test us out with free GPU computing hours they can use for their personal work or for taking part in competitions on Kaggle. Criteria for this is that you must be super engaged on Kaggle or other highly competitive sites as a signal that you use GPU's pretty often for work and competition. You can send me a personal message and i will send you a personal invite to access your GPU computing credits. \\u200b \\u200b Regards Saurabh Vij Q Blocks    submitted by    /u/svij137   [link] [comments]\",\n",
       " 'Hello, I participated in a few machine learning competitions at Kaggle, MICCAI, CVPR. I am Kaggle Grandmaster. Last fall I helped to host the Lyft 3D Object Detection for Autonomous Vehicles competition. In the past month, I had 3 discussions with people that are thinking about creating a machine learning competition for their company. To save time for people that are also interested in the topic I will write a blog post that summarizes my experience. To do a better job I need your help. Could you please write questions about hosting the competition in the comments? Any questions will work: Smart, stupid, provocative :) To start:  How much does it cost to host the competition at Kaggle? Why prize money becomes smaller every year? How does Kaggle check the data before the competition? How should the organizer promote the competition? How to engage the academic audience? Why do we have data leaks? What happens if the organizer wants the metric that Kaggle does not have? Do winning solutions go to production? Should organizers write an academic paper about the results of the competition? Do hiring managers look at the top competitors? How to prepare the data for a competition? How to estimate what type of hardware is necessary? What are the other things that should be prepared except the data?  etc I do not know the answer to many of them, but I have a few ideas that I would like to share. Ask your questions!    submitted by    /u/ternausX   [link] [comments]',\n",
       " '  submitted by    /u/analyticsindiam   [link] [comments]',\n",
       " 'Hello guys, I recently published my first kernel on Kaggle. You can access the notebook here. Any feedback/comment is greatly appreciated!    submitted by    /u/tesbiha   [link] [comments]',\n",
       " 'I trained a model using object detection API (tensorflow) and saved the corresponding Inference graph file on drive (.pb file) I was able to perform Inference on colab but not on kaggle it shows the following error. The session graph is empty.Add operations to the graph before calling run() If someone could help me out I can give more info or even share the link to the .pb model \\u200b https://imgur.com/a/zGSZNdB link contains the code and the error    submitted by    /u/bjain1   [link] [comments]',\n",
       " 'I published my first Kaggle notebook \"How to annotate your image dataset effectively\". I am pretty new to Kaggle so it would be great, if you guys take the time to actually visit the kernel and upvote it to help me get a wider reach on Kaggle , a noob wants to learn. https://www.kaggle.com/divyanshtripathi/annotate-your-own-image-dataset-efficiently/notebook    submitted by    /u/dee_tee_   [link] [comments]',\n",
       " 'I have been thinking of getting into kaggle for a long time but cant because i know nothing about making models and all. I have done programming with c++ for quite a while and had recently started with python for the same intension. I decided to start with kaggle course but it was hard to follow. But should i just dive into it or complete the course first( i am talking about the courses on coursera)?    submitted by    /u/Mkrsharma   [link] [comments]',\n",
       " 'Hi, everyone. This is my first post on medium sharing the tips and tricks that help me to win a kaggle competition. I hope this is helpful for your upcoming challenge. https://medium.com/@outiscjh/how-to-win-a-kaggle-classification-competition-bcf87273968e Feel free to comment. Please clap, share and follow me on medium if you think the content is helpful. I plan to come out with more topics on kaggle competition. Do let me know if there is any topic you wish me to cover. Thanks    submitted by    /u/eistint   [link] [comments]',\n",
       " 'https://analyticsindiamag.com/kaggle-master-interview-oleg/    submitted by    /u/analyticsindiam   [link] [comments]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ktrain import text \n",
    "zsl = text.ZeroShotClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate questions and answers from both real and synthetic contextsPhoto Credit(This post was originally published on my personal\\xa0blog.)Preamble“Training Question Answering Models From Synthetic Data” is an NLP paper from Nvidia that I found very interesting. Question and answer(QA) data is expansive to obtain. If we can use the data we have to generate more data, that will be a huge time saver and create a lot of new possibilities. This paper shows some promising results in this direction.Some caveats:We need big models to be able to get decent results. (The paper reported question generation models with the number of parameters from 117M to 8.3B. See the ablation study in the following sections.)Generated QA data is still not at the same level as the real data. (At least 3x+ more synthetic data is needed to reach the same level of accuracy.)There are a lot of contents in this paper, and it can be a bit overwhelming. I wrote down parts of the paper that I think is most relevant in this post, and hopefully, it can be helpful to you as\\xa0well.MethodComponentsThere are three or four stages in the data generation process. Each stage requires a separate\\xa0model:Stage 0 [Optional]\\u200a—\\u200aContext generation: The SQuAD 1.1 training data were used to train the following three stages (Figure 2 below). But when testing/generating, we can choose to use real Wikipedia data or use a model to generate Wikipedia-like data.Stage 1\\u200a—\\u200aAnswer Generation: A BERT-style model to do answer extraction from the given context. The start and the end of the token span are jointly\\xa0sampled.Stage 2\\u200a—\\u200aQuestion Generation: Fine-tuned GPT-2 model to generation question from the context and the\\xa0answer.Stage 3\\u200a—\\u200aRoundtrip Filtration: A trained extractive QA model to get the answer from the context and the generated question. If the predicted answer matches the generated answer, we keep this triplet (context, answer, and question). Otherwise, the triplet is discarded.The last step seems to be very strict. Any deviation from the generated answer will not be tolerated. However, given the EM(exact match) of the model trained on SQuAD 1.1 alone is already 87.7%, it’s reasonable to expect that the quality of answer predicted by the filtration model to be quite accurate. The paper also proposes an over-generation technique (generate two questions for each answer and context pair) to compensate for those valid triplets being discarded.(Taken from the source\\xa0paper)More DetailsContext GenerationBeside using Wikipedia documents as contexts, this paper also generates completely synthetic contexts using an 8.3B GPT-2\\xa0model:This model was first trained with the Megatron-LM codebase for 400k iterations before being fine-tuned on only Wikipedia documents for 2k iterations. This allows us to generate high-quality text from a distribution similar to Wikipedia by using top-p (p = 0.96) nucleus sampling.Answer GenerationThis paper train the answer generation model to match the exact answer in the training data. This naturally ignores the other possible answers from the context but seems to be a more generalizable way to do\\xa0it.The joint modeling of the starts and the ends of the answer span, which is reported to perform better, creates more candidates in the denominator in the calculation of the likelihood.(Taken from the source\\xa0paper)(I’m not very sure about the complexity and performance impact of this joint approach.)Question GenerationThis paper uses token type ids to identify the components in the triplets. The answer span in the context are also marked by the answer token id. Special tokens is also added to the start and the end of the questions.(Taken from the source\\xa0paper)Number of Triplets GeneratedAs explained in the previous section, the paper uses an over-generation technique to compensate for the model precision problem. Two questions are generated for each answer and context pair (a.k.a. answer candidate). Answer candidates of the context are generated by top-k sampling within a nucleus of p = 0.9 (that means we take the samples with the highest likelihoods until we either get K samples or the cumulative probabilities of the samples taken reaches\\xa00.9).(Taken from the source\\xa0paper)In the ablation study(which will be covered in the following sections), the models in stage 1 to 3 are trained with half of the SQuAD 1.1 training data, and the other half is used to generate synthetic data. The performance of the QA model trained on synthetic data is used to evaluate the quality of synthetic data.From the table above (Table 4), we can see that the smaller model on average generated 2.75 valid triplets per context, and the larger model generated 4.36 triplets. Those synthetic datasets are already bigger than the SQuAD 1.1 training\\xa0set.ExperimentsModel ScaleTable 4 (in the previous section) shows that larger models in stage 1 to 3 create better data for the downstream model, but it is not clear whether it was the quality of the data or the quantity of the data that\\xa0helped.(Taken from the source\\xa0paper)Table 5 shows that the quality of questions generated does increase as the model scales\\xa0up.(Taken from the source\\xa0paper)To test the quality of the generated answers, the paper used the 1.2B question generator (see Table 5) to generate questions without filtration from the generated answers, fine-tune a QA model and test against the dev set. Table 6 shows that bigger model increases the quality of generated answers, but only marginally.(I am not sure how they obtained the benchmark for BERT-Large, though. I think BERT-Large expects a context and question pair to generate an answer, but here we want to generate answers from only the context. Maybe they take the pre-trained BERT-Large model and fine-tune it like the other\\xa0two.)(Taken from the source\\xa0paper)In Table 7 we can see that filtration does improve the performance of the downstream models (compare to Table 5). When using real answers to generate questions, less than 50% of the triplets generated by the 345M model were rejected, while about 55% by the 1.2B model was rejected. Note that all the models in this set under-performed to the model trained with only human-generated data (SQuAD training\\xa0set).The additional triplets from using generated answers are quite helpful, the 1.2B model finally surpassed the baseline model (human-generated data), but it used 3x+ more\\xa0data.To sum up, the ablation study shows that scaling up the model improved the quality of the generated data, but the increase in the quantity of the data also played a\\xa0part.Fully Synthetic DataIn this part of the paper, they trained the models for stage 1 to 3 using the full SQuAD 1.1 training set, and use the deduplicated Wikipedia documents as contexts to generate answers and questions. They also fine-tune an 8.3B GPT-2 model on Wikipedia documents to generate synthetic contexts.(Taken from the source\\xa0paper)Table 2 shows that synthetic contexts can be as good as the real ones. Also, further fine-tuning on the real SQuAD 1.1 data can further improve the performance, which might imply that there is still something missing in the fully or partially synthetic triplets.However, using 200x+ more data to get less 1% more accuracy seems wasteful. We want to know how much synthetic data we need to reach the baseline accuracy. The next section answers this question.The Quantity of Data Required to Beat the\\xa0Baseline(Taken from the source\\xa0paper)(The “data labeled” seems to mean the size of the corpus used to generate the triplets, not the size of generated triplets.)Figure 3 shows that we need at least 50 MB of text labeled to reach the baseline accuracy (without fine-tuning with the real data), 100 MB to surpass. That’s 2.5x+ and 7x+ more than the real one used by the baseline. Considering there are multiple triplets generated by one context, the number of triplets required is estimated (by me) to be around 20x and 40x\\xa0more.The silver lining is that only 10 MB of text is needed to be labeled if we fine-tune the model with the real SQuAD data to surpass the baseline. That roughly translates to 3 to 4 times more triplets used than the baseline. So real plus synthetic data is probably the way to go for\\xa0now.Wrapping UpThere are quite a few more details I did not cover in this post. Please refer to the source paper if you want to know\\xa0more.All in all, very interesting results in this paper. Unfortunately, the amount of compute needed to synthesize the data and the amount of synthetic data needed to reach good results are still staggering. But on the other hand, it is essentially trade compute for the human labors required by the annotation process, and it might not be a bad\\xa0deal.[Notes] Training Question Answering Models From Synthetic Data was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_texts[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_article = article_texts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_strings = [\n",
    "    'deep learning',\n",
    "    'natural language processing',\n",
    "    'computer vision',\n",
    "    'visualization',\n",
    "    'industry',\n",
    "    'implementation',\n",
    "    'computer programming',\n",
    "    'reddit question',\n",
    "    'research',\n",
    "    'startup'\n",
    "]\n",
    "results = zsl.predict(example_article, topic_strings=topic_strings, include_labels=True, max_length=256)\n",
    "results = sorted(results, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Author: Chris Padwick, Director of Computer Vision and Machine Learning at Blue River TechnologyHow did farming affect your day today? If you live in a city, you might feel disconnected from the farms and fields that produce your food. Agriculture is a core piece of our lives, but we often take it for\\xa0granted.A 2017 prototype of See & Spray, Blue River Technology’s precision weed control\\xa0machineFarmers today face a huge challenge\\u200a—\\u200afeeding a growing global population with less available land. The world’s population is expected to grow to nearly 10 billion by 2050, increasing the global food demand by 50%. As this demand for food grows, land, water, and other resources will come under even more pressure. The variability inherent in farming, like changing weather conditions, and threats like weeds and pests also have consequential effects on a farmer’s ability to produce food. The only way to produce more food while using less resources is through smart machines that can help farmers with difficult jobs, offering more consistency, precision, and efficiency.Alex Marsh, one of our Field Operations Specialists, pictured with a Self Propelled Sprayer. We work with big machines at Blue River\\u200a—\\u200aAlex is 6’4” tall and is about level with the top of the\\xa0tire.Agricultural roboticsAt Blue River Technology, we are building the next generation of smart machines. Farmers use our tools to control weeds and reduce costs in a way that promotes agricultural sustainability. Our weeding robot integrates cameras, computer vision, machine learning and robotics to make an intelligent sprayer that drives through fields (using AutoTrac to minimize the load on the driver) and quickly targets and sprays weeds, leaving the crops\\xa0intact.The machine needs to make real-time decisions on what is a crop and what is a weed. As the machine drives through the field, high resolution cameras collect imagery at a high frame rate. We developed a convolutional neural network (CNN) using PyTorch to analyze each frame and produce a pixel-accurate map of where the crops and weeds are. Once the plants are all identified, each weed and crop is mapped to field locations, and the robot sprays only the weeds. This entire process happens in milliseconds, allowing the farmer to cover as much ground as possible since efficiency matters. Here is a great See & Spray Video that explains the process in more\\xa0detail.To support the Machine Learning (ML) and robotics stack we built an impressive compute unit, based on the NVIDIA Jetson AGX Xavier Edge AI platform. Since all our inference happens in real time, uploading to the cloud would take too long, so we bring the server farms to the field. The total compute power on board the robot just dedicated to visual inference and spray robotics is on par with IBM’s super computer, Blue Gene (2007). This makes this a machine with some of the highest compute capacity of any moving machine machinery in the\\xa0world!Building weed detection modelsMy team of researchers and engineers is responsible for training the neural network model that identifies crops and weeds. This is a challenging problem because many weeds look just like crops. Professional agronomists and weed scientists train our labeling workforce to label the images correctly\\u200a—\\u200acan you spot the weeds\\xa0below?You are looking at cotton plants and some weeds. Can you tell the difference?In the image below, the cotton plants are in green and the weeds are in\\xa0red.The cotton plants are in green and the weeds are in\\xa0red.Machine learning\\xa0stackOn the machine learning front, we have a sophisticated stack. We use PyTorch for training all our models. We have built a set of internal libraries on top of PyTorch which allow us to perform repeatable machine learning experiments. The responsibilities of my team fall into three categories:Build production models to deploy onto the\\xa0robotsPerform machine learning experiments and research with the goal of continually improving model performanceData analysis / data science related to machine learning, A/B testing, process improvement, software engineeringWe chose PyTorch because it’s very flexible and easy to debug. New team members can quickly get up to speed, and the documentation is thorough. Before working with PyTorch, our team used Caffe and Tensorflow extensively. In 2019, we made a decision to switch to PyTorch and the transition was seamless. The framework gives us the ability to support production model workflows and research workflows simultaneously. For example we use the torchvision library for image transforms and tensor transformations. It contains some basic functionality and it also integrates really nicely with sophisticated augmentation packages like imgaug. The transforms object in torchvision is a piece of cake to integrate with imgaug. Below is a code example using the Fashion MNIST dataset. A class called CustomAugmentor initializes the iaa.Sequential object in the constructor, then calls augment_image() in the __call__ method. CustomAugmentor() is then added to the call to transforms.Compose(), prior to ToTensor(). Now the train and val data loaders will apply the augmentations defined in CustomAugmentor() when the batches are loaded for training and validation.https://medium.com/media/fb45b468be83d1466047055fb6c243ad/hrefAdditionally, PyTorch has emerged as a favorite tool in the computer vision ecosystem (looking at Papers With Code, PyTorch is a common submission). This makes it easy for us to try out new techniques like Debiased Contrastive Learning for semi-supervised training.On the model training front, we have two normal workflows: production and research. For research applications, our team runs PyTorch on an internal, on-prem compute cluster. Jobs being executed on the on-premise cluster are managed by Slurm, which is an HPC batch job based scheduler. It is free, easy to set up and maintain, and provides all the functionality our group needs for running thousands of machine learning jobs. For our production based workflows we utilize an Argo workflow on top of a Kubernetes (K8s) cluster hosted in AWS. Our PyTorch training code is deployed to the cloud using\\xa0Docker.Deploying models on field\\xa0robotsFor production deployment, one of our top priorities is high-speed inference on the edge computing device. If the robot needs to drive more slowly to wait for inferences, it can’t be as efficient in the fields. To this end, we use TensorRT to convert the network to an NVIDIA Jetson AGX Xavier optimized model. TensorRT doesn’t accept JIT models as input so we use ONNX to convert from JIT to ONNX format, and from there we use TensorRT to convert to a TensorRT engine file that we deploy directly to the device. As the toolstack evolves, we expect this process to improve as well. Our models are deployed to Artifactory using a Jenkins build process and they are deployed to remote machines in the field by pulling from Artifactory.To monitor and evaluate our machine learning runs, we have found the Weights & Biases platform to be the best solution. Their API makes it fast to integrate W&B logging into an existing codebase. We use W&B to monitor training runs in progress, including live curves of the training and validation loss.SGD vs Adam\\xa0ProjectAs an example of using PyTorch and W&B, I will run an experiment and compare the results of using different solvers in PyTorch. There are a number of different solvers in PyTorch\\u200a—\\u200athe obvious question is which one should you pick? A popular choice of solver is Adam. It often gives good results without needing to set any parameters and is our usual choice for our models. In PyTorch, this solver is available under torch.optim.adam. Another popular choice of solver for machine learning researchers is Stochastic Gradient Descent (SGD). This solver is available in PyTorch as torch.optim.SGD. If you’re not sure of the differences between the two, or if you need a refresher, I suggest reviewing this write up. Momentum is an important concept in machine learning, as it can help the solver to find better solutions by avoiding getting stuck in local minima in the optimization space. Using SGD and momentum the question is this: Can I find a momentum setting for SGD that beats\\xa0Adam?The experimental setup is as follows. I use the same training data for each run, and evaluate the results on the same test set. I’m going to compare the F1 score for plants between different runs. I set up a number of runs with SGD as the solver and sweeping through momentum values from 0–0.99 (when using momentum, anything greater than 1.0 causes the solver to diverge). I set up 10 runs with momentum values from 0 to 0.9 in increments of 0.1. Following that I performed another set of 10 runs, this time with momentum values between 0.90 and 0.99, with increments of 0.01. After looking at these results, I also ran a set of experiments at momentum values of 0.999 and 0.9999. Each run was done with a different random seed, and was given a tag of “SGD Sweep” in W&B. The results are shown in Figure\\xa01.Figure 1: On the left hand side the f1 score for crops is shown on the x-axis, and the run name is shown on the y axis. On the right hand side the f1 score for plants as a function of momentum value is\\xa0shown.It is very clear from Figure 1 that larger values of momentum are increasing the f1 score. The best value of 0.9447 occurs at momentum value of 0.999, and drops off to a value of 0.9394 at a momentum value of 0.9999. The values are shown in the table\\xa0below.Table 1: Each run is shown as a row in the table above. The last column is the momentum setting for the run. The F1 score, precision, and recall for class 2 (crops) is\\xa0shown.How do these results compare to Adam? To test this I ran 10 identical runs using torch.optim.Adam with just the default parameters. I used the tag “Adam runs” in W&B to identify these runs. I also tagged each set of SGD runs for comparison. Since a different random seed is used for each run, the solver will initialize differently each time and will end up with different weights at the last epoch. This gives slightly different results on the test set for each run. To compare them I will need to measure the spread of values for the Adam and SGD runs. This is easy to do with a box plot grouped by tag in\\xa0W&B.Figure 2: The spread of values for Adam and SGD. The Adam runs are shown in the left of the graph in green. The SGD runs are shown as brown (0.999), teal (0–0.99), blue (0.9999) and yellow\\xa0(0.95).The results are shown in graph form in Figure 2, and in tabular form in Table 2. The full report is available online too. You can see that I haven’t been able to beat the results for Adam by just adjusting momentum values with SGD. The momentum setting of 0.999 gives very comparable results, but the variance on the Adam runs is tighter and the average value is higher as well. So Adam appears to be a good choice of solver for our plant segmentation problem!Table 2: Run table showing f1 score, optimizer and momentum value for each\\xa0run.PyTorch VisualizationsWith the PyTorch integration, W&B picks up the gradients at each layer, letting us inspect the network during training.W&B experiment tracking also makes it easy to visualize PyTorch models during training, so you can see the loss curves in real time in a central dashboard. We use these visualizations in our team meetings to discuss the latest results and share\\xa0updates.As the images pass through our PyTorch model, we seamlessly log predictions to Weights & Biases to visualize the results of model training. Here we can see the predictions, ground truth, and labels. This makes it easy to identify scenarios where model performance isn’t meeting our expectations.The ground truth, predictions and the difference between the two. Crops are shown in green, while weeds are shown in\\xa0red.Here we can quickly browse the ground truth, predictions and the difference between the two. We’ve labeled the crops in green and the weeds in red. As you can see, the model is doing a pretty reasonable job of identifying the crops and the weeds in the\\xa0image.Here is a short code example of how to work with data frames in\\xa0W&B:https://medium.com/media/32ada41235b3678efea15cbb5fc58b5c/hrefReproducible modelsReproducibility and traceability are key features of any ML system, and it’s hard to get right. When comparing different network architectures and hyperparameters, the input data needs to be the same to make runs comparable. Often individual practitioners on ML teams save YAML or JSON config files\\u200a—\\u200ait’s excruciating to find a team member’s run and wade through their config file to find out what training set and hyperparameters were used. We’ve all done it, and we all hate\\xa0it.A new feature that W&B just released solves this problem. Artifacts allow us to track the inputs and outputs of our training and evaluation runs. This helps us a lot with reproducibility and traceability. By inspecting the Artifacts section of a run in W&B I can tell what datasets were used to train the model, what models were produced (from multiple runs), and the results of the model evaluation.A typical use case is the following. A data staging process downloads the latest and greatest data and stages it to disk for training and test (separate data sets for each). These datasets are specified as artifacts. A training run takes the training set artifact as input and outputs a trained model as an output artifact. The evaluation process takes the test set artifact as input, along with the trained model artifact, and outputs an evaluation that might include a set of metrics or images. A directed acyclic graph (DAG) is formed and visualized within W&B. This is helpful since it is very important to track the artifacts that are involved with releasing a machine learning model into production. A DAG like this can be formed\\xa0easily:One of the big advantages of the Artifacts feature is that you can choose to upload all the artifacts (datasets, models, evaluations) or you can choose to upload only references to the artifacts. This is a nice feature because moving lots of data around is time consuming and slow. With the dataset artifacts, we simply store a reference to those artifacts in W&B. That allows us to maintain control of our data (and avoid long transfer times) and still get traceability and reproducibility in machine learning.https://medium.com/media/56b63553a5b4abaefb1037221b2b9cb5/hrefLeading ML\\xa0teamsLooking back on the years I’ve spent leading teams of machine learning engineers, I’ve seen some common challenges:Efficiency: As we develop new models, we need to experiment quickly and share results. PyTorch makes it easy for us to add new features fast, and Weights & Biases gives us the visibility we need to debug and improve our\\xa0models.Flexibility: Working with our customers in the fields, every day can bring a new challenge. Our team needs tools that can keep up with our constantly evolving needs, which is why we chose PyTorch for its thriving ecosystem and W&B for the lightweight, modular integrations.Performance: At the end of the day, we need to build the most accurate and fastest models for our field machines. PyTorch enables us to iterate quickly, then productionize our models and deploy them in the field. We have full visibility and transparency in the development process with W&B, making it easy to identify the most performant models.I hope you have enjoyed this short tour of how my team uses PyTorch and Weights and Biases to enable the next generation of intelligent agricultural machines!About the\\xa0authorI am the Director of Computer Vision and Machine Learning at Blue River Technology. We build robots that distinguish crop from weed in an agricultural field and then only spray the weeds. I’ve worked at Blue River for 4 and a half years. My background is in Physics and Astronomy and in grad school I helped build a telescope to measure the Cosmic Background Radiation. Check out our careers page, we are\\xa0hiring!AI for AG: Production machine learning for agriculture was originally published in PyTorch on Medium, where people are continuing the conversation by highlighting and responding to this story.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [c for c, __ in results]\n",
    "scores = [score for __, score in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(scores, categories)\n",
    "sns.despine(left=True, bottom=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
