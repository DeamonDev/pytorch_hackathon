{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp haystack_code_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "\n",
    "from haystack import Finder\n",
    "from haystack.database.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.database.memory import InMemoryDocumentStore\n",
    "\n",
    "from haystack.retriever.dense import EmbeddingRetriever\n",
    "from haystack.utils import print_answers\n",
    "from pytorch_hackathon import rss_feeds\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/pytorch_hackathon\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeds.txt  topics.txt  zsl_feed_results.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feed_urls = list(pd.read_table('data/feeds.txt', header=None).iloc[:,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:09<00:00,  1.69it/s]\n",
      "/home/kuba/Projects/pytorch_hackathon/pytorch_hackathon/rss_feeds.py:63: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 63 of the file /home/kuba/Projects/pytorch_hackathon/pytorch_hackathon/rss_feeds.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  feed_df['text'] = feed_df['summary'].apply(lambda s: bs4.BeautifulSoup(s).text)\n"
     ]
    }
   ],
   "source": [
    "feed_df = rss_feeds.get_feed_df(rss_feed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print = pprint.PrettyPrinter(indent=2).pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>links</th>\n",
       "      <th>link</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_detail</th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>...</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>comments</th>\n",
       "      <th>authors</th>\n",
       "      <th>author</th>\n",
       "      <th>author_detail</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>content</th>\n",
       "      <th>href</th>\n",
       "      <th>media_thumbnail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Norm-in-Norm Loss with Faster Convergence and Better Performance for Image Q...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/norm-in-norm-loss-with-faster-convergence-and</td>\n",
       "      <td>Experiments on two relevant datasets (KonIQ-10k and CLIVE) show that, compar...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/norm-in-norm-loss-with-faster-convergence-and</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Image quality assessment', 'scheme': None, 'label': None}]</td>\n",
       "      <td>Experiments on two relevant datasets (KonIQ-10k and CLIVE) show that, compar...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prototype Mixture Models for Few-shot Semantic Segmentation</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/prototype-mixture-models-for-few-shot</td>\n",
       "      <td>Few-shot segmentation is challenging because objects within the support and ...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/prototype-mixture-models-for-few-shot</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Few-shot semantic segmentation', 'scheme': None, 'label': None}, ...</td>\n",
       "      <td>Few-shot segmentation is challenging because objects within the support and ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Automatic Failure Recovery and Re-Initialization for Online UAV Tracking wit...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/automatic-failure-recovery-and-re</td>\n",
       "      <td>Current unmanned aerial vehicle (UAV) visual tracking algorithms are primari...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/automatic-failure-recovery-and-re</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Visual tracking', 'scheme': None, 'label': None}]</td>\n",
       "      <td>Current unmanned aerial vehicle (UAV) visual tracking algorithms are primari...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vision Meets Wireless Positioning: Effective Person Re-identification with R...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/vision-meets-wireless-positioning-effective</td>\n",
       "      <td>Existing person re-identification methods rely on the visual sensor to captu...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/vision-meets-wireless-positioning-effective</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Person re-identification', 'scheme': None, 'label': None}]</td>\n",
       "      <td>Existing person re-identification methods rely on the visual sensor to captu...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T-GD: Transferable GAN-generated Images Detection Framework</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/t-gd-transferable-gan-generated-images</td>\n",
       "      <td>First, we train the teacher model on the source dataset and use it as a star...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...</td>\n",
       "      <td>https://paperswithcode.com/paper/t-gd-transferable-gan-generated-images</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First, we train the teacher model on the source dataset and use it as a star...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             title  \\\n",
       "0  Norm-in-Norm Loss with Faster Convergence and Better Performance for Image Q...   \n",
       "1                      Prototype Mixture Models for Few-shot Semantic Segmentation   \n",
       "2  Automatic Failure Recovery and Re-Initialization for Online UAV Tracking wit...   \n",
       "3  Vision Meets Wireless Positioning: Effective Person Re-identification with R...   \n",
       "4                      T-GD: Transferable GAN-generated Images Detection Framework   \n",
       "\n",
       "                                                                      title_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....   \n",
       "1  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....   \n",
       "2  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....   \n",
       "3  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....   \n",
       "4  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds....   \n",
       "\n",
       "                                                                             links  \\\n",
       "0  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...   \n",
       "1  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...   \n",
       "2  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...   \n",
       "3  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...   \n",
       "4  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.c...   \n",
       "\n",
       "                                                                             link  \\\n",
       "0  https://paperswithcode.com/paper/norm-in-norm-loss-with-faster-convergence-and   \n",
       "1          https://paperswithcode.com/paper/prototype-mixture-models-for-few-shot   \n",
       "2              https://paperswithcode.com/paper/automatic-failure-recovery-and-re   \n",
       "3    https://paperswithcode.com/paper/vision-meets-wireless-positioning-effective   \n",
       "4         https://paperswithcode.com/paper/t-gd-transferable-gan-generated-images   \n",
       "\n",
       "                                                                           summary  \\\n",
       "0  Experiments on two relevant datasets (KonIQ-10k and CLIVE) show that, compar...   \n",
       "1  Few-shot segmentation is challenging because objects within the support and ...   \n",
       "2  Current unmanned aerial vehicle (UAV) visual tracking algorithms are primari...   \n",
       "3  Existing person re-identification methods rely on the visual sensor to captu...   \n",
       "4  First, we train the teacher model on the source dataset and use it as a star...   \n",
       "\n",
       "                                                                    summary_detail  \\\n",
       "0  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...   \n",
       "1  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...   \n",
       "2  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...   \n",
       "3  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...   \n",
       "4  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.c...   \n",
       "\n",
       "                                                                               id  \\\n",
       "0  https://paperswithcode.com/paper/norm-in-norm-loss-with-faster-convergence-and   \n",
       "1          https://paperswithcode.com/paper/prototype-mixture-models-for-few-shot   \n",
       "2              https://paperswithcode.com/paper/automatic-failure-recovery-and-re   \n",
       "3    https://paperswithcode.com/paper/vision-meets-wireless-positioning-effective   \n",
       "4         https://paperswithcode.com/paper/t-gd-transferable-gan-generated-images   \n",
       "\n",
       "  guidislink  \\\n",
       "0      False   \n",
       "1      False   \n",
       "2      False   \n",
       "3      False   \n",
       "4      False   \n",
       "\n",
       "                                                                              tags  \\\n",
       "0            [{'term': 'Image quality assessment', 'scheme': None, 'label': None}]   \n",
       "1  [{'term': 'Few-shot semantic segmentation', 'scheme': None, 'label': None}, ...   \n",
       "2                     [{'term': 'Visual tracking', 'scheme': None, 'label': None}]   \n",
       "3            [{'term': 'Person re-identification', 'scheme': None, 'label': None}]   \n",
       "4                                                                              NaN   \n",
       "\n",
       "                                                                              text  \\\n",
       "0  Experiments on two relevant datasets (KonIQ-10k and CLIVE) show that, compar...   \n",
       "1  Few-shot segmentation is challenging because objects within the support and ...   \n",
       "2  Current unmanned aerial vehicle (UAV) visual tracking algorithms are primari...   \n",
       "3  Existing person re-identification methods rely on the visual sensor to captu...   \n",
       "4  First, we train the teacher model on the source dataset and use it as a star...   \n",
       "\n",
       "   ... published_parsed comments authors author author_detail updated  \\\n",
       "0  ...              NaN      NaN     NaN    NaN           NaN     NaN   \n",
       "1  ...              NaN      NaN     NaN    NaN           NaN     NaN   \n",
       "2  ...              NaN      NaN     NaN    NaN           NaN     NaN   \n",
       "3  ...              NaN      NaN     NaN    NaN           NaN     NaN   \n",
       "4  ...              NaN      NaN     NaN    NaN           NaN     NaN   \n",
       "\n",
       "  updated_parsed content href media_thumbnail  \n",
       "0            NaN     NaN  NaN             NaN  \n",
       "1            NaN     NaN  NaN             NaN  \n",
       "2            NaN     NaN  NaN             NaN  \n",
       "3            NaN     NaN  NaN             NaN  \n",
       "4            NaN     NaN  NaN             NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RegexpTokenizer.tokenize of WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def setup_document_store_with_retriever(model_name, df, text_col='text', max_document_length=256, use_gpu=use_gpu, quantize_model=True):\n",
    "    embedding_col = text_col + '_emb'\n",
    "    document_store = InMemoryDocumentStore(\n",
    "        embedding_field=embedding_col,\n",
    "    )\n",
    "    retriever = EmbeddingRetriever(\n",
    "        document_store=document_store,\n",
    "        embedding_model=model_name,\n",
    "        use_gpu=use_gpu)\n",
    "    \n",
    "\n",
    "    if not use_gpu and quantize_model:\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            retriever.embedding_model.model,\n",
    "            {torch.nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "\n",
    "    article_embeddings = retriever.embed_queries(\n",
    "        texts=[\n",
    "            ' '.join(tokenize.wordpunct_tokenize(text)[:max_document_length])\n",
    "            for text in df[text_col] \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df[embedding_col] = article_embeddings\n",
    "    document_store.write_documents(df.to_dict(orient='records'))\n",
    "    return document_store, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_texts = feed_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 18:29:02 - INFO - haystack.retriever.dense -   Init retriever using embeddings of model deepset/sentence_bert\n",
      "08/13/2020 18:29:02 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "08/13/2020 18:29:02 - INFO - farm.infer -   Could not find `deepset/sentence_bert` locally. Try to download from model hub ...\n",
      "08/13/2020 18:29:04 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "08/13/2020 18:29:10 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "Inferencing Samples: 100%|██████████| 74/74 [00:09<00:00,  7.46 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepset/sentence_bert\"\n",
    "document_store, retriever = setup_document_store_with_retriever(model_name, feed_df, 'text', use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Few-shot segmentation is challenging because objects within the support and query images could significantly differ in appearance and pose. Code: https://github.com/Yang-Bob/PMMs'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_df.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def doc_to_dict(doc):\n",
    "    d = {}\n",
    "    d['text'] = doc.text\n",
    "    d['title'] = doc.meta['title']\n",
    "    d['score'] = doc.query_score\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_strings = pd.read_table('data/topics.txt', header=None).iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning\n",
      "natural language processing\n",
      "computer vision and image processing\n",
      "statistics\n",
      "implementation\n",
      "visualization\n",
      "industry\n",
      "software engineering\n",
      "reddit question\n",
      "arxiv\n",
      "cloud computing\n",
      "deployment\n",
      "competitions\n",
      "business\n",
      "business intelligence\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(topic_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_query_strings = [\n",
    "    'text is about {}'.format(topic)\n",
    "    for topic in topic_strings\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 15.79 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "raw_results = retriever.retrieve(\n",
    "    topic_query_strings[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_scored_df(raw_results, topic_strings):\n",
    "    topic_query_strings = [\n",
    "        'text is about {}'.format(topic)\n",
    "        for topic in topic_strings\n",
    "    ]\n",
    "    \n",
    "    results = [\n",
    "        doc_to_dict(doc)\n",
    "        for doc in raw_results \n",
    "    ]\n",
    "    result_embeddings = np.array([\n",
    "        doc.meta['text_emb']\n",
    "        for doc in raw_results\n",
    "    ]).astype('float32')\n",
    "    topic_query_embeddings = np.array(retriever.embed_passages(\n",
    "        list(topic_strings)\n",
    "    )).astype('float32')\n",
    "    \n",
    "    scores_df = pd.DataFrame({})\n",
    "    scores_df['title'] = list(map(itemgetter('title'), results))\n",
    "    scores_df['text'] = list(map(itemgetter('text'), results))\n",
    "\n",
    "    scores = pd.DataFrame(metrics.pairwise.cosine_similarity(\n",
    "        result_embeddings,\n",
    "        topic_query_embeddings\n",
    "    ))\n",
    "    scores.columns = topic_strings\n",
    "\n",
    "    scores_df = pd.concat(\n",
    "        [scores_df, scores],\n",
    "        axis=1\n",
    "    )\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 4/4 [00:00<00:00,  7.12 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "scores_df = get_scored_df(raw_results, topic_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(scores_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col2 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col3 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col4 {\n",
       "            background-color:  #a4dba4;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col5 {\n",
       "            background-color:  #b8e6b8;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col6 {\n",
       "            background-color:  #c8efc8;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col7 {\n",
       "            background-color:  #a8dda8;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col8 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col9 {\n",
       "            background-color:  #329c32;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col10 {\n",
       "            background-color:  #4fac4f;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col11 {\n",
       "            background-color:  #87cb87;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col12 {\n",
       "            background-color:  #d2f4d2;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col13 {\n",
       "            background-color:  #b2e3b2;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col14 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col15 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col16 {\n",
       "            background-color:  #aadeaa;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col2 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col3 {\n",
       "            background-color:  #76c176;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col4 {\n",
       "            background-color:  #7ac47a;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col5 {\n",
       "            background-color:  #a8dda8;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col6 {\n",
       "            background-color:  #399f39;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col7 {\n",
       "            background-color:  #67b967;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col8 {\n",
       "            background-color:  #219221;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col9 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col10 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col11 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col12 {\n",
       "            background-color:  #88cb88;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col13 {\n",
       "            background-color:  #7fc67f;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col14 {\n",
       "            background-color:  #2f9a2f;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col15 {\n",
       "            background-color:  #1a8e1a;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col16 {\n",
       "            background-color:  #7dc57d;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col2 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col3 {\n",
       "            background-color:  #76c176;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col4 {\n",
       "            background-color:  #7ac47a;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col5 {\n",
       "            background-color:  #a8dda8;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col6 {\n",
       "            background-color:  #399f39;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col7 {\n",
       "            background-color:  #67b967;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col8 {\n",
       "            background-color:  #219221;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col9 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col10 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col11 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col12 {\n",
       "            background-color:  #88cb88;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col13 {\n",
       "            background-color:  #7fc67f;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col14 {\n",
       "            background-color:  #2f9a2f;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col15 {\n",
       "            background-color:  #1a8e1a;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col16 {\n",
       "            background-color:  #7dc57d;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col2 {\n",
       "            background-color:  #1f911f;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col3 {\n",
       "            background-color:  #75c175;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col4 {\n",
       "            background-color:  #96d396;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col5 {\n",
       "            background-color:  #a6dca6;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col6 {\n",
       "            background-color:  #0e880e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col7 {\n",
       "            background-color:  #64b764;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col8 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col9 {\n",
       "            background-color:  #028102;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col10 {\n",
       "            background-color:  #acdfac;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col11 {\n",
       "            background-color:  #bee9be;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col12 {\n",
       "            background-color:  #9ed89e;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col13 {\n",
       "            background-color:  #6cbc6c;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col14 {\n",
       "            background-color:  #41a441;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col15 {\n",
       "            background-color:  #108910;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col16 {\n",
       "            background-color:  #87cb87;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col2 {\n",
       "            background-color:  #a6dca6;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col3 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col4 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col5 {\n",
       "            background-color:  #d1f4d1;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col6 {\n",
       "            background-color:  #ddfbdd;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col7 {\n",
       "            background-color:  #caf0ca;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col8 {\n",
       "            background-color:  #bee9be;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col9 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col10 {\n",
       "            background-color:  #aadeaa;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col11 {\n",
       "            background-color:  #76c176;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col12 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col13 {\n",
       "            background-color:  #b9e7b9;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col14 {\n",
       "            background-color:  #aee0ae;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col15 {\n",
       "            background-color:  #69ba69;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col16 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col2 {\n",
       "            background-color:  #a6dca6;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col3 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col4 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col5 {\n",
       "            background-color:  #d1f4d1;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col6 {\n",
       "            background-color:  #ddfbdd;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col7 {\n",
       "            background-color:  #caf0ca;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col8 {\n",
       "            background-color:  #bee9be;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col9 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col10 {\n",
       "            background-color:  #aadeaa;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col11 {\n",
       "            background-color:  #76c176;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col12 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col13 {\n",
       "            background-color:  #b9e7b9;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col14 {\n",
       "            background-color:  #aee0ae;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col15 {\n",
       "            background-color:  #69ba69;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col16 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col2 {\n",
       "            background-color:  #a9dea9;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col3 {\n",
       "            background-color:  #70be70;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col4 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col5 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col6 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col7 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col8 {\n",
       "            background-color:  #058205;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col9 {\n",
       "            background-color:  #078407;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col10 {\n",
       "            background-color:  #219221;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col11 {\n",
       "            background-color:  #2d992d;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col12 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col13 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col14 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col15 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col16 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col2 {\n",
       "            background-color:  #a9dea9;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col3 {\n",
       "            background-color:  #70be70;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col4 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col5 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col6 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col7 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col8 {\n",
       "            background-color:  #058205;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col9 {\n",
       "            background-color:  #078407;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col10 {\n",
       "            background-color:  #219221;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col11 {\n",
       "            background-color:  #2d992d;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col12 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col13 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col14 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col15 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col16 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col2 {\n",
       "            background-color:  #a2daa2;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col3 {\n",
       "            background-color:  #6ebd6e;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col4 {\n",
       "            background-color:  #0e870e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col5 {\n",
       "            background-color:  #6ebd6e;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col6 {\n",
       "            background-color:  #44a644;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col7 {\n",
       "            background-color:  #0c860c;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col8 {\n",
       "            background-color:  #57b057;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col9 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col10 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col11 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col12 {\n",
       "            background-color:  #b5e4b5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col13 {\n",
       "            background-color:  #66b866;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col14 {\n",
       "            background-color:  #2e992e;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col15 {\n",
       "            background-color:  #b8e6b8;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col16 {\n",
       "            background-color:  #66b866;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col2 {\n",
       "            background-color:  #caf0ca;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col3 {\n",
       "            background-color:  #7ac37a;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col4 {\n",
       "            background-color:  #bee9be;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col5 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col6 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col7 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col8 {\n",
       "            background-color:  #d5f6d5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col9 {\n",
       "            background-color:  #369e36;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col10 {\n",
       "            background-color:  #4daa4d;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col11 {\n",
       "            background-color:  #5cb35c;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col12 {\n",
       "            background-color:  #9ed89e;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col13 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col14 {\n",
       "            background-color:  #a2daa2;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col15 {\n",
       "            background-color:  #7cc57c;\n",
       "            color:  #000000;\n",
       "        }    #T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col16 {\n",
       "            background-color:  #75c175;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >title</th>        <th class=\"col_heading level0 col1\" >text</th>        <th class=\"col_heading level0 col2\" >deep learning</th>        <th class=\"col_heading level0 col3\" >natural language processing</th>        <th class=\"col_heading level0 col4\" >computer vision and image processing</th>        <th class=\"col_heading level0 col5\" >statistics</th>        <th class=\"col_heading level0 col6\" >implementation</th>        <th class=\"col_heading level0 col7\" >visualization</th>        <th class=\"col_heading level0 col8\" >industry</th>        <th class=\"col_heading level0 col9\" >software engineering</th>        <th class=\"col_heading level0 col10\" >reddit question</th>        <th class=\"col_heading level0 col11\" >arxiv</th>        <th class=\"col_heading level0 col12\" >cloud computing</th>        <th class=\"col_heading level0 col13\" >deployment</th>        <th class=\"col_heading level0 col14\" >competitions</th>        <th class=\"col_heading level0 col15\" >business</th>        <th class=\"col_heading level0 col16\" >business intelligence</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col0\" class=\"data row0 col0\" >Using Julia to Do Whole Word Masking</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col1\" class=\"data row0 col1\" >Syntax almost as friendly as Python, while running up to 100x fasterPhoto Credit(This post was originally published on my personal blog.)IntroductionIn my last post, [Failure Report] Distill Fine-tuned Transformers into Recurrent Neural Networks, I tried to distill the knowledge of a fine-tuned BERT model into an LSTM or GRU model without any data augmentation and failed to achieve satisfiable results. In the follow-up works, I tried to replicate the easies-to-implement augmentation method — masking — used in [1] and see its effect. The masking described in [1] is called “whole word masking” [2], that is, masking the whole word instead of just masking a single word piece.It is non-trivial to implement whole word masking, as it would require the sampling process to be aware of which word piece is itself a whole word, and which is part of a word. As you may know, doing text processing in pure Python is quite slow comparing to other compiled languages. I recently picked up the Julia programming language, which promises the flexibility of scripting languages and the speed of compiled languages, and thought that it was a good opportunity to test Julia in the field.This post describes the Julia code I wrote for this task and shows that for this specific task the Julia code is as simple to write as Python, while runs up to 100x faster than its pure Python counterpart.The AlgorithmThis is the algorithm I used to do whole word masking (given that the examples are already tokenized to word pieces):For each example, mark all the word pieces that are either a whole word or the first piece of a word (by using a mask).Randomly sample N marked pieces for each example (N is a hyper-parameter).Replacing the selected pieces with “[MASK]“.Check if the next piece is a part of this word (tokens start with “##” in BERT tokenizer). If so, also replace it with “[MASK]“.Repeat step 4 until the condition is false or the end of the example is reached.BenchmarksNotebook used in this section:PythonJuliaSummary(Comparing the mean run time here as the %timeit magic doesn’t provide the median run time.)Tokenizing examples:15 seconds (shared by both Python and Julia Pipeline)Adding Special Tokens:Python: 42 ms (estimated)Julia: 41 msMarking First PiecesPython: 326 msJulia: 47 ms (single-threaded)Julia: 39 ms (multi-threaded)Sample One Word to MaskPython: 8.2 s (using Numpy.random.choice)Julia: 69 msMaskingPython: 725 ms (copying the examples)Julia: 426 ms (copying the examples)Python: 300 ms (estimated)Julia: 10 msRemarksThe most time-consuming part is tokenizing the examples. So in reality optimizing the tokenizer has the most potential (That’s why huggingface has re-implemented the word-piece tokenizers in Rust).But the eight seconds saved on sampling by switching to Julia is also a significant improvement, and just took a few lines to implement.Copying the examples takes around 300 to 500 ms, and is the most expensive operation besides tokenization. So try to avoid it if possible. (If you need the augment the same dataset multiple times, you have no choice to copy the examples.)Adding Special TokensA simple operation that adds “[CLS]” to the head and “[SEP]” to the tail. Python and Julia are equally fast in this one.Pythondef add_special_tokens(sentence):    sentence.insert(0, \"[CLS]\")    sentence.append(\"[SEP]\")tmp = deepcopy(sentences)for sentence in tmp:    add_special_tokens(sentence)Juliafunction add_special_tokens!(sentence)    pushfirst!(sentence, \"[CLS]\")    push!(sentence, \"[SEP]\")endtmp = deepcopy(sentences)results = add_special_tokens!.(tmp)Marking First PiecesCreate binary masks to filter out word piece that is not the first word piece of a word. Julia is starting to outperform Python.Pythondef is_first_piece(tokens):    return [not token.startswith(\"##\") for token in tokens]first_piece_masks = [is_first_piece(sent) for sent in sentences]JuliaVectorized (single-thread) version:function is_first_piece(arr::Array{String,1})    return .!startswith.(arr, \"##\")endresults = is_first_piece.(sentences)A multi-thread version is also provided, which can sometimes be faster depending on your hardware:results = [Bool[] for _ in 1:length(sentences)]Threads.@threads for i in 1:length(sentences)   results[i] = is_first_piece(sentences[i])endSamplingRandomly sample one word from each example to be masked. Since I can’t think of any simple way to vectorized this in Python, a naive for-loop approach is used. Vectorizing in Julia, on the other hand, is fairly straight-forward. As a result, the Julia version is vastly faster (100x) than the Python one.Note: I used Numpy in the Python implementation, so it’s not really “pure python” in this case.Pythondef sample(first_piece_masks, n=1):    results = []    for mask in first_piece_masks:        if sum(mask) <= n:            results.append([])            continue        probabilities = np.asarray(mask) / float(sum(mask))        results.append(np.random.choice(np.arange(len(mask)), size=n, p=probabilities))    return resultsmasking_points =  sample(first_piece_masks)Juliausing StatsBasefunction sample_mask_position(first_piece_mask, n=1)    if sum(first_piece_mask) <= n        return Int64[]    end    return sample(1:length(first_piece_mask), Weights(first_piece_mask), n, replace=false)endmasking_points = sample_mask_position.(first_piece_masks)MaskingFull word masking. This one inevitably has to use some loop to scan the example. For loops are not a problem for Julia, so the Julia version is much faster (30x) than Python.The implementation presented here copies the examples inside the function so the original examples can be augmented multiple times.Pythondef masking(rows, first_piece_masks, masking_points):    augmented_rows = deepcopy(rows)    for idx in range(len(masking_points)):        for pos in masking_points[idx]:            augmented_rows[idx][pos] = \"[MASK]\"            while pos +1 < len(first_piece_masks[idx]) and first_piece_masks[idx][pos + 1] == 0:                pos += 1                augmented_rows[idx][pos] = \"[MASK]\"    return augmented_rowsaugmented_sentences = masking(sentences, first_piece_masks, masking_points)Juliafunction masking(rows::Vector{Vector{String}}, first_piece_masks::Vector{Vector{Bool}}, masking_points::Vector{Vector{Int64}})    augmented_rows = deepcopy(rows)    for idx in 1:length(masking_points)        for pos in masking_points[idx]            augmented_rows[idx][pos] = \"[MASK]\"            while pos + 1 <= length(first_piece_masks[idx]) && first_piece_masks[idx][pos + 1] == 0                pos += 1                augmented_rows[idx][pos] = \"[MASK]\"            end        end    end    return augmented_rowsendaugmented_sentences = masking(sentences, first_piece_masks, masking_points)ConclusionThis is the first time I integrate Julia in an NLP pipeline, and the results are encouraging. The easy of development of Julia is on the same level as Python, but the is on a totally different level. In this example, the most improvement in speed comes from the sampling process, but it only represents less than 40 % of the total run time. And the total run time in Python is relatively short. I look forward to seeing what kind of speedup Julia can bring in bigger datasets or more complicated tasks.(The notebook actually used in the pipeline).ReferencesTang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., & Lin, J. (2019). Distilling Task-Specific Knowledge from BERT into Simple Neural Networks.BERT: New May 31st, 2019: Whole Word Masking ModelsUsing Julia to Do Whole Word Masking was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col2\" class=\"data row0 col2\" >0.158500</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col3\" class=\"data row0 col3\" >0.534408</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col4\" class=\"data row0 col4\" >0.213675</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col5\" class=\"data row0 col5\" >0.230116</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col6\" class=\"data row0 col6\" >0.290797</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col7\" class=\"data row0 col7\" >0.279370</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col8\" class=\"data row0 col8\" >0.172413</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col9\" class=\"data row0 col9\" >0.300122</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col10\" class=\"data row0 col10\" >0.206606</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col11\" class=\"data row0 col11\" >0.126351</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col12\" class=\"data row0 col12\" >0.061703</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col13\" class=\"data row0 col13\" >0.169138</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col14\" class=\"data row0 col14\" >0.027748</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col15\" class=\"data row0 col15\" >0.111149</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row0_col16\" class=\"data row0 col16\" >0.173553</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col0\" class=\"data row1 col0\" >Component Divide-and-Conquer for Real-World Image Super-Resolution</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col1\" class=\"data row1 col1\" >Learning an SR model with conventional pixel-wise loss usually is easily dominated by flat regions and edges, and fails to infer realistic details of complex textures. Code: https://github.com/xiezw5/Component-Divide-and-Conquer-for-Real-World-Image-Super-Resolution</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col2\" class=\"data row1 col2\" >0.829934</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col3\" class=\"data row1 col3\" >0.386449</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col4\" class=\"data row1 col4\" >0.283810</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col5\" class=\"data row1 col5\" >0.266542</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col6\" class=\"data row1 col6\" >0.505949</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col7\" class=\"data row1 col7\" >0.396436</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col8\" class=\"data row1 col8\" >0.321342</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col9\" class=\"data row1 col9\" >0.362384</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col10\" class=\"data row1 col10\" >0.059275</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col11\" class=\"data row1 col11\" >0.011338</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col12\" class=\"data row1 col12\" >0.151264</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col13\" class=\"data row1 col13\" >0.244933</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col14\" class=\"data row1 col14\" >0.274586</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col15\" class=\"data row1 col15\" >0.237258</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row1_col16\" class=\"data row1 col16\" >0.226929</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col0\" class=\"data row2 col0\" >Transfer Learning with KERAS</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col1\" class=\"data row2 col1\" >Learning is a never-ending process, but it’s more important to use previously gained knowledge in a new experiment.Continue reading on Towards AI — Multidisciplinary Science Journal »</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col2\" class=\"data row2 col2\" >0.829934</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col3\" class=\"data row2 col3\" >0.386449</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col4\" class=\"data row2 col4\" >0.283810</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col5\" class=\"data row2 col5\" >0.266542</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col6\" class=\"data row2 col6\" >0.505949</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col7\" class=\"data row2 col7\" >0.396436</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col8\" class=\"data row2 col8\" >0.321342</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col9\" class=\"data row2 col9\" >0.362384</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col10\" class=\"data row2 col10\" >0.059275</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col11\" class=\"data row2 col11\" >0.011338</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col12\" class=\"data row2 col12\" >0.151264</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col13\" class=\"data row2 col13\" >0.244933</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col14\" class=\"data row2 col14\" >0.274586</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col15\" class=\"data row2 col15\" >0.237258</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row2_col16\" class=\"data row2 col16\" >0.226929</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col0\" class=\"data row3 col0\" >How PyTorch Lightning became the first ML framework to runs continuous integration on TPUs</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col1\" class=\"data row3 col1\" >Learn how PyTorch Lightning added CI tests on TPUsContinue reading on PyTorch »</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col2\" class=\"data row3 col2\" >0.736946</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col3\" class=\"data row3 col3\" >0.386888</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col4\" class=\"data row3 col4\" >0.236468</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col5\" class=\"data row3 col5\" >0.272225</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col6\" class=\"data row3 col6\" >0.569504</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col7\" class=\"data row3 col7\" >0.402101</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col8\" class=\"data row3 col8\" >0.346875</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col9\" class=\"data row3 col9\" >0.359938</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col10\" class=\"data row3 col10\" >0.115612</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col11\" class=\"data row3 col11\" >0.059553</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col12\" class=\"data row3 col12\" >0.124438</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col13\" class=\"data row3 col13\" >0.271960</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col14\" class=\"data row3 col14\" >0.250051</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col15\" class=\"data row3 col15\" >0.242955</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row3_col16\" class=\"data row3 col16\" >0.215216</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col0\" class=\"data row4 col0\" >Textual Description for Mathematical Equations</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col1\" class=\"data row4 col1\" >Reading of mathematical expression or equation in the document images is very challenging due to the large variability of mathematical symbols and expressions. Code: https://github.com/ajoymondal/Equation-Description-PyTorch</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col2\" class=\"data row4 col2\" >0.346423</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col3\" class=\"data row4 col3\" >0.245931</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col4\" class=\"data row4 col4\" >0.099521</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col5\" class=\"data row4 col5\" >0.170653</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col6\" class=\"data row4 col6\" >0.257660</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col7\" class=\"data row4 col7\" >0.219150</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col8\" class=\"data row4 col8\" >0.202658</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col9\" class=\"data row4 col9\" >0.076608</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col10\" class=\"data row4 col10\" >0.117804</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col11\" class=\"data row4 col11\" >0.147765</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col12\" class=\"data row4 col12\" >0.036884</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col13\" class=\"data row4 col13\" >0.157269</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col14\" class=\"data row4 col14\" >0.103631</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col15\" class=\"data row4 col15\" >0.188191</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row4_col16\" class=\"data row4 col16\" >0.102440</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col0\" class=\"data row5 col0\" >[Notes] “Statistical Inference Enables Bad Science; Statistical Thinking Enables Good Science”</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col1\" class=\"data row5 col1\" >Reading the Article by Christopher Tong on The American Statistician Volume 73, 2019Photo Credit(This is a republication of this post on my personal blog.)This article by Christopher Tong has got a lot of love from people I followed on Twitter, so I decided to read it. It was very enlightening. But to be honest, I don’t fully understand quite a few arguments made by this article, probably because I lack the experience of more rigorous scientific experiments and research. Nonetheless, I think writing down the parts I find interesting and put it into a blog post would be beneficial for myself and other potential readers. Hopefully, it makes it easier to reflect on these materials later.This article argues that instead of relying on the statistical inference on an isolated study, we should use guide scientific research of all kinds by statistical thinking, and validate claims by replicating and predicting finds in new data and new settings.Replicating and predicting findings in new data and new settings is a stronger way of validating claims than blessing results from an isolated study with statistical inferences.Let’s see the reasoning behind this claim.IntroductionFirst, Tong makes clear what “statistical inferences” are:Statistical inferences are claims made using probability models of data generating processes, intended to characterize unknown features of the population(s) or process(es) from which data are thought to be sampled. Examples include estimates of parameters such as the population mean (often attended by confidence intervals), hypothesis test results (such as p-values), and posterior probabilities.Some of the widely used tools in statistical inference has come under fire recently for being misused or abused (as in The ASA’s Statement on p-Values: Context, Process, and Purpose)Among these criticisms, McShane and Gelman (2017) succinctly stated that null hypothesis testing “was supposed to protect researchers from over-interpreting noisy data. Now it has the opposite effect.”Tong tries to distinguish exploratory and confirmatory objectives of a study. He argues that most scientific research tends to be exploratory and flexible, but the statistical inference is only suitable in a confirmatory setting where study protocol and statistical model are fully prespecified.We shall argue that these issues stem largely from the Optimism Principle (Picard and Cook 1984)that is an inevitable byproduct of the necessarily flexible data analysis and modeling work that attends most scientific research.And the lack of this distinction in the current use of inferential methods in science has enabled biased statistical inference and encouraged a Cult of the Isolated Study that short-circuits the iterative nature of research.Statistical Inference and the Optimism PrincipleAs Efron and Hastie stated in their new book “Computer-Age Statistical Inference: Algorithms, Evidence, and Data Science”:It is a surprising, and crucial, aspect of statistical theory that the same data that supplies an estimate can also assess its accuracy.I had similar doubts when receiving traditional statistics education. The bias and variance tradeoff is mentioned but it is generally up to use to decide where to draw the line. The cross-validation is clearly a more principled and objective approach. (See the Breiman’s classic paper “Statistical Modeling: The Two Culture”)As Harrell, F. E., Jr. (2015) observed:Using the data to guide the data analysis is almost as dangerous as not doing so.Essentially, when researchers devise their analysis approach based on the data, it creates a chance to overfit the data. Simmons, Nelson, and Simonsohn (2011) called these opportunities researcher degrees of freedom, and when abused to fish for publishable p-values, p-hacking.The resulting inferences from the final model tend to be biased, with uncertainties underestimated, and statistical significance overestimated, a phenomenon dubbed the Optimism Principle by Picard andCook (1984).In extreme cases, nonsense data can still seem to make sense.In other words, it is possible to obtain a seemingly informative linear model, with decent R² and several statistically significant predictor variables, from data that is utter nonsense. This finding was later dubbed “Freedman’s paradox” (Raftery, Madigan, and Hoeting 1993).This kind of bias would lead to an underestimation of the uncertainty because we picked the model that has fit the training data best.Chatfield (1995) used the term model selection bias to describe the distorted inferences that result when using the same data that determines the form of the final model to also produce inferences from that model.Exploratory and Confirmatory Objectives in Scientific ResearchThe obvious way to avoid the difficulties of overfitting and produce valid statistical inferences is to completely prespecify the study design and statistical analysis plan prior to the start of data collection.Tong uses the phased experimentation of medical clinical trials as an example of scientific research where exploratory/confirmatory distinction is clearly made.This framework helps to separate therapeutic exploratory (typically Phase II) with therapeutic confirmatory (typically Phase III) objectives.It doesn’t prevent the expensive clinical dataset to be used for further exploratory work — to generate hypotheses for further testing in later experiments.A succinct perspective on such inferences is given by Sir Richard Peto, often quoted (e.g., Freedman 1998) as saying “you should always do subgroup analysis and never believe the results.”And it doesn’t mean the result from exploratory studies shouldn’t be published.If the result is important and exciting, we want to publish exploratory studies, but at the same time make clear that they are generally statistically underpowered, and need to be reproduced.From the Cult of the Isolated Study to TriangulationThe treatment of statistical inferences from exploratory research as if they were confirmatory enables what Nelder (1986) called The Cult of the Isolated Study, so that The effects claimed may never be checked by painstaking reproduction of the study elsewhere, and when this absence of checking is combined with the possibility that the original results would not have been reported unless the effects could be presented as significant, the result is a procedure which hardly deserves the attribute ‘scientific.Simple replication is usually not sufficient. Tong uses the Wright Brothers as a demonstrative example.Munafo and Davey Smith (2018) define triangulation as “the strategic use of multiple approaches to address one question. Each approach has its own unrelated assumptions, strengths, and weaknesses. Results that agree across different methodologies are less likely to be artifacts.”The notorious example of the report by the OPERA collaboration shows the importance of triangulation to uncover systematic errors.A particular weakness of the Isolated Study is that systematic errors may contaminate an entire study but remain hidden if no further research is done.Technical Solutions and Their DeficienciesThe most widely known class of such methods is based on adjusting for multiple inferences. These range from the simple Bonferroni inequality to the modern methods of false discovery rate and false coverage rate (e.g., Dickhaus 2014).A second class of methods incorporates resistance to overfitting into the statistical modeling process, often through an optimization procedure that penalizes model complexity, an approach sometimes called regularization.Tong also indicates that random splitting is still not the perfect solution.Unfortunately, such procedures (or their variants) are still vulnerable to the Optimism Principle, because random splitting implies that “left-out” samples are similar to the “left-in” samples (Gunter and Tong 2017).So it’s better to collect more data to overcome model uncertainty:Obtaining “more than one set of data, whenever possible, is a potentially more convincing way of overcoming model uncertainty and is needed anyway to determine the range of conditions under which a model is valid” (Chatfield 1995).Tong also discussed another widely advocated solution — model averaging. Those who are familiar with Kaggle competitions should already have a firm grasp on this.Only through the iterative learning process, using multiple lines of evidence and many sets of data, can systematic error be discovered, and model refinement be continually guided by new data.More Thoughtful SolutionsOne strategy requires preregistering both the research hypotheses to be tested and the statistical analysis plan prior to data collection, much as in a late-stage clinical trial (e.g., Nosek et al. 2018).However, the fact that most scientific research cannot fit the above paradigm is a big problem. A more realistic approach is preregistered replication.A variation on this theme is preregistered replication, where a replication study, rather than the original study, is subject to strict preregistration (e.g., Gelman 2015). A broader vision of this idea (Mogil andMacleod 2017) is to carry out a whole series of exploratory experiments without any formal statistical inference, and summarize the results by descriptive statistics (including graphics) or even just disclosure of the raw data.Enabling Good ScienceTong adapts a taxonomy of statistical activity by Cox (1957) and Moore (1992):Data production. The planning and execution of a study (either observational or experimental).Descriptive and exploratory analysis. Study the data at hand.Generalization. Make claims about the world beyond the data at hand.The first step of statistical thinking is to understand the objective of the study, its context, and its constraints, so that planning for study design and analysis can be fit for purpose.Data ProductionFeller (1969) pronounced that “The purpose of statistics in laboratories should be to save labor, time, and expense by efficient experimental designs” rather than null hypothesis significance testing.Tong discusses a few experiment design techniques that should already be familiar to those who have taken formal statistics education. He also raises some practical concerns when conducting the experiment and its analysis.Data acquisition and storage systems should have appropriate resolution and reliability. (We once worked with an instrument that allowed the user to retrieve stored time series data with a choice of time-resolution. Upon investigation, we found that the system was artificially interpolating data, and reporting values not actually measured, if the user chose a high resolution.)And other research degrees of freedom that is related to decisions around experiment design:Other researcher degrees of freedom can affect study design and execution. An instructive example for the latter is the decision to terminate data collection. Except in clinical trials, where this decision is tightly regulated and accounted for in the subsequent analysis (e.g., Chow and Chang 2012), many researchers have no formal termination rule, stopping when funding is exhausted, lab priorities shift, apparent statistical significance is achieved (or becomes clearly hopeless), or for some other arbitrary reason, often involving unblinded interim looks at the data.Data DescriptionMoses (1992)warned us that Good statistical description is demanding and challenging work: it requires sound conceptualization, and demands insightfully organizing the data, and effectively communicating the results; not one of those tasks is easy. To mistakenly treat description as ‘routine’ is almost surely to botch the job.Theory of Description:Mallows (1983) provided an interesting perspective on a Theory of Description. He noted that “A good descriptive technique should be appropriate for its purpose; effective as a mode of communication, accurate, complete, and resistant.”Something like Tukey’s (1977) five number summary (the minimum, first quartile, median, third quartile, and maximum) can be helpful to describe the variability of the data.Though we might not quantify uncertainty using probability statements, we can attempt to convey the observed variability of the data at hand, while acknowledging that it does not fully capture uncertainty… However, the use of such data summaries is not free of assumptions (e.g., unimodality, in some cases symmetry), so they are descriptive only in relation to these assumptions, not in an absolute sense.Disciplined Data ExplorationAccording to Tukey (1973), exploratory analysis of the data is not “just descriptive statistics,” but rather an “actively incisive rather than passively descriptive” activity, “with a real emphasis on the discovery of the unexpected.”An example of how exploratory analysis may be essential for scientific inquiry is in the detection of and adjustment for batch effects.Leek et al. (2010) defined batch effects as “sub-groups of measurements that have qualitatively different behavior across conditions and are unrelated to the biological or scientific variables in a study.”Tong also cites the warning of Diaconis (1985) about the danger of undisciplined exploratory analysis.If such patterns are accepted as gospel without considering that they may have arisen by chance, he considers it magical thinking, which he defines as“our inclination to seek and interpret connections and events around us, together with our disinclination to revise belief after further observation.”Statistical ThinkingStatistical thinking begins with a relentless focus on fitness for purpose (paraphrasing Tukey 1962: seeking approximate answers to the right questions, not exact answers to the wrong ones), sound attitudes about data production and its pitfalls, and good habits of data display and disciplined data exploration.Statistical thinking also involves a keen awareness of the pitfalls of data analysis and its interpretation, including:The correlation versus causation fallacy.The distinction between interpolation and extrapolation.The distinction between experimental and observational data.Regression to the mean.Simpson’s paradox, and the ecological fallacy.The curse of dimensionalityDiscussionThere is no scientifically sound way to quantify uncertainty from a single set of data, in isolation from other sets of data comprising an exploratory/learning process. This brings to mind an observation made about certain research in materials science: “Even if the studies had reported an error value, the trustworthiness of the result would not depend on that value alone” (Wenmackers and Vanpouke 2012). By emphasizing principles of data production, data description, enlightened data display, disciplined data exploration, and exposing statistical pitfalls in interpretation, there is much that statisticians can do to ensure that statistics is “a catalyst to iterative scientific learning” (Box 1999).[Notes] “Statistical Inference Enables Bad Science; Statistical Thinking Enables Good Science” was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col2\" class=\"data row5 col2\" >0.346423</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col3\" class=\"data row5 col3\" >0.245931</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col4\" class=\"data row5 col4\" >0.099521</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col5\" class=\"data row5 col5\" >0.170653</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col6\" class=\"data row5 col6\" >0.257660</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col7\" class=\"data row5 col7\" >0.219150</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col8\" class=\"data row5 col8\" >0.202658</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col9\" class=\"data row5 col9\" >0.076608</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col10\" class=\"data row5 col10\" >0.117804</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col11\" class=\"data row5 col11\" >0.147765</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col12\" class=\"data row5 col12\" >0.036884</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col13\" class=\"data row5 col13\" >0.157269</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col14\" class=\"data row5 col14\" >0.103631</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col15\" class=\"data row5 col15\" >0.188191</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row5_col16\" class=\"data row5 col16\" >0.102440</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col0\" class=\"data row6 col0\" >A generic pipeline to make offline inferences</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col1\" class=\"data row6 col1\" >Data Science has been at the core of Doctrine.fr since the beginning. As such, a lot of effort has been put into developing data science models and industrializing them using pipelines tailored to provide quality answers to business questions. This article focuses on the industrialization part, as we detail how we are doing offline inferences using our models. For insights about our online inference system, you can refer to our article about a dedicated API for machine learning.More precisely, this article describes the generic logic we have developed in order to make the inference step straightforward such that we only have to focus on the specific processing for a given task.Technical stackBefore going into the details of our generic pipeline, here’s some context with a small summary of the technical stack used in our data science projects.These projects are coded in Python.Data is mainly queried from a PostgreSQL database (we are also using AWS S3 but less frequently).Different versions of a model are stored on AWS S3.Offline tasks are scheduled and run using Airflow.Global overview of the generic pipelineLet’s say we have a production-ready model which is trained to predict the structure of a decision (it predicts a label for each paragraph of a decision) and we want to apply it to our corpus of decisions. If you want to know how we trained this model, you can read our article about structuring legal document through deep learning.We are using the example of predicting classes on decisions, but it could be inference of any type of model on any type of content, which emphasizes the need for a truly generic data pipeline. This is how the generic logic works for this specific use case, steps 3 to 5 are parallelizable:Query decisions to be processed through the unique ID we have for each decision at DoctrineSeparate those IDs in batches of N decisions for multiprocessing purposesFetch the data needed for those ids (e.g. contents of the decision)Process each decision (its contents) and apply the modelInsert inferred results in the databaseEvery single one of the steps laid out above is implemented in a generic python class we have called Project.Generic pipeline in Project classThis class also includes other things like connectors to database, connectors to Elasticsearch, loaders for models on AWS S3 or management of asynchronous operations . Those technical components are of tremendous value and help us avoid fragmentation and duplication, but fall outside the scope of this article, so we will not go into further details about them.This is a simplified implementation of thisProject class:https://medium.com/media/d7a491a98a3a96431962f1ddc88880d3/hrefThe different steps of the pipeline are launched in the run method of Project. All new data science projects thus inherit from theProject class and overload its get_rows, process_element and insert_results methods. In the next section, we will also go into more detail for get_ids method, which is used to get the identifiers of documents to process, in this use case decisions. So, if we apply it here to our task of structure prediction on decisions, our data script looks like this:https://medium.com/media/af8810ead74c16c99588508ad7dce52c/hrefLet’s now dig a bit more into what each step of the pipeline is doing.Identify elements to processThe first step of our pipeline is to query identifiers of documents we want to process, which is done with get_ids method. In our example we're dealing with court decisions, so let's suppose we have the identifiers stored in a PostgreSQL table called decisions. This table also contains data like the content of the decision on which we want to apply our model of classification.The table decisions is defined with:https://medium.com/media/8f5d00a5a1823072ce57ebf6be7cc060/hrefThis first step highlights a first natural need: we have to store information about which decisions have been processed and when they have been processed.Storing this information has several purposes:It is used for debugging, knowing when a model has given a label for a decision can be very helpful if we detect errors in predictionsIt is used to only process new decisions and not those already processedIf the prediction script fails for any reason, we can rerun the script from the decisions where it stoppedHow do we store this information?We simply use a table in our PostgreSQL database to do it. We have a schema named operation_states to store tables of this type in our database. And this is how the structure of the table is defined:https://medium.com/media/aad1a6a7c472c8887b792a033b5fe503/hrefWe store the ID of the decision with the date of the first and the last time the model has made a prediction on it.How do we select the IDs for reprocessing?We need the first and last time we have made a prediction for a given decision because we can make predictions several times in the lifetime of a decision. There are two main reasons why we would make several structure predictions for a decision:Since a given decision is fetched from multiple external sources at Doctrine, it sometimes happens that the metadata or actual contents for a given decision get updated several times in its lifetime. When that happens, the model has to make a new prediction.A new, better model has been trained and we want to apply it again on decisions.The first and naive solution we have found to automatically reprocess decisions is to only process a fraction of them every day. But with this solution, in most of the cases, we are processing decisions which did not change since the last computation. It was a waste of time and resources.Hence we have thought of a more efficient solution. It is clear that we need to focus on modified content only. We need to store information about when a decision has been modified. For this purpose, we are still using a PostgreSQL table, which stores the last modification date about the decision (date of the content change, date of the metadata change etc…). That’s why we are storing those kind of tables in a schema called modification_states. The structure of the table looks like this:https://medium.com/media/1876796260677d5ec492e9a0ee0fcefd/hrefHow is the modification information updated?The modification information is obtained from the decision loading scripts. In those scripts, the content or metadata of a decision are compared to the existing ones using a hash function. If there should be a difference, we have rules to determine which content to keep, and if the kept content is new then the script updates the updated_at field of modification_states.decisions_modified_at table for this decision.Finally, the method get_ids takes as input decisions, operation_states.decisions_classified_atand modification_states.decisions_modified_attables in order to select decisions to be processed.Selection of IDs to be processedFor example, this is what the SQL query in get_ids looks like as we want to get the new decisions and the decisions which have been modified:https://medium.com/media/f21aa80e205ca8ab876ab56e55dc443a/hrefActually, in our Project class, we have implemented a generic get_ids function which takes input table names as arguments among other arguments. An interesting argument is stock, when True it selects the entire list of decisions, in the case we want to apply a new model on the whole corpus of decisions.In order to use these IDs, we split them into batches of N elements to be processed, because the subsequent steps are fully parallelizable. The parallelization is done using python’s multiprocessing package inside the Project.run method.Query data to be processedHaving a batch of decisions to process, through their ids, we now have to get useful information about the decisions. We want to predict a class for each paragraph of the decision, therefore we need to have the contents of the decision.The get_rows method is simply about querying this information for the chosen decisions from the previous step. It actually translates into simple SQL query:https://medium.com/media/1c261465729f18a91e25449fc67a050f/hrefWhere science happensWe have queried information required for the batch of decisions, we now need to apply the logic specific to the task of predicting a class for each paragraph of a decision. The logic of the task is implemented in the process_element method. This method takes one decision as input and includes preprocessing on the raw contents of the decision (lowercase, stemming, etc.), separation of the content into paragraphs and use of the model to make a prediction for each of them. Finally, the method returns predictions as outputs.In addition to the fact that we can parallelize several batches of decisions, we can also do it inside a single batch by doing asynchronous operations when we are requesting different services which can be done simultaneously (Database, ElasticSearch index, etc.). This is not presented in our simplified implementation of the Project class, but in practice it is based on the asyncio package. In our example, we could take advantage of asynchronous processings because the limiting resources could be the database or the available CPUs, however in practice it processes in reasonable time, such that we did not need to leverage the asynchronous part.Insert resultsFinally, having our predictions, the last step of our pipeline is to insert data in our database using the insert_results method.At this step, we are inserting several pieces of data:Results of the predictions in a defined tableProcessed IDs in the operation_states.decisions_classified_at table to store the information about which decisions have been processed and whenThe insert_results can also deal with data comparison before insertion. Typically, if we make a prediction for a decision because the content has changed for example, we compare the prediction against the previous one, and we only update data if the prediction has changed. It optimizes resource consumption, as aSELECT is more efficient than a DELETE followed by INSERT in PostgreSQL. The comparison can be done using some hash functions (PostgreSQL has an implementation of MD5 algorithm).ConclusionUsing this generic pipeline, we have removed a lot of the redundant code in the productionization of a model such that we only have to focus on some limited aspects. We took the example of inferring classes for decisions using a model, but this pipeline can deal with any kind of content and any kind of processing (inference using models, text processing etc.).Thanks to Pauline Chavallard, Bertrand Chardon and Nicolas Fiorini for their valuable feedbacks.A generic pipeline to make offline inferences was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col2\" class=\"data row6 col2\" >0.336029</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col3\" class=\"data row6 col3\" >0.392912</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col4\" class=\"data row6 col4\" >0.495046</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col5\" class=\"data row6 col5\" >0.664724</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col6\" class=\"data row6 col6\" >0.591172</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col7\" class=\"data row6 col7\" >0.582234</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col8\" class=\"data row6 col8\" >0.343187</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col9\" class=\"data row6 col9\" >0.352927</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col10\" class=\"data row6 col10\" >0.251432</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col11\" class=\"data row6 col11\" >0.235865</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col12\" class=\"data row6 col12\" >0.318167</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col13\" class=\"data row6 col13\" >0.432824</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col14\" class=\"data row6 col14\" >0.338490</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col15\" class=\"data row6 col15\" >0.253371</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row6_col16\" class=\"data row6 col16\" >0.375210</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col0\" class=\"data row7 col0\" >[D] Data Augmentation using Pre-Trained Transformers (BERT, GPT, etc) | Research Paper Walkthrough</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col1\" class=\"data row7 col1\" >Data augmentation is a widely used technique to increase the size of the training data. It helps in significatly increasing the diversity of data available for training models resulting in reducing over fitting and enhancing robustness of ML model, without actually collecting new data. In this video we will understand how we can use Transformers to do augmentation in NLP. 🔥  Check out at - https://youtu.be/9O9scQb4sNo Original Paper - https://arxiv.org/abs/2003.02245 Also check out, Easy Data Augmentation in NLP - https://youtu.be/-1unNLkwImw Feel free to share your thoughts 👍    submitted by    /u/prakhar21   [link] [comments]</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col2\" class=\"data row7 col2\" >0.336029</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col3\" class=\"data row7 col3\" >0.392912</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col4\" class=\"data row7 col4\" >0.495046</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col5\" class=\"data row7 col5\" >0.664724</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col6\" class=\"data row7 col6\" >0.591172</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col7\" class=\"data row7 col7\" >0.582234</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col8\" class=\"data row7 col8\" >0.343187</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col9\" class=\"data row7 col9\" >0.352927</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col10\" class=\"data row7 col10\" >0.251432</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col11\" class=\"data row7 col11\" >0.235865</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col12\" class=\"data row7 col12\" >0.318167</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col13\" class=\"data row7 col13\" >0.432824</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col14\" class=\"data row7 col14\" >0.338490</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col15\" class=\"data row7 col15\" >0.253371</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row7_col16\" class=\"data row7 col16\" >0.375210</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col0\" class=\"data row8 col0\" >Tensorflow Profiler with Custom Training Loop</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col1\" class=\"data row8 col1\" >Analyze and Optimize TensorFlow Performance on GPUPhoto Credit(This article was first published on my personal blog.)IntroductionThe Tensorflow Profiler in the upcoming Tensorflow 2.2 release is a much-welcomed addition to the ecosystem. For image-related tasks, often the bottleneck is the input pipeline. But you also don’t want to spend time optimizing the input pipeline unless it is necessary. The Tensorflow Profiler makes pinpointing the bottleneck of the training process much easier, so you can decide where the optimization effort should be put into.An Input-Bound Example. SourceThe official documentation demonstrates how to use the profiler with the Keras interface via a callback(tf.keras.callbacks.TensorBoard). However, there are no mentions of custom training loops. I did some research and came up with a working solution, which will be described in this post, along with some obstacles I had met and how I overcame them.PreparationInstall the Latest Tensorflow and the Profiler PluginThis comes directly from the documentation:# Uninstall twice to uninstall both the 1.15.0 and 2.1.0 version of TensorFlow and TensorBoard.pip uninstall -y -q tensorflow tensorboardpip uninstall -y -q tensorflow tensorboardpip install -U -q tf-nightly tb-nightly tensorboard_plugin_profile(This will no longer be required once Tensorflow and TensorBoard 2.2 are released)NVIDIA GPU Libraries(This section is for training on a single GPU. For training on multiple GPUs, please refer to this guide.)You’ll need to install NVIDIA GPU drivers and CUDA Toolkit as you normally do when training models on GPU.The next step is more specifically for the profiler. First, make sure that CUPTI 10.1 exists on the path (source):/sbin/ldconfig -N -v $(sed 's/:/ /g' <<< $LD_LIBRARY_PATH) | grep libcuptiIf not, update the LD_LIBRARY_PATH environment variable:export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATHTroubleshoot: CUPTI_ERROR_INSUFFICIENT_PRIVILEGESYou’ll likely see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES and CUPTI_ERROR_INVALID_PARAMETER errors in the log when trying to profile your model. This is because NVIDIA GPU performance counters, when running on one of the newer drivers, is only available to system administrators.Please read this document from NVIDIA to find a solution to your system.For my Linux system, the recommended modprobe nvidia NVreg_RestrictProfilingToAdminUsers=0 does not work. An alternative solution, which writes a file to /etc/modprobe.d, works for me. It is also offered in this Github thread:Adding options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\" to /etc/modprobe.d/nvidia-kernel-common.conf and reboot should resolve the permission issue.Profile the Training LoopThis guide(Profile Tensorflow performance) describes four ways to collect performance data. One of them is specific to Keras interface. Another one(sampling mode) is interactive through Tensorboard web UI. I’ll describe the two that works programmatically and are compatible with custom training loops.Using tf.profiler Function API:tf.profiler.experimental.start('logdir')# Train the model heretf.profiler.experimental.stop()2. Using Context Manager:with tf.profiler.experimental.Profile('logdir'):    # Train the model here    passThere is one additional way. By reading the source code of the Keras Tensorboard callback, I reconstructed the Tensorflow Profiler part in the callback as:from tensorflow.python.profiler import profiler_v2 as profilerprofiler.warmup()profiler.start(logdir='logdir')# Train the model hereprofiler.stop()A Working ExampleHere is an example that trains an Efficientnet-B3 model and collect performance data using two different ways(with no obvious differences in results):overview_pageNotice that the Device Compute Precisions indicates that 87.6% of the GPU time was spent in 16-bit computation, showing that the mixed-precision training is configured correctly. Judging from the graph, the GPU is well fed with basically no time spent on waiting for input (I enabled prefetch in the data pipeline, so this tells us that it hadn’t run out of the prefetched batches).The input_pipeline_analyzer page shows that most time on host(CPU side) is spent on data preprocessing, so disk IO doesn’t seem to be a problem:input_pipeline_analyzerThe kernel_stats page shows that 25% of the time is spent on SwapDimension1And2InTensor3UsingTiles. I’m not sure swapping dimensions should take up that much time (doesn’t seem so). Some more research is required to answer that. The page also provides a helpful indication of whether an Op is Tensor Core eligible and whether Tensor Cores were actually used:kernel_statsThe notebook used (I used my library tf-helper-bot to wrap my custom training loop in a Fast.ai-inspired API. ):https://medium.com/media/50af2c437d3da78615887a96ce5f916b/hrefConclusionThanks for reading! Hopefully this post shows to you that Tensorflow Profiler is a powerful and easy-to-use tool (once you overcome the installation hurdles) that can potentially save you tons of time.This post only covers part of the profiler capabilities. There are a lot of things I don’t fully understand yet. The profiling report should give you some sense of where to look. I’d love to know if you found any other interesting resources on this topic (leave a comment!).Tensorflow Profiler with Custom Training Loop was originally published in Veritable on Medium, where people are continuing the conversation by highlighting and responding to this story.</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col2\" class=\"data row8 col2\" >0.356077</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col3\" class=\"data row8 col3\" >0.395847</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col4\" class=\"data row8 col4\" >0.470875</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col5\" class=\"data row8 col5\" >0.404392</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col6\" class=\"data row8 col6\" >0.487572</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col7\" class=\"data row8 col7\" >0.559991</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col8\" class=\"data row8 col8\" >0.280598</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col9\" class=\"data row8 col9\" >0.362830</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col10\" class=\"data row8 col10\" >0.284162</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col11\" class=\"data row8 col11\" >0.291214</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col12\" class=\"data row8 col12\" >0.096652</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col13\" class=\"data row8 col13\" >0.281851</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col14\" class=\"data row8 col14\" >0.275644</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col15\" class=\"data row8 col15\" >0.139623</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row8_col16\" class=\"data row8 col16\" >0.253902</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col0\" class=\"data row9 col0\" >HOLMES: Health OnLine Model Ensemble Serving for Deep Learning Models in Intensive Care Units</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col1\" class=\"data row9 col1\" >HOLMES is tested on risk prediction task on pediatric cardio ICU data with above 95% prediction accuracy and sub-second latency on 64-bed simulation. Code: https://github.com/hsd1503/HOLMES</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col2\" class=\"data row9 col2\" >0.241421</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col3\" class=\"data row9 col3\" >0.382152</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col4\" class=\"data row9 col4\" >0.167640</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col5\" class=\"data row9 col5\" >0.121318</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col6\" class=\"data row9 col6\" >0.244979</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col7\" class=\"data row9 col7\" >0.168957</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col8\" class=\"data row9 col8\" >0.184917</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col9\" class=\"data row9 col9\" >0.294742</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col10\" class=\"data row9 col10\" >0.209302</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col11\" class=\"data row9 col11\" >0.178837</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col12\" class=\"data row9 col12\" >0.124122</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col13\" class=\"data row9 col13\" >0.091974</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col14\" class=\"data row9 col14\" >0.119952</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col15\" class=\"data row9 col15\" >0.176332</td>\n",
       "                        <td id=\"T_373fc37c_dd84_11ea_9df5_f4d108645659row9_col16\" class=\"data row9 col16\" >0.236092</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5d781f7890>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc.text for doc in raw_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc.text for doc in raw_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain\n",
    "\n",
    "zsl_clf = ktrain.text.ZeroShotClassifier(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [\n",
    "    zsl_clf.predict(d['text'], topic_strings=topic_strings, max_length=256, include_labels=True)\n",
    "    for d in results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_records(\n",
    "    {**result, **dict(doc_scores)}\n",
    "    for (result, doc_scores) in zip(results, scores)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
